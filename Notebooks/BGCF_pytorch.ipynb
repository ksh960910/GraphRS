{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277c185d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "\n",
    "# directory = '../MF/ml-1m'\n",
    "# ratings = []\n",
    "# with open(os.path.join(directory, 'ratings.dat'), encoding='latin1') as f:\n",
    "#     for l in f:\n",
    "#         user_id, movie_id, rating, timestamp = [int(_) for _ in l.split('::')]\n",
    "#         ratings.append({\n",
    "#             'user_id': user_id,\n",
    "#             'movie_id': movie_id,\n",
    "#             'rating': rating,\n",
    "#             'timestamp': timestamp,\n",
    "#             })\n",
    "# ratings = pd.DataFrame(ratings)\n",
    "\n",
    "# ratings = ratings.drop(['timestamp'],axis=1)\n",
    "# print(ratings)\n",
    "# # 어떤 비율로 표본 추출을 하고싶은지 df.sample에서 frac을 0~1 사이로 설정\n",
    "# df = ratings.sample(frac=1).reset_index(drop=True)\n",
    "# filter_user = deepcopy(df)\n",
    "# counts = filter_user['user_id'].value_counts()\n",
    "# filter_user = filter_user[filter_user['user_id'].isin(counts[counts >= 10].index)]\n",
    "\n",
    "# filtered_df = deepcopy(filter_user)\n",
    "# counts = filtered_df['movie_id'].value_counts()\n",
    "# filtered_df = filtered_df[filtered_df['movie_id'].isin(counts[counts >=10].index)]\n",
    "# filtered_df = filtered_df.reset_index(drop=True)\n",
    "# print(filtered_df)\n",
    "\n",
    "\n",
    "class generate_graph(object):\n",
    "    def __init__(self, path):\n",
    "        self.path = path  # '../NGCF-PyTorch/Data/ml-1m'\n",
    "        train_file = path + '/train.txt'\n",
    "        \n",
    "        self.neighbor_dict = {}\n",
    "        self.user, self.item = [], []\n",
    "        \n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    self.neighbor_dict[int(l[0])] = [int(i) for i in l[1:]]\n",
    "                    self.user.append(int(l[0]))\n",
    "        self.user = self.user[:1000]\n",
    "        \n",
    "    def jaccard_index(self, u_i, u_j, neighbor_dict):\n",
    "        u_i_neighbor = self.neighbor_dict[u_i]\n",
    "        u_j_neighbor = self.neighbor_dict[u_j]\n",
    "        return len(list(set(u_i_neighbor) & set(u_j_neighbor))) / len(list(set(u_i_neighbor) | set(u_j_neighbor)))\n",
    "\n",
    "    # 새로운 graph의 node j 에다가 기존 graph의 어떤 node의 neighborhood를 복사할지 zeta에 담기\n",
    "    def node_copying(self):\n",
    "        t1 = time()\n",
    "        zeta = []\n",
    "        \n",
    "        for u_j in self.user:\n",
    "            nor = 0\n",
    "            zeta_distribution = []\n",
    "            for u_i in self.user:\n",
    "                nor+=self.jaccard_index(u_j, u_i, self.neighbor_dict)\n",
    "            for u_m in self.user:\n",
    "                zeta_distribution.append(self.jaccard_index(u_j, u_m, self.neighbor_dict) / nor)\n",
    "            zeta.append(random.choices(self.user, weights=zeta_distribution)[0])\n",
    "        print('total node copying time cost : ', time() - t1)\n",
    "        np.save(self.path + '/zeta.npy', zeta)\n",
    "        \n",
    "        return zeta\n",
    "    \n",
    "    def generate_graph(self, epsilon, iteration):\n",
    "        t2 = time()\n",
    "        self.epsilon = epsilon\n",
    "        self.iteration = iteration\n",
    "        \n",
    "        generated_node = []\n",
    "        \n",
    "        with open(self.path+'/sampled_graph/sampled_graph_'+str(iteration+1), 'w') as f:\n",
    "            for i in self.user:\n",
    "                if random.uniform(0,1) < 1-self.epsilon:  # 1-epsilon의 확률로 원래 neighbor 넣기\n",
    "                    generated_node.append(i)\n",
    "                else:                                # epsilon의 확률로 zeta에 있는 node의 neighbor로 copy해서 넣기\n",
    "                    generated_node.append(zeta[i])\n",
    "                    \n",
    "                # 만들어지는 새로운 graph를 txt 파일로 저장\n",
    "                f.write(str(i))\n",
    "                f.write(' ')\n",
    "                for j in self.neighbor_dict[generated_node[i]][:-1]:\n",
    "                    f.write(str(j))\n",
    "                    f.write(' ')\n",
    "                f.write(str(self.neighbor_dict[generated_node[i]][-1]))\n",
    "                f.write('\\n')\n",
    "        print('#',iteration+1,' Graph sampled time cost : ', time() - t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a07a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = generate_graph('ml-1m')\n",
    "# zeta = node_copying(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta = t.node_copying()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enabling-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta = np.load('ml-1m/zeta.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0790388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1  Graph sampled time cost :  0.19529509544372559\n",
      "# 2  Graph sampled time cost :  0.19852304458618164\n",
      "# 3  Graph sampled time cost :  0.1815776824951172\n"
     ]
    }
   ],
   "source": [
    "# zeta를 한번 만든 후 iteration마다 generate graph하는 방식\n",
    "\n",
    "for epoch in range(3):\n",
    "    t.generate_graph(epsilon = 0.01, iteration = epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c05d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT\n",
    "# 만들어지는 graph마다 embedding 초기화 \n",
    "# embedding 학습하는 GNN 구조 짜기\n",
    "# 학습된 embedding 가지고 x_hat (eq.11) 구하고 BPR-OPT를 maximization 시키는 방향으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9df93286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data로 sparse matrix 및 adjacency matrix 만들기\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        train_file = path + '/train.txt'\n",
    "        # path : ml-1m\n",
    "        \n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.exist_users = []\n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "\n",
    "        self.n_users+=1\n",
    "        self.n_items+=1\n",
    "        print(self.n_users, self.n_items)\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.train_items = {}\n",
    "\n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l)==0:\n",
    "                    break\n",
    "                l = l.strip('\\n').split(' ')\n",
    "                uid, items = int(l[0]), [int(i) for i in l[1:]]\n",
    "\n",
    "                for i in items:\n",
    "                    self.R[uid, i] = 1\n",
    "\n",
    "                self.train_items[uid] = items\n",
    "                \n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            obs_adj_mat = sp.load_npz(self.path + '/obs_adj_mat.npz')\n",
    "            \n",
    "        except Exception:\n",
    "            obs_adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/obs_adj_mat.npz', obs_adj_mat)\n",
    "            \n",
    "        return obs_adj_mat\n",
    "    \n",
    "    \n",
    "    def create_adj_mat(self):\n",
    "        obs_adj_mat = self.R.todok()\n",
    "        print('already create observed graph adjacency matrix', obs_adj_mat.shape)\n",
    "        return obs_adj_mat.tocsr()\n",
    "    \n",
    "    # bgcf는 G_obs로부터 만들어진 sampled graphs에 대해 x hat들의 integral을 구함\n",
    "    def sample(self):\n",
    "        # positive / negative items 나누기\n",
    "        if self.batch_size <= self.n_users:\n",
    "            obs_users = random.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            obs_users = [random.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "            \n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            # u유저의 neighbor중 num개 만큼 positive item sampling\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num:\n",
    "                    break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            # u유저의 neighbor가 아닌 item 중 num개 만큼 sampling\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num:\n",
    "                    break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "\n",
    "    #         def sample_neg_items_for_u_from_pools(u, num):\n",
    "    #             neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "    #             return random.sample(neg_items, num)     \n",
    "\n",
    "        obs_pos_items, obs_neg_items = [], []\n",
    "        for u in obs_users:\n",
    "            obs_pos_items += sample_pos_items_for_u(u,1)\n",
    "            obs_neg_items += sample_neg_items_for_u(u,1)\n",
    "\n",
    "        return obs_users, obs_pos_items, obs_neg_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b5b0a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# node copying으로 만들어진 graph들에 대해서 adj matrix 만들고 npz 저장하는 과정 \n",
    "\n",
    "class sampled_graph_to_matrix(object):\n",
    "    def __init__(self, path, iteration, batch_size):\n",
    "        self.path =path\n",
    "        self.iteration = iteration\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        sampled_graph = path + '/sampled_graph/sampled_graph_' + str(iteration+1)\n",
    "        # path : 'ml-1m'\n",
    "        \n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.exist_users = []\n",
    "        self.neg_pools = {}\n",
    "        \n",
    "        with open(sampled_graph, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "\n",
    "        self.n_users+=1\n",
    "        self.n_items+=1\n",
    "        print(self.n_users, self.n_items)\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.train_items = {}\n",
    "\n",
    "        with open(sampled_graph, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l)==0:\n",
    "                    break\n",
    "                l = l.strip('\\n').split(' ') \n",
    "                uid, items = int(l[0]), [int(i) for i in l[1:]]\n",
    "\n",
    "                for i in items:\n",
    "                    self.R[uid, i] = 1\n",
    "\n",
    "                self.train_items[uid] = items\n",
    "                \n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat_' + str(self.iteration+1) + '.npz')\n",
    "            \n",
    "        except Exception:\n",
    "            adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat_' + str(self.iteration+1) + '.npz', adj_mat)\n",
    "            \n",
    "        return adj_mat\n",
    "    \n",
    "     \n",
    "    def create_adj_mat(self):\n",
    "        adj_mat = self.R.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape)\n",
    "        return adj_mat.tocsr()\n",
    "    \n",
    "    def negative_pool(self):\n",
    "        t1 = time()\n",
    "        for u in self.train_items.keys():\n",
    "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\n",
    "            pools = [random.choice(neg_items) for _ in range(100)]\n",
    "            self.neg_pools[u] = pools\n",
    "        print('refresh negative pools', time() - t1)\n",
    "    \n",
    "    # bgcf는 G_obs로부터 만들어진 sampled graphs에 대해 x hat들의 integral을 구함\n",
    "    def sample(self):\n",
    "        # positive / negative items 나누기\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = random.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [random.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "            \n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            # u유저의 neighbor중 num개 만큼 positive item sampling\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num:\n",
    "                    break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "                \n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "        \n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            # u유저의 neighbor가 아닌 item 중 num개 만큼 sampling\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num:\n",
    "                    break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "        \n",
    "#         def sample_neg_items_for_u_from_pools(u, num):\n",
    "#             neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "#             return random.sample(neg_items, num)     \n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u,1)\n",
    "            neg_items += sample_neg_items_for_u(u,1)\n",
    "            \n",
    "        return users, pos_items, neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "promotional-particular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 3953\n"
     ]
    }
   ],
   "source": [
    "obs_graph = Data(path='ml-1m', batch_size = 1000)\n",
    "obs_users, obs_pos_items, obs_neg_items = obs_graph.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044aa381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 3953\n",
      "1000 3953\n",
      "1000 3953\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(3):\n",
    "    sampled_graph = sampled_graph_to_matrix(path='ml-1m', iteration = iteration, batch_size=1000)\n",
    "    sampled_graph.get_adj_mat()\n",
    "    users, pos_items, neg_items = sampled_graph.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c07d7",
   "metadata": {},
   "source": [
    "#### ngcf에서의 w_gc와 w_bi는 gc는 neighbor aggregate하는 부분의 weight, bi는 element-wise product하는 부분의 weight\n",
    "#### bgcf에서는 single feed forward layer로 구성하였고 총 3개의 weight가 학습됨 (GAT에서 2개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f0be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous version of GAT layer\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    # TODO : 초기함수에 positive/negative item 리스트를 받아서 각 positive/negative item들의 embedding이 output으로 나오게끔\n",
    "    def __init__(self, n_users, n_items, in_features, out_features, dropout = 0.2):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features      # 기존 노드의 embedding 차원수 \n",
    "        self.out_features = out_features    # 결과 weight의 embedding 차원수\n",
    "        \n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        \n",
    "        # self.W = nn.Parameter(initializer(torch.empty(in_features, out_features)))\n",
    "        # W_1은 a*e aggregate 부분, W_2는 e aggregate 부분\n",
    "        self.weight_dict = nn.ParameterDict({\n",
    "            'W_1' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))),\n",
    "            'W_2' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))),\n",
    "            'W_observed' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))) \n",
    "        })\n",
    "        \n",
    "        self.embedding_dict = nn.ParameterDict({\n",
    "            'user_emb' : nn.Parameter(initializer(torch.empty(self.n_users, self.out_features))),\n",
    "            'item_emb' : nn.Parameter(initializer(torch.empty(self.n_items, self.out_features)))\n",
    "        })\n",
    "        \n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "        \n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "        \n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "        \n",
    "        regularizer = (torch.norm(users) ** 2\n",
    "                       + torch.norm(pos_items) ** 2\n",
    "                       + torch.norm(neg_items) ** 2) / 2\n",
    "        emb_loss = regularizer / self.batch_size\n",
    "        #emb_loss = self.decay * regularizer / self.batch_size  (최종)\n",
    "            \n",
    "    \n",
    "    def forward(self, users, pos_items, neg_items, adj_matrix, iteration):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.iteration = iteration\n",
    "        coef = torch.zeros(self.n_users, self.n_items)\n",
    "        \n",
    "#         adj_matrix = sp.load_npz(path + '/s_adj_mat_' + str(iteration+1) +'.npz').toarray()\n",
    "#         adj_matrix = torch.Tensor(adj)\n",
    "        \n",
    "        user_emb = self.embedding_dict['user_emb']\n",
    "        item_emb = self.embedding_dict['item_emb']\n",
    "        \n",
    "        # self.W의 크기를 정확하게 확인해야함, 최종적인 각 node의 embedding size 는 1 x emb_size 로 나오게끔\n",
    "        # score는 각각 user-item 간의 attention score\n",
    "        score = torch.exp(torch.inner(user_emb, item_emb))\n",
    "        score = torch.multiply(score, adj_matrix)\n",
    "        \n",
    "        for i in range(score.shape[0]):\n",
    "            norm = sum(score[i])\n",
    "            normalized_score = score[i] / norm\n",
    "            coef[i] = normalized_score\n",
    "        # coef에다가 normalized된 최종적인 attention score 저장\n",
    "        w_1 = self.weight_dict['W_1']\n",
    "        w_2 = self.weight_dict['W_2']\n",
    "        # h_tilde_1은 w_1과 곱해지는 부분의 embedding (e_k)를 user와 item으로 쪼개서 계산후 concat\n",
    "        h_tilde_1_user = torch.matmul(coef, item_emb)\n",
    "        h_tilde_1_user = torch.matmul(h_tilde_1_user, w_1)\n",
    "        h_tilde_1_item = torch.matmul(coef.T, user_emb)\n",
    "        h_tilde_1_item = torch.matmul(h_tilde_1_item, w_1)\n",
    "        h_tilde_1 = torch.cat((h_tilde_1_user, h_tilde_1_item), dim=0)\n",
    "        \n",
    "        neighbor_num_user, neighbor_num_item = torch.zeros(self.n_users), torch.zeros(self.n_items)\n",
    "        for i in range(self.n_users):\n",
    "            neighbor_num_user[i] = sum(adj_matrix[i])\n",
    "        for j in range(self.n_items):\n",
    "            neighbor_num_item[i] = sum(adj_matrix.T[i])\n",
    "        # neighbor_num은 n_j에 해당하는 부분\n",
    "        \n",
    "        # h_tilde_2는 w_2와 곱해지는 부분\n",
    "        h_tilde_2_user = torch.matmul(adj_matrix, item_emb)\n",
    "        # h_tilde_2_user = torch.matmul(neighbor_num_user, h_tilde_2_user)\n",
    "        # 각 user의 neighbor의 수 만큼 item embedding에 곱해주는 과정\n",
    "        for i in range(neighbor_num_user.shape[0]):\n",
    "            h_tilde_2_user[i] = neighbor_num_user[i] * h_tilde_2_user[i]\n",
    "        h_tilde_2_user = torch.matmul(h_tilde_2_user, w_2)\n",
    "        h_tilde_2_item = torch.matmul(adj_matrix.T, user_emb)\n",
    "        # h_tilde_2_item = torch.matmul(neighbor_num_item, h_tilde_2_item)\n",
    "        # 각 item의 neighbor의 수 만큼 user embedding에 곱해주는 과정\n",
    "        for j in range(neighbor_num_item.shape[0]):\n",
    "            h_tilde_2_item[i] = neighbor_num_item[i] * h_tilde_2_item[i]\n",
    "        h_tilde_2_item = torch.matmul(h_tilde_2_item, w_2)\n",
    "        h_tilde_2 = torch.cat((h_tilde_2_user, h_tilde_2_item), dim=0)\n",
    "        \n",
    "        print(h_tilde_1.shape, h_tilde_2.shape)\n",
    "        \n",
    "        h_tilde_sampled = torch.cat((h_tilde_1, h_tilde_2), dim=0)\n",
    "        \n",
    "        # h_tilde_observed \n",
    "        \n",
    "        # h_tilde_sampled를 user embedidng / pos, neg item embedding들로 쪼개야함\n",
    "        return h_tilde_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "strategic-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGCFLayer(nn.Module):\n",
    "    # TODO : 초기함수에 positive/negative item 리스트를 받아서 각 positive/negative item들의 embedding이 output으로 나오게끔\n",
    "    def __init__(self, n_users, n_items, in_features, out_features, dropout = 0.2):\n",
    "        super(BGCFLayer, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features      # 기존 노드의 embedding 차원수 \n",
    "        self.out_features = out_features    # 결과 weight의 embedding 차원수\n",
    "        \n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        \n",
    "        # self.W = nn.Parameter(initializer(torch.empty(in_features, out_features)))\n",
    "        # W_1은 a*e aggregate 부분, W_2는 e aggregate 부분\n",
    "        self.weight_dict = nn.ParameterDict({\n",
    "            'W_1' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))),\n",
    "            'W_2' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))),\n",
    "            'W_obs' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features)))\n",
    "        })\n",
    "        \n",
    "        self.embedding_dict = nn.ParameterDict({\n",
    "            'user_emb' : nn.Parameter(initializer(torch.empty(self.n_users, self.out_features))),\n",
    "            'item_emb' : nn.Parameter(initializer(torch.empty(self.n_items, self.out_features)))\n",
    "        })\n",
    "        \n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "        \n",
    "        print('pos_scores : ',pos_scores)\n",
    "        print('neg_scores : ',neg_scores)\n",
    "        \n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "        \n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "        \n",
    "        regularizer = (torch.norm(users) ** 2\n",
    "                       + torch.norm(pos_items) ** 2\n",
    "                       + torch.norm(neg_items) ** 2) / 2\n",
    "        \n",
    "        emb_loss = regularizer / 1000\n",
    "        # emb_loss = self.decay * regularizer / self.batch_size  (최종)\n",
    "        \n",
    "        mf_loss = torch.nan_to_num(mf_loss)\n",
    "        emb_loss = torch.nan_to_num(emb_loss)\n",
    "        \n",
    "        return mf_loss+emb_loss, mf_loss, emb_loss\n",
    "            \n",
    "    \n",
    "    def forward(self, \n",
    "                users, \n",
    "                pos_items, \n",
    "                neg_items, \n",
    "                adj_matrix, \n",
    "                obs_users, \n",
    "                obs_pos_items, \n",
    "                obs_neg_items, \n",
    "                obs_adj_matrix, \n",
    "                iteration):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.iteration = iteration\n",
    "        coef = torch.zeros(self.n_users, self.n_items)\n",
    "        \n",
    "#         adj_matrix = sp.load_npz(path + '/s_adj_mat_' + str(iteration+1) +'.npz').toarray()\n",
    "#         adj_matrix = torch.Tensor(adj)\n",
    "        \n",
    "        user_emb = self.embedding_dict['user_emb']\n",
    "        item_emb = self.embedding_dict['item_emb']\n",
    "        \n",
    "        # score는 각각 user-item 간의 attention score\n",
    "        score = torch.exp(torch.inner(user_emb, item_emb))\n",
    "        score = torch.multiply(score, adj_matrix)\n",
    "        \n",
    "        for i in range(score.shape[0]):\n",
    "            norm = sum(score[i])\n",
    "            normalized_score = score[i] / norm\n",
    "            coef[i] = normalized_score\n",
    "        # coef에다가 normalized된 최종적인 attention score 저장\n",
    "        w_1 = self.weight_dict['W_1']\n",
    "        w_2 = self.weight_dict['W_2']\n",
    "        # h_tilde_1은 w_1과 곱해지는 부분의 embedding (e_k)를 user와 item으로 쪼개서 계산후 concat\n",
    "        h_tilde_1_user = torch.matmul(coef, item_emb)\n",
    "        h_tilde_1_user = torch.matmul(h_tilde_1_user, w_1)\n",
    "        h_tilde_1_item = torch.matmul(coef.T, user_emb)\n",
    "        h_tilde_1_item = torch.matmul(h_tilde_1_item, w_1)\n",
    "        # h_tilde_1 = torch.cat((h_tilde_1_user, h_tilde_1_item), dim=0)\n",
    "        \n",
    "        neighbor_num_user, neighbor_num_item = torch.zeros(self.n_users), torch.zeros(self.n_items)\n",
    "        for i in range(self.n_users):\n",
    "            neighbor_num_user[i] = sum(adj_matrix[i])\n",
    "        for j in range(self.n_items):\n",
    "            neighbor_num_item[i] = sum(adj_matrix.T[i])\n",
    "        # neighbor_num은 n_j에 해당하는 부분\n",
    "        \n",
    "        # h_tilde_2는 w_2와 곱해지는 부분\n",
    "        h_tilde_2_user = torch.matmul(adj_matrix, item_emb)\n",
    "        # h_tilde_2_user = torch.matmul(neighbor_num_user, h_tilde_2_user)\n",
    "        # 각 user의 neighbor의 수 만큼 item embedding에 곱해주는 과정\n",
    "        for i in range(neighbor_num_user.shape[0]):\n",
    "            h_tilde_2_user[i] = (1 / neighbor_num_user[i]) * h_tilde_2_user[i]\n",
    "        h_tilde_2_user = torch.matmul(h_tilde_2_user, w_2)\n",
    "        h_tilde_2_item = torch.matmul(adj_matrix.T, user_emb)\n",
    "        # h_tilde_2_item = torch.matmul(neighbor_num_item, h_tilde_2_item)\n",
    "        # 각 item의 neighbor의 수 만큼 user embedding에 곱해주는 과정\n",
    "        for j in range(neighbor_num_item.shape[0]):\n",
    "            h_tilde_2_item[i] = (1 / neighbor_num_item[i]) * h_tilde_2_item[i]\n",
    "        h_tilde_2_item = torch.matmul(h_tilde_2_item, w_2)\n",
    "        # h_tilde_2 = torch.cat((h_tilde_2_user, h_tilde_2_item), dim=0)\n",
    "        \n",
    "        h_tilde_sampled_user = torch.cat((h_tilde_1_user, h_tilde_2_user), dim=1)\n",
    "        h_tilde_sampled_item = torch.cat((h_tilde_2_item, h_tilde_2_item), dim=1)\n",
    "        \n",
    "        h_tilde_sampled_pos_item = h_tilde_sampled_item[pos_items,:]\n",
    "        h_tilde_sampled_neg_item = h_tilde_sampled_item[neg_items,:]\n",
    "        \n",
    "        print('user embedding shape : ',h_tilde_sampled_user.shape)\n",
    "        print('positive item embedding shape : ',h_tilde_sampled_pos_item.shape)\n",
    "        print('negative item embedding shape : ',h_tilde_sampled_neg_item.shape)\n",
    "        \n",
    "        \n",
    "        ### h_tilde_observed \n",
    "        w_obs = self.weight_dict['W_obs']\n",
    "        \n",
    "        # observed graph의 neighbor 정보 \n",
    "        neighbor_num_user, neighbor_num_item = torch.zeros(self.n_users), torch.zeros(self.n_items)\n",
    "        # 각 원소 = 각 node의 neighbor수\n",
    "        for i in range(self.n_users):\n",
    "            obs_neighbor_num_user[i] = sum(obs_adj_matrix[i])\n",
    "        for j in range(self.n_items):\n",
    "            obs_neighbor_num_item[i] = sum(obs_adj_matrix.T[i])\n",
    "        # neighbor_num은 n_j에 해당하는 부분\n",
    "        \n",
    "        # h_tilde_obs_user\n",
    "        h_tilde_obs_user = torch.matmul(obs_adj_mat, item_emb)\n",
    "        for i in range(obs_neighbor_num_user.shape[0]):\n",
    "            h_tilde_obs_user[i] = (1 / obs_neighbor_num_user[i]) * h_tilde_obs_user[i]\n",
    "        h_tilde_obs_user = torch.sigmoid(torch.matmul(h_tilde_obs_user, w_obs))\n",
    "        \n",
    "        # h_tilde_obs_item\n",
    "        h_tilde_obs_item = torch.matmul(obs_adj_mat.T, user_emb)\n",
    "        for j in range(obs_neighbor_num_item.shape[0]):\n",
    "            h_tilde_obs_item[i] = (1 / obs_neighbor_num_item[i]) * h_tilde_obs_item[i]\n",
    "        h_tilde_obs_item = torch.sigmoid(torch.matmul(h_tilde_obs_item, w_obs))\n",
    "        \n",
    "        h_tilde_obs_pos_item = h_tilde_obs_item[obs_pos_items,:]\n",
    "        h_tilde_obs_neg_item = h_tilde_obs_item[obs_neg_items,:]\n",
    "        \n",
    "        # Final embedding\n",
    "        h_tilde_user = torch.sigmoid(torch.cat(h_tilde_sampled_user, h_tilde_obs_user), dim=1)\n",
    "        h_tilde_pos_item = torch.sigmoid(torch.cat(h_tilde_sampled_pos_item, h_tilde_obs_pos_item), dim=1)\n",
    "        h_tilde_neg_item = torch.sigmoid(torch.cat(h_tilde_sampled_neg_item, h_tilde_obs_neg_item), dim=1)\n",
    "        \n",
    "        # h_tilde_sampled를 user embedidng / pos, neg item embedding들로 쪼개야함\n",
    "        return h_tilde_user, h_tilde_pos_item, h_tilde_neg_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44826b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 3953\n",
      "1000 1000 1000\n",
      "1000 3953\n"
     ]
    }
   ],
   "source": [
    "# pos_items, neg_items = sampled_graph_to_matrix(path='ml-1m', iteration = iteration).divide_pos_neg()\n",
    "# 완성형은 GATLayer(6040, 3953, 64,64, pos_items, neg_items)\n",
    "model = BGCFLayer(1000, 3953, 64, 64)\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "path = 'ml-1m'\n",
    "for epoch in range(1):\n",
    "    loss, mf_loss, emb_loss = 0., 0., 0.\n",
    "    \n",
    "    obs_graph = Data(path = path, batch_size = 1000)\n",
    "    obs_adj_matrix = obs_graph.get_adj_mat().toarray()\n",
    "    obs_adj_matrix = torch.Tensor(obs_adj_matrix)\n",
    "    obs_users, obs_pos_items, obs_neg_items = obs_graph.sample()\n",
    "    \n",
    "    sampled_graph = sampled_graph_to_matrix(path = path, iteration = epoch, batch_size=1000)\n",
    "    adj_matrix = sampled_graph.get_adj_mat().toarray()\n",
    "    adj_matrix = torch.Tensor(adj_matrix)\n",
    "    users, pos_items, neg_items = sampled_graph.sample()\n",
    "    \n",
    "    for iteration in range(1):\n",
    "        t1 = time()\n",
    "        \n",
    "        # sampled_graph.get_adj_mat 함수로 바꿔야할듯\n",
    "#         adj_matrix = sp.load_npz(path + '/s_adj_mat_' + str(iteration+1) +'.npz').toarray()\n",
    "#         adj_matrix = torch.Tensor(adj_matrix)\n",
    "\n",
    "        '''불러오는 sampled graph matrix마다 pos, neg item set을 만들고 (함수 사용) \n",
    "        bpr loss 함수에다가 각 item들에 해당하는 embedding을 입력으로 넣어줌'''\n",
    "\n",
    "        # sampled_graph = sampled_graph_to_matrix(path = path, iteration = iteration, batch_size=1000)\n",
    "        # users, pos_items, neg_items = sampled_graph.sample()\n",
    "        u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings = model(users,\n",
    "                                                                       pos_items,\n",
    "                                                                       neg_items,\n",
    "                                                                       adj_matrix,\n",
    "                                                                       obs_users,\n",
    "                                                                       obs_pos_items,\n",
    "                                                                       obs_neg_items,\n",
    "                                                                       obs_adj_matrix,\n",
    "                                                                       iteration)\n",
    "        \n",
    "        batch_loss, batch_mf_loss, batch_emb_loss = model.create_bpr_loss(u_g_embeddings,\n",
    "                                                                          pos_i_g_embeddings,\n",
    "                                                                          neg_i_g_embeddings)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print(batch_loss, batch_mf_loss, batch_emb_loss)\n",
    "        \n",
    "        loss += batch_loss\n",
    "        mf_loss += batch_mf_loss\n",
    "        emb_loss += batch_emb_loss\n",
    "        \n",
    "        print(loss, mf_loss, emb_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f8825c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3953)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'initializer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2c47a566dc4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0madj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcoef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3953\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0muser_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mitem_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3953\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mw_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initializer' is not defined"
     ]
    }
   ],
   "source": [
    "adj = sp.load_npz('ml-1m/s_adj_mat_1.npz')\n",
    "adj = adj.toarray()\n",
    "print(adj.shape)\n",
    "adj = torch.Tensor(adj)\n",
    "coef = torch.zeros(3,3953)\n",
    "user_emb = nn.Parameter(initializer(torch.empty(1000,32)))\n",
    "item_emb = nn.Parameter(initializer(torch.empty(3953,32)))\n",
    "w_1 = nn.Parameter(initializer(torch.empty(32,32)))\n",
    "w_2 = nn.Parameter(initializer(torch.empty(32,32)))\n",
    "\n",
    "x = torch.exp(torch.inner(user_emb, item_emb))\n",
    "print(x)\n",
    "x = torch.multiply(x, adj)\n",
    "print(x)\n",
    "for i in range(3):\n",
    "    tot = sum(x[i])\n",
    "    score = x[i] / tot\n",
    "    coef[i] = score\n",
    "    \n",
    "w1 = torch.matmul(coef, item_emb)\n",
    "print(w1.shape)\n",
    "w1 = torch.matmul(w1, w_1)\n",
    "print(w1.shape)\n",
    "w2 = torch.matmul(coef.T, user_emb[:3])\n",
    "print(w2.shape)\n",
    "w2 = torch.matmul(w2, w_2)\n",
    "print(w2.shape)\n",
    "w3 = torch.cat((w1, w2), dim=0)\n",
    "w3.shape\n",
    "\n",
    "user_emb = nn.Parameter(initializer(torch.empty(1000,32)))\n",
    "item_emb = nn.Parameter(initializer(torch.empty(3953,32)))\n",
    "avg_emb = torch.matmul(adj.T, user_emb)\n",
    "avg_emb.shape\n",
    "# x = torch.multiply(x, adj)\n",
    "# print(x.shape)\n",
    "# x = F.softmax(x, dim=1)\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3840a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_num(row):\n",
    "    return sum(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumemb = torch.zeros((5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    sumemb[i] = sum(adj[i])\n",
    "    \n",
    "torch.matmul(sumemb, item_emb[:5]).shape\n",
    "# sumemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead80d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = nn.init.xavier_uniform_\n",
    "a = nn.Parameter(initializer(torch.empty(5,5)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646153ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = nn.init.xavier_uniform_\n",
    "user_emb = nn.Parameter(initializer(torch.empty(5,10)))\n",
    "user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.matrix([[0,1,2],[1,1,1],[0,0,1]])\n",
    "adj = np.matrix([[0,0,1],[1,0,0],[1,1,0]])\n",
    "e = np.matrix([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "print(a,'\\n')\n",
    "print(adj,'\\n')\n",
    "print(e,'\\n')\n",
    "\n",
    "a_jk = np.multiply(a,adj)\n",
    "print(a_jk, '\\n')\n",
    "\n",
    "np.matmul(a_jk, e)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80349c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
