{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277c185d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "\n",
    "# directory = '../MF/ml-1m'\n",
    "# ratings = []\n",
    "# with open(os.path.join(directory, 'ratings.dat'), encoding='latin1') as f:\n",
    "#     for l in f:\n",
    "#         user_id, movie_id, rating, timestamp = [int(_) for _ in l.split('::')]\n",
    "#         ratings.append({\n",
    "#             'user_id': user_id,\n",
    "#             'movie_id': movie_id,\n",
    "#             'rating': rating,\n",
    "#             'timestamp': timestamp,\n",
    "#             })\n",
    "# ratings = pd.DataFrame(ratings)\n",
    "\n",
    "# ratings = ratings.drop(['timestamp'],axis=1)\n",
    "# print(ratings)\n",
    "# # 어떤 비율로 표본 추출을 하고싶은지 df.sample에서 frac을 0~1 사이로 설정\n",
    "# df = ratings.sample(frac=1).reset_index(drop=True)\n",
    "# filter_user = deepcopy(df)\n",
    "# counts = filter_user['user_id'].value_counts()\n",
    "# filter_user = filter_user[filter_user['user_id'].isin(counts[counts >= 10].index)]\n",
    "\n",
    "# filtered_df = deepcopy(filter_user)\n",
    "# counts = filtered_df['movie_id'].value_counts()\n",
    "# filtered_df = filtered_df[filtered_df['movie_id'].isin(counts[counts >=10].index)]\n",
    "# filtered_df = filtered_df.reset_index(drop=True)\n",
    "# print(filtered_df)\n",
    "\n",
    "\n",
    "class generate_graph(object):\n",
    "    def __init__(self, path):\n",
    "        self.path = path  # '../NGCF-PyTorch/Data/ml-1m'\n",
    "        train_file = path + '/train.txt'\n",
    "        \n",
    "        self.neighbor_dict = {}\n",
    "        self.user, self.item = [], []\n",
    "        \n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    self.neighbor_dict[int(l[0])] = [int(i) for i in l[1:]]\n",
    "                    self.user.append(int(l[0]))\n",
    "        self.user = self.user[:1000]\n",
    "        \n",
    "    def jaccard_index(self, u_i, u_j, neighbor_dict):\n",
    "        u_i_neighbor = self.neighbor_dict[u_i]\n",
    "        u_j_neighbor = self.neighbor_dict[u_j]\n",
    "        return len(list(set(u_i_neighbor) & set(u_j_neighbor))) / len(list(set(u_i_neighbor) | set(u_j_neighbor)))\n",
    "\n",
    "    # 새로운 graph의 node j 에다가 기존 graph의 어떤 node의 neighborhood를 복사할지 zeta에 담기\n",
    "    def node_copying(self):\n",
    "        t1 = time()\n",
    "        zeta = []\n",
    "        \n",
    "        for u_j in self.user:\n",
    "            nor = 0\n",
    "            zeta_distribution = []\n",
    "            for u_i in self.user:\n",
    "                nor+=self.jaccard_index(u_j, u_i, self.neighbor_dict)\n",
    "            for u_m in self.user:\n",
    "                zeta_distribution.append(self.jaccard_index(u_j, u_m, self.neighbor_dict) / nor)\n",
    "            zeta.append(random.choices(self.user, weights=zeta_distribution)[0])\n",
    "        print('total node copying time cost : ', time() - t1)\n",
    "        np.save(self.path + '/zeta.npy', zeta)\n",
    "        \n",
    "        return zeta\n",
    "    \n",
    "    def generate_graph(self, epsilon, iteration):\n",
    "        t2 = time()\n",
    "        self.epsilon = epsilon\n",
    "        self.iteration = iteration\n",
    "        \n",
    "        generated_node = []\n",
    "        \n",
    "        with open(self.path+'/sampled_graph/sampled_graph_'+str(iteration+1), 'w') as f:\n",
    "            for i in self.user:\n",
    "                if random.uniform(0,1) < 1-self.epsilon:  # 1-epsilon의 확률로 원래 neighbor 넣기\n",
    "                    generated_node.append(i)\n",
    "                else:                                # epsilon의 확률로 zeta에 있는 node의 neighbor로 copy해서 넣기\n",
    "                    generated_node.append(zeta[i])\n",
    "                    \n",
    "                # 만들어지는 새로운 graph를 txt 파일로 저장\n",
    "                f.write(str(i))\n",
    "                f.write(' ')\n",
    "                for j in self.neighbor_dict[generated_node[i]][:-1]:\n",
    "                    f.write(str(j))\n",
    "                    f.write(' ')\n",
    "                f.write(str(self.neighbor_dict[generated_node[i]][-1]))\n",
    "                f.write('\\n')\n",
    "        print('#',iteration+1,' Graph sampled time cost : ', time() - t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a07a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = generate_graph('ml-1m')\n",
    "# zeta = node_copying(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta = t.node_copying()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enabling-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta = np.load('ml-1m/zeta.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0790388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1  Graph sampled time cost :  0.17095708847045898\n",
      "# 2  Graph sampled time cost :  0.2802455425262451\n",
      "# 3  Graph sampled time cost :  0.24878883361816406\n"
     ]
    }
   ],
   "source": [
    "# zeta를 한번 만든 후 iteration마다 generate graph하는 방식\n",
    "\n",
    "for epoch in range(3):\n",
    "    t.generate_graph(epsilon = 0.01, iteration = epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8c05d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT\n",
    "# 만들어지는 graph마다 embedding 초기화 \n",
    "# embedding 학습하는 GNN 구조 짜기\n",
    "# 학습된 embedding 가지고 x_hat (eq.11) 구하고 BPR-OPT를 maximization 시키는 방향으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9df93286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data로 sparse matrix 및 adjacency matrix 만들기\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "        train_file = path + '/train.txt'\n",
    "        # path : ml-1m\n",
    "        \n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.exist_users = []\n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "\n",
    "        self.n_users+=1\n",
    "        self.n_items+=1\n",
    "        print(self.n_users, self.n_items)\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.train_items = {}\n",
    "\n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l)==0:\n",
    "                    break\n",
    "                l = l.strip('\\n').split(' ')\n",
    "                uid, items = int(l[0]), [int(i) for i in l[1:]]\n",
    "\n",
    "                for i in items:\n",
    "                    self.R[uid, i] = 1\n",
    "\n",
    "                self.train_items[uid] = items\n",
    "                \n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            adj_mat = sp.load_npz(self.path + '/train_adj_mat.npz')\n",
    "            \n",
    "        except Exception:\n",
    "            adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/train_adj_mat.npz', adj_mat)\n",
    "            \n",
    "        return adj_mat\n",
    "    \n",
    "    \n",
    "    def create_adj_mat(self):\n",
    "        adj_mat = self.R.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape)\n",
    "        return adj_mat.tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b5b0a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# node copying으로 만들어진 graph들에 대해서 adj matrix 만들고 npz 저장하는 과정 \n",
    "\n",
    "class sampled_graph_to_matrix(object):\n",
    "    def __init__(self, path, iteration, batch_size):\n",
    "        self.path =path\n",
    "        self.iteration = iteration\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        sampled_graph = path + '/sampled_graph/sampled_graph_' + str(iteration+1)\n",
    "        # path : 'ml-1m'\n",
    "        \n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.exist_users = []\n",
    "        self.neg_pools = {}\n",
    "        \n",
    "        with open(sampled_graph, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "\n",
    "        self.n_users+=1\n",
    "        self.n_items+=1\n",
    "        print(self.n_users, self.n_items)\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.train_items = {}\n",
    "\n",
    "        with open(sampled_graph, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l)==0:\n",
    "                    break\n",
    "                l = l.strip('\\n').split(' ') \n",
    "                uid, items = int(l[0]), [int(i) for i in l[1:]]\n",
    "\n",
    "                for i in items:\n",
    "                    self.R[uid, i] = 1\n",
    "\n",
    "                self.train_items[uid] = items\n",
    "                \n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat_' + str(self.iteration+1) + '.npz')\n",
    "            \n",
    "        except Exception:\n",
    "            adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat_' + str(self.iteration+1) + '.npz', adj_mat)\n",
    "            \n",
    "        return adj_mat\n",
    "    \n",
    "     \n",
    "    def create_adj_mat(self):\n",
    "        adj_mat = self.R.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape)\n",
    "        return adj_mat.tocsr()\n",
    "    \n",
    "    def negative_pool(self):\n",
    "        t1 = time()\n",
    "        for u in self.train_items.keys():\n",
    "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\n",
    "            pools = [random.choice(neg_items) for _ in range(100)]\n",
    "            self.neg_pools[u] = pools\n",
    "        print('refresh negative pools', time() - t1)\n",
    "    \n",
    "    # bgcf는 G_obs로부터 만들어진 sampled graphs에 대해 x hat들의 integral을 구함\n",
    "    def sample(self):\n",
    "        # positive / negative items 나누기\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = random.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [random.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "            \n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            # u유저의 neighbor중 num개 만큼 positive item sampling\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num:\n",
    "                    break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "                \n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "        \n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            # u유저의 neighbor가 아닌 item 중 num개 만큼 sampling\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num:\n",
    "                    break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "        \n",
    "#         def sample_neg_items_for_u_from_pools(u, num):\n",
    "#             neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "#             return random.sample(neg_items, num)     \n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u,1)\n",
    "            neg_items += sample_neg_items_for_u(u,1)\n",
    "            \n",
    "        return users, pos_items, neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "044aa381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 3953\n",
      "1000 3953\n",
      "1000 3953\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(3):\n",
    "    sampled_graph = sampled_graph_to_matrix(path='ml-1m', iteration = iteration, batch_size=1000)\n",
    "    sampled_graph.get_adj_mat()\n",
    "    users, pos_items, neg_items = sampled_graph.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c07d7",
   "metadata": {},
   "source": [
    "#### ngcf에서의 w_gc와 w_bi는 gc는 neighbor aggregate하는 부분의 weight, bi는 element-wise product하는 부분의 weight\n",
    "#### bgcf에서는 single feed forward layer로 구성하였고 총 3개의 weight가 학습됨 (GAT에서 2개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f0be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous version of GAT layer\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    # TODO : 초기함수에 positive/negative item 리스트를 받아서 각 positive/negative item들의 embedding이 output으로 나오게끔\n",
    "    def __init__(self, n_users, n_items, in_features, out_features, dropout = 0.2):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features      # 기존 노드의 embedding 차원수 \n",
    "        self.out_features = out_features    # 결과 weight의 embedding 차원수\n",
    "        \n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        \n",
    "        # self.W = nn.Parameter(initializer(torch.empty(in_features, out_features)))\n",
    "        # W_1은 a*e aggregate 부분, W_2는 e aggregate 부분\n",
    "        self.weight_dict = nn.ParameterDict({\n",
    "            'W_1' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))),\n",
    "            'W_2' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features)))\n",
    "        })\n",
    "        \n",
    "        self.embedding_dict = nn.ParameterDict({\n",
    "            'user_emb' : nn.Parameter(initializer(torch.empty(self.n_users, self.out_features))),\n",
    "            'item_emb' : nn.Parameter(initializer(torch.empty(self.n_items, self.out_features)))\n",
    "        })\n",
    "        \n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "        \n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "        \n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "        \n",
    "        regularizer = (torch.norm(users) ** 2\n",
    "                       + torch.norm(pos_items) ** 2\n",
    "                       + torch.norm(neg_items) ** 2) / 2\n",
    "        emb_loss = regularizer / self.batch_size\n",
    "        #emb_loss = self.decay * regularizer / self.batch_size  (최종)\n",
    "            \n",
    "    \n",
    "    def forward(self, users, pos_items, neg_items, adj_matrix, iteration):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.iteration = iteration\n",
    "        coef = torch.zeros(self.n_users, self.n_items)\n",
    "        \n",
    "#         adj_matrix = sp.load_npz(path + '/s_adj_mat_' + str(iteration+1) +'.npz').toarray()\n",
    "#         adj_matrix = torch.Tensor(adj)\n",
    "        \n",
    "        user_emb = self.embedding_dict['user_emb']\n",
    "        item_emb = self.embedding_dict['item_emb']\n",
    "        \n",
    "        # self.W의 크기를 정확하게 확인해야함, 최종적인 각 node의 embedding size 는 1 x emb_size 로 나오게끔\n",
    "        # score는 각각 user-item 간의 attention score\n",
    "        score = torch.exp(torch.inner(user_emb, item_emb))\n",
    "        score = torch.multiply(score, adj_matrix)\n",
    "        \n",
    "        for i in range(score.shape[0]):\n",
    "            norm = sum(score[i])\n",
    "            normalized_score = score[i] / norm\n",
    "            coef[i] = normalized_score\n",
    "        # coef에다가 normalized된 최종적인 attention score 저장\n",
    "        w_1 = self.weight_dict['W_1']\n",
    "        w_2 = self.weight_dict['W_2']\n",
    "        # h_tilde_1은 w_1과 곱해지는 부분의 embedding (e_k)를 user와 item으로 쪼개서 계산후 concat\n",
    "        h_tilde_1_user = torch.matmul(coef, item_emb)\n",
    "        h_tilde_1_user = torch.matmul(h_tilde_1_user, w_1)\n",
    "        h_tilde_1_item = torch.matmul(coef.T, user_emb)\n",
    "        h_tilde_1_item = torch.matmul(h_tilde_1_item, w_1)\n",
    "        h_tilde_1 = torch.cat((h_tilde_1_user, h_tilde_1_item), dim=0)\n",
    "        \n",
    "        neighbor_num_user, neighbor_num_item = torch.zeros(self.n_users), torch.zeros(self.n_items)\n",
    "        for i in range(self.n_users):\n",
    "            neighbor_num_user[i] = sum(adj_matrix[i])\n",
    "        for j in range(self.n_items):\n",
    "            neighbor_num_item[i] = sum(adj_matrix.T[i])\n",
    "        # neighbor_num은 n_j에 해당하는 부분\n",
    "        \n",
    "        # h_tilde_2는 w_2와 곱해지는 부분\n",
    "        h_tilde_2_user = torch.matmul(adj_matrix, item_emb)\n",
    "        # h_tilde_2_user = torch.matmul(neighbor_num_user, h_tilde_2_user)\n",
    "        # 각 user의 neighbor의 수 만큼 item embedding에 곱해주는 과정\n",
    "        for i in range(neighbor_num_user.shape[0]):\n",
    "            h_tilde_2_user[i] = neighbor_num_user[i] * h_tilde_2_user[i]\n",
    "        h_tilde_2_user = torch.matmul(h_tilde_2_user, w_2)\n",
    "        h_tilde_2_item = torch.matmul(adj_matrix.T, user_emb)\n",
    "        # h_tilde_2_item = torch.matmul(neighbor_num_item, h_tilde_2_item)\n",
    "        # 각 item의 neighbor의 수 만큼 user embedding에 곱해주는 과정\n",
    "        for j in range(neighbor_num_item.shape[0]):\n",
    "            h_tilde_2_item[i] = neighbor_num_item[i] * h_tilde_2_item[i]\n",
    "        h_tilde_2_item = torch.matmul(h_tilde_2_item, w_2)\n",
    "        h_tilde_2 = torch.cat((h_tilde_2_user, h_tilde_2_item), dim=0)\n",
    "        \n",
    "        print(h_tilde_1.shape, h_tilde_2.shape)\n",
    "        \n",
    "        h_tilde_sampled = torch.cat((h_tilde_1, h_tilde_2), dim=0)\n",
    "        \n",
    "        # h_tilde_observed \n",
    "        \n",
    "        # h_tilde_sampled를 user embedidng / pos, neg item embedding들로 쪼개야함\n",
    "        return h_tilde_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "strategic-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    # TODO : 초기함수에 positive/negative item 리스트를 받아서 각 positive/negative item들의 embedding이 output으로 나오게끔\n",
    "    def __init__(self, n_users, n_items, in_features, out_features, dropout = 0.2):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features      # 기존 노드의 embedding 차원수 \n",
    "        self.out_features = out_features    # 결과 weight의 embedding 차원수\n",
    "        \n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        \n",
    "        # self.W = nn.Parameter(initializer(torch.empty(in_features, out_features)))\n",
    "        # W_1은 a*e aggregate 부분, W_2는 e aggregate 부분\n",
    "        self.weight_dict = nn.ParameterDict({\n",
    "            'W_1' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))),\n",
    "            'W_2' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features)))\n",
    "        })\n",
    "        \n",
    "        self.embedding_dict = nn.ParameterDict({\n",
    "            'user_emb' : nn.Parameter(initializer(torch.empty(self.n_users, self.out_features))),\n",
    "            'item_emb' : nn.Parameter(initializer(torch.empty(self.n_items, self.out_features)))\n",
    "        })\n",
    "        \n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "        \n",
    "        print('pos_scores : ',pos_scores)\n",
    "        print('neg_scores : ',neg_scores)\n",
    "        \n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "        \n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "        \n",
    "        regularizer = (torch.norm(users) ** 2\n",
    "                       + torch.norm(pos_items) ** 2\n",
    "                       + torch.norm(neg_items) ** 2) / 2\n",
    "        \n",
    "        emb_loss = regularizer / 1000\n",
    "        # emb_loss = self.decay * regularizer / self.batch_size  (최종)\n",
    "        \n",
    "        mf_loss = torch.nan_to_num(mf_loss)\n",
    "        emb_loss = torch.nan_to_num(emb_loss)\n",
    "        \n",
    "        return mf_loss+emb_loss, mf_loss, emb_loss\n",
    "            \n",
    "    \n",
    "    def forward(self, users, pos_items, neg_items, adj_matrix, iteration):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.iteration = iteration\n",
    "        coef = torch.zeros(self.n_users, self.n_items)\n",
    "        \n",
    "#         adj_matrix = sp.load_npz(path + '/s_adj_mat_' + str(iteration+1) +'.npz').toarray()\n",
    "#         adj_matrix = torch.Tensor(adj)\n",
    "        \n",
    "        user_emb = self.embedding_dict['user_emb']\n",
    "        item_emb = self.embedding_dict['item_emb']\n",
    "        \n",
    "        # self.W의 크기를 정확하게 확인해야함, 최종적인 각 node의 embedding size 는 1 x emb_size 로 나오게끔\n",
    "        # score는 각각 user-item 간의 attention score\n",
    "        score = torch.exp(torch.inner(user_emb, item_emb))\n",
    "        score = torch.multiply(score, adj_matrix)\n",
    "        \n",
    "        for i in range(score.shape[0]):\n",
    "            norm = sum(score[i])\n",
    "            normalized_score = score[i] / norm\n",
    "            coef[i] = normalized_score\n",
    "        # coef에다가 normalized된 최종적인 attention score 저장\n",
    "        w_1 = self.weight_dict['W_1']\n",
    "        w_2 = self.weight_dict['W_2']\n",
    "        # h_tilde_1은 w_1과 곱해지는 부분의 embedding (e_k)를 user와 item으로 쪼개서 계산후 concat\n",
    "        h_tilde_1_user = torch.matmul(coef, item_emb)\n",
    "        h_tilde_1_user = torch.matmul(h_tilde_1_user, w_1)\n",
    "        h_tilde_1_item = torch.matmul(coef.T, user_emb)\n",
    "        h_tilde_1_item = torch.matmul(h_tilde_1_item, w_1)\n",
    "        # h_tilde_1 = torch.cat((h_tilde_1_user, h_tilde_1_item), dim=0)\n",
    "        \n",
    "        neighbor_num_user, neighbor_num_item = torch.zeros(self.n_users), torch.zeros(self.n_items)\n",
    "        for i in range(self.n_users):\n",
    "            neighbor_num_user[i] = sum(adj_matrix[i])\n",
    "        for j in range(self.n_items):\n",
    "            neighbor_num_item[i] = sum(adj_matrix.T[i])\n",
    "        # neighbor_num은 n_j에 해당하는 부분\n",
    "        \n",
    "        # h_tilde_2는 w_2와 곱해지는 부분\n",
    "        h_tilde_2_user = torch.matmul(adj_matrix, item_emb)\n",
    "        # h_tilde_2_user = torch.matmul(neighbor_num_user, h_tilde_2_user)\n",
    "        # 각 user의 neighbor의 수 만큼 item embedding에 곱해주는 과정\n",
    "        for i in range(neighbor_num_user.shape[0]):\n",
    "            h_tilde_2_user[i] = neighbor_num_user[i] * h_tilde_2_user[i]\n",
    "        h_tilde_2_user = torch.matmul(h_tilde_2_user, w_2)\n",
    "        h_tilde_2_item = torch.matmul(adj_matrix.T, user_emb)\n",
    "        # h_tilde_2_item = torch.matmul(neighbor_num_item, h_tilde_2_item)\n",
    "        # 각 item의 neighbor의 수 만큼 user embedding에 곱해주는 과정\n",
    "        for j in range(neighbor_num_item.shape[0]):\n",
    "            h_tilde_2_item[i] = neighbor_num_item[i] * h_tilde_2_item[i]\n",
    "        h_tilde_2_item = torch.matmul(h_tilde_2_item, w_2)\n",
    "        # h_tilde_2 = torch.cat((h_tilde_2_user, h_tilde_2_item), dim=0)\n",
    "        \n",
    "        user_h_tilde_sampled = torch.cat((h_tilde_1_user, h_tilde_2_user), dim=1)\n",
    "        item_h_tilde_sampled = torch.cat((h_tilde_2_item, h_tilde_2_item), dim=1)\n",
    "        \n",
    "        pos_item_h_tilde_sampled = item_h_tilde_sampled[pos_items,:]\n",
    "        neg_item_h_tilde_sampled = item_h_tilde_sampled[neg_items,:]\n",
    "        \n",
    "        print('user embedding shape : ',user_h_tilde_sampled.shape)\n",
    "        print('positive item embedding shape : ',pos_item_h_tilde_sampled.shape)\n",
    "        print('negative item embedding shape : ',neg_item_h_tilde_sampled.shape)\n",
    "        \n",
    "        print(neg_item_h_tilde_sampled)\n",
    "        \n",
    "        # h_tilde_observed \n",
    "        \n",
    "        # h_tilde_sampled를 user embedidng / pos, neg item embedding들로 쪼개야함\n",
    "        return user_h_tilde_sampled, pos_item_h_tilde_sampled, neg_item_h_tilde_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c44826b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 3953\n",
      "user embedding shape :  torch.Size([1000, 128])\n",
      "positive item embedding shape :  torch.Size([1000, 128])\n",
      "negative item embedding shape :  torch.Size([1000, 128])\n",
      "tensor([[-0.0239, -0.0164,  0.4286,  ..., -0.1929, -0.0031, -0.3903],\n",
      "        [-0.1558,  0.0764,  0.1468,  ...,  0.1076, -0.0265, -0.1088],\n",
      "        [ 0.1481, -0.0580, -0.0046,  ...,  0.2016, -0.0496, -0.0786],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0391,  0.0742, -0.0331,  ...,  0.0794, -0.0956,  0.0973]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "pos_scores :  tensor([-1.3261e+01, -8.3256e+01, -6.5596e+01, -7.4099e+00, -1.0302e+02,\n",
      "        -9.8130e+00,  4.7828e+00, -2.4349e+02,  1.0465e+02, -1.2730e+03,\n",
      "        -1.6656e+01,  7.9462e+00,  2.0100e+01, -1.0578e+01,  1.4955e+02,\n",
      "        -7.4379e+00, -4.3382e+02, -2.8916e+02, -2.0063e+01, -4.6998e+00,\n",
      "         2.1722e+00, -1.0488e+03, -1.7184e+02, -1.0824e+01, -5.0323e+01,\n",
      "         6.0198e+02, -5.0422e+01, -8.6094e+01, -1.9417e+01, -1.6345e+01,\n",
      "        -1.2471e+02,  9.6601e+00, -3.4684e+02,  1.6637e+02,  2.8589e+02,\n",
      "         1.9847e+02, -4.3285e+01,  3.1770e+01, -4.6643e+00, -1.9029e+01,\n",
      "        -1.0586e+00,  1.3663e+02,  1.3453e+00, -3.6269e+02, -3.0254e+02,\n",
      "         4.1162e+00, -1.4997e+00,  1.2055e+03, -9.4043e+01,  2.7672e+01,\n",
      "        -6.1223e+00, -8.7497e+00, -3.6883e+03,  2.1028e+01,  7.5907e+00,\n",
      "         1.6238e+01, -1.0503e+01, -1.4010e+03, -2.7739e+02,  3.5977e+01,\n",
      "         1.5502e+01,  9.2331e+00, -9.9995e+01, -6.5812e+00, -8.5614e+01,\n",
      "         4.3700e-01, -2.8530e+01, -2.5102e+01, -5.1580e+00, -1.0526e+01,\n",
      "        -1.3755e+00,  2.2935e+01,  6.5715e+02,  2.3949e-01,  1.2317e+02,\n",
      "        -1.0281e+00, -1.0479e+01,  7.8536e+01,  2.2524e+00,  4.1014e-01,\n",
      "        -8.1040e+01, -1.9995e+02, -2.2312e+01, -1.7662e+01,  3.2708e+00,\n",
      "        -7.4765e+00, -5.9053e+01, -5.5939e+01,  1.5658e+01, -6.6140e+01,\n",
      "         5.6415e+00,  4.6395e+01, -3.7237e+02, -5.2888e-01,  9.0789e+00,\n",
      "         2.9119e+01,  2.3076e+01, -2.3852e+01,  1.1044e+01, -4.3835e+01,\n",
      "        -2.0936e+01, -2.8387e+02,  1.2453e+02, -1.2362e+00,  5.3431e+00,\n",
      "        -7.5287e+00,  9.9346e-01, -1.0163e+01, -8.0066e+01, -1.5487e+01,\n",
      "         1.7088e+01, -8.9433e+00,  4.1484e+00, -3.0800e+01,  8.8269e+00,\n",
      "        -5.8414e+01,  9.0916e+00,  2.5684e+02, -2.6182e+01,  2.0135e+01,\n",
      "        -1.2442e+02, -1.6758e+01,  2.9049e+02,  1.8592e+01, -1.2820e+02,\n",
      "         1.1872e+01, -5.9595e+01, -1.8563e+01, -6.4706e+01, -3.4141e+01,\n",
      "         2.8702e+02,  1.8690e+01,  9.4045e+01, -3.7233e+01,  4.6349e+01,\n",
      "         6.1559e+02, -9.4219e+02,  4.0287e+00, -5.5669e+02, -2.6019e+01,\n",
      "        -4.3023e+00,  2.0625e+01, -1.5413e+01, -1.5392e+01,  4.2218e+00,\n",
      "         2.6080e+02,  1.5845e+02, -9.0320e+02, -2.7545e+02, -3.0618e+02,\n",
      "        -5.5862e+02,  6.3540e+00,  4.7104e+00, -4.8044e+01,  2.0593e+01,\n",
      "        -5.9962e+01, -2.4134e+02,  9.6319e+00,  2.0988e+01,  3.2588e+00,\n",
      "        -7.5892e+01, -2.6955e+01, -5.8324e+02,  9.0064e-01, -2.3399e+01,\n",
      "        -1.0344e+02,  1.0535e+02, -1.1210e+01, -1.4674e+01,  1.6466e+01,\n",
      "         5.7687e+00, -9.7915e+00, -6.2722e+02, -4.3389e+01, -3.8426e+02,\n",
      "        -4.6821e+01, -1.0131e+01,  3.7171e+01,  1.5190e+02, -3.5182e+01,\n",
      "        -3.5714e+01, -5.7764e+00,  1.9453e+01, -9.7927e+00, -5.2686e+01,\n",
      "        -1.2191e+02, -9.2743e+02,  1.0089e+01,  1.0557e+00, -5.2196e+01,\n",
      "         3.1980e-02, -1.1231e+03,  1.5979e+02,  1.2511e+01, -1.2029e+03,\n",
      "         8.3736e+00,  1.3678e+00,  4.7917e+02, -1.0556e+02, -7.3062e-01,\n",
      "         1.5405e+01, -8.6123e+02, -2.0518e+01,  3.1205e+02, -2.2649e+01,\n",
      "        -3.2385e+00,  5.1933e+00,  2.3999e+02,  2.5792e+00, -1.0659e+02,\n",
      "        -9.5904e+00,  1.3968e+01, -4.1839e+00, -1.8237e+02,  9.8791e-01,\n",
      "         1.4041e+03, -2.4589e+00,  3.7755e+01, -5.1660e+00, -7.8342e+01,\n",
      "         3.9652e-01,  3.3207e+01,  1.1261e+02, -1.6288e+02, -1.8523e+02,\n",
      "        -4.1332e+00, -1.5509e+01,  2.3621e+01, -8.6276e+01,  3.8485e+01,\n",
      "        -2.7534e+01,  8.8417e+01, -2.4078e+01, -5.5521e+00,  9.6295e+01,\n",
      "        -3.1528e+01, -6.6034e+01,  2.2974e+01, -4.0980e+01, -1.4450e+02,\n",
      "        -6.5237e+00,  4.0586e+01,  2.2822e+01, -4.1888e+01, -6.9301e+01,\n",
      "        -1.9704e+00, -4.5666e+00,  1.2473e+01, -3.8731e+01,  4.8964e+00,\n",
      "        -2.6311e+01,  2.0156e+01, -3.5978e+01,  1.8040e+00,  2.1240e+02,\n",
      "        -2.0871e+02,  3.2389e+01, -2.3650e+01,  5.5300e+01, -1.7707e+02,\n",
      "        -1.2913e+03, -7.6594e+01,  9.4477e+00, -1.6522e+03,  3.2277e+00,\n",
      "         1.1307e+01, -5.2045e+01,  3.1603e+02, -5.5485e+00, -9.8513e+01,\n",
      "        -4.6830e+01, -2.1409e+02, -5.6527e+01, -1.4543e+01,  1.0761e+02,\n",
      "        -2.2547e+01,  5.8754e+00,  1.3380e+02, -2.2563e+00,  3.5483e+00,\n",
      "         1.9220e+02,  1.4688e+01, -4.7764e+01,  1.5516e+00,  9.1329e+00,\n",
      "        -1.9915e+01, -7.6734e+00, -4.8173e+01, -4.3723e+01, -1.9243e+00,\n",
      "        -2.6417e+01, -1.0345e+01,  7.9709e+01,  1.7839e+00,  1.2478e+02,\n",
      "        -5.1344e+01, -2.7037e+01,  3.0497e+00, -1.3650e+02,  3.9364e+02,\n",
      "         1.1201e+02,  1.2264e+03,  1.9144e+02,  1.8154e+01, -3.8732e+02,\n",
      "         1.2684e+02,  1.1584e+02, -1.5228e+03, -8.8993e+01,  9.8774e+01,\n",
      "        -2.0476e+01,  6.9137e+00, -2.4473e+00, -6.0119e+01, -1.9479e+01,\n",
      "         2.0054e+01, -1.3936e+01,  5.7507e+00, -1.1386e+03, -1.7442e+00,\n",
      "        -2.4367e+02, -1.9140e+01,  3.7888e+01, -1.0527e+01, -9.1119e+00,\n",
      "        -1.2800e+02, -9.8944e+01, -2.8772e+01, -2.7729e+02, -7.0755e+00,\n",
      "        -1.3816e+03,  1.3494e+01,  4.0353e+01,  7.6872e+01,  4.0235e+01,\n",
      "        -1.7709e+02,  8.3288e+01, -1.7482e+02,  1.4187e+02,  1.9711e+00,\n",
      "        -1.3457e+01,  1.0785e+01, -1.9114e+01,  2.7487e+00, -8.6275e-01,\n",
      "        -7.9696e+01, -5.6300e+01, -4.7727e+01, -3.5759e+02, -6.7478e+01,\n",
      "         1.1340e+01, -5.5603e+02,  8.0880e+00,  2.8532e+01,  1.2784e+03,\n",
      "        -1.6169e+00,  1.7848e+01, -7.3317e+01,  6.3727e+01, -2.2949e+01,\n",
      "        -6.2928e+00, -2.1647e+01, -2.7957e+01, -4.2125e+00, -3.0422e+01,\n",
      "        -3.1426e+02, -4.0562e+00,  5.3676e+02, -2.2337e+01, -5.0542e+01,\n",
      "        -2.8276e+01, -5.1989e+02, -4.3160e+00,  7.0148e+00, -9.8001e+00,\n",
      "        -7.2133e+01, -2.5183e+02,  1.0892e+01, -3.6781e+01, -1.9124e+01,\n",
      "        -2.5042e+01,  6.7478e+00,  2.9755e+00, -1.5061e+01, -3.2666e+01,\n",
      "        -1.3560e+02,  8.9346e-01, -1.3946e+01,  2.1453e+01,  6.7000e+01,\n",
      "        -9.9115e+01,  3.8982e+02,  4.9195e+01, -4.1914e+00,  2.9467e+01,\n",
      "         4.3621e+01,  7.1644e+00,  5.7045e+01,  2.0956e+01, -1.0159e+01,\n",
      "         5.4059e+01,  1.7702e+02, -5.7846e+02, -1.7520e+01,  7.5327e+00,\n",
      "         1.9775e+02,  2.4790e-01, -2.3644e+02,  9.1500e+01,  1.5970e+01,\n",
      "        -5.3644e+02,  4.7927e+02, -2.9262e+01, -5.7447e+01,  4.0470e+02,\n",
      "         5.3217e+01, -4.9613e+00, -1.5396e+01, -3.4874e+01,  1.6909e+01,\n",
      "        -6.3967e+00, -4.1167e+00, -1.2080e+01, -3.1033e+03, -9.9652e+01,\n",
      "        -3.8740e+02, -1.4157e+01,  1.4958e+01, -7.0546e+02,  1.3219e+01,\n",
      "        -2.2697e+02, -1.2278e+02,  2.8253e-01, -3.2309e+00, -1.9904e+02,\n",
      "        -2.0530e+02, -4.7801e+01,  3.5237e+02,  3.2761e+01,  1.8208e-01,\n",
      "        -5.0941e+01, -2.5085e+03, -2.1877e+01, -2.6884e+01,  1.9621e+02,\n",
      "         7.6426e+00,  2.2223e+01, -1.0430e+01, -1.6351e+01,  2.2867e+02,\n",
      "         1.6135e+02,  6.9403e+01, -2.7744e+02, -2.4670e+02,  8.2327e-01,\n",
      "        -2.1426e+02, -9.2095e+01,  2.2884e+01,  3.2552e+01,  6.4764e+00,\n",
      "        -8.0228e+02, -1.4421e+02, -1.5088e+02, -2.2391e+01, -1.9490e+01,\n",
      "        -5.9883e+02,  3.9090e+01, -2.0341e+01, -2.4193e+01,  2.8997e+00,\n",
      "         1.5126e+01, -1.2554e+01,  6.7318e+01,  8.1199e+01, -3.0947e+01,\n",
      "         5.7860e+02, -4.0261e+01, -2.2783e+01,  3.9318e+01, -1.6142e+01,\n",
      "        -3.1371e+02,  4.9370e+03, -1.4911e+01,  2.2237e+01,  1.8218e+01,\n",
      "         1.7714e+01,  6.6880e+00, -1.6128e-01,  1.6770e+01,  1.7636e+02,\n",
      "        -1.4234e+00, -5.8967e+01, -4.2401e+01,  2.1716e+02, -5.2436e+01,\n",
      "         1.4283e+02, -3.2907e+00,  1.4888e+02, -1.5780e+02, -8.0707e+01,\n",
      "         2.1111e+01, -5.9911e+00,  3.8834e+00, -1.7660e+01, -1.3483e+01,\n",
      "         5.3436e+00,  8.7212e+00, -1.1159e+02, -1.2789e+03, -3.3295e+01,\n",
      "         1.2735e+01,  9.2710e+00,  8.7241e+00,  3.0909e+01,  1.0970e+02,\n",
      "        -8.9624e+01, -2.2297e+01,  8.5035e+02, -3.0094e+00,  4.1405e+01,\n",
      "        -6.9441e+00, -6.6620e+00, -3.2822e+01, -6.2302e+03, -7.8732e+00,\n",
      "         4.7104e+00, -2.4984e-01, -9.5955e+02, -1.0871e+02, -8.1744e+01,\n",
      "        -3.0316e+03,  2.4314e+00, -6.4218e+02,  3.0453e+00, -7.8931e+00,\n",
      "        -1.3393e+02, -1.0074e+01, -6.1184e+00, -9.9164e+01,  1.3045e+01,\n",
      "        -4.3028e+01,  2.3568e+00, -1.8019e+03,  6.9505e+00, -1.6565e+01,\n",
      "         1.3171e+02,  3.5035e+01,  9.4790e+00, -1.4982e+03, -5.7908e+02,\n",
      "        -8.4773e+01, -8.5423e+01,  7.7978e+00,  3.0734e+00, -1.1973e+01,\n",
      "        -5.2799e+02, -1.4385e+01, -5.9765e+02,  2.8460e+01, -1.0261e+02,\n",
      "        -2.4804e+01,  1.9142e+00, -2.4303e+02, -3.3747e+01, -7.5248e+00,\n",
      "        -4.3858e+02, -1.4686e+00,  1.2793e+01, -1.0125e+02, -3.0135e+01,\n",
      "        -5.2140e+01,  5.4687e-01,  6.9065e-01, -1.4501e+01, -6.9653e+01,\n",
      "         6.5012e+01, -3.1796e+01,  1.2702e+00, -3.9369e+00,  4.8275e+00,\n",
      "         1.7910e+01,  1.7680e+00, -5.9062e+01,  4.7169e-01, -1.1889e+02,\n",
      "         3.6633e+00, -1.0757e+03, -3.0692e+02,  7.4494e+00, -1.1720e+01,\n",
      "        -1.4269e+02,  1.0591e+01,  6.3188e+01, -1.4748e+01,  5.0691e+01,\n",
      "         1.5794e+01,  8.7903e+00, -2.1426e+00,  6.0863e-01,  1.7503e+00,\n",
      "         6.5423e+01, -2.6025e+01, -3.5899e+01,  6.0519e+02, -3.6425e+01,\n",
      "        -2.9778e+01,  1.6946e+01, -7.3500e+01,  4.0692e+01, -4.7095e+01,\n",
      "        -1.4525e+02,  5.5600e+01,  3.3683e+01,  3.3144e+00, -1.5747e+01,\n",
      "         4.9877e+01,  3.2139e+00, -2.6032e+01,  3.6021e+00, -7.2081e+01,\n",
      "        -1.5669e+02,  3.7420e+01, -1.4718e+02, -7.0883e+02,  1.9547e+01,\n",
      "        -4.3961e+01, -3.2208e+00, -5.5492e+00, -3.5688e+02, -5.9048e+01,\n",
      "        -5.9511e+02,  3.5061e+00,  2.4704e+01, -2.0105e+02,  1.5697e+01,\n",
      "         7.8826e+00,  1.9542e+02, -6.4407e+00,  4.9320e+02,  2.5598e+00,\n",
      "        -1.7638e+02, -5.4032e-01,  5.0499e+00, -6.5496e+01, -5.0022e+01,\n",
      "        -3.3723e+02,  4.8726e+00,  1.0934e+01,  8.5558e+00, -7.5385e+00,\n",
      "        -4.0048e+02, -9.8478e+01, -2.3853e+01,  1.2621e+01, -5.1721e+02,\n",
      "         7.2023e+01,  2.0914e+01, -3.5284e+01, -2.0166e+01, -1.9760e+03,\n",
      "         2.1100e+01, -1.2370e+02, -1.3136e+02,  5.7072e+00, -1.7978e+01,\n",
      "         5.3953e+01, -1.4422e+02,  5.2024e+00, -7.0969e+02, -8.0826e+01,\n",
      "         1.1790e+02, -3.9667e+01, -4.0108e+02, -3.0801e+01, -5.9025e+00,\n",
      "        -1.9588e+03, -1.2408e+02, -2.1782e+02,  4.3414e+00,  5.3609e+00,\n",
      "         5.2240e+00, -1.2508e+02,  9.5311e+00,  7.3691e+00,  4.9724e+01,\n",
      "        -1.9677e+01, -6.6107e+01, -3.6934e+00,  2.7534e+01,  2.2369e+02,\n",
      "         3.3180e+01, -2.2729e+03, -1.3258e+01, -1.5649e+00, -1.5591e+02,\n",
      "        -1.5868e+03, -4.7078e+00, -4.7281e+02, -1.0306e+03, -1.3475e+01,\n",
      "         1.4583e+02,  2.3885e+02,  4.0037e+00, -4.2339e+01,  2.4051e+01,\n",
      "         3.6402e+01,  1.0064e+01,  1.3529e+00, -8.0992e+01, -1.1588e+02,\n",
      "        -3.1399e+01, -4.7664e+01, -4.0021e+02, -1.6585e+02, -2.8401e+01,\n",
      "        -4.1263e+02,  1.7212e+01, -8.2560e+01,  8.9608e-02,  1.2226e+01,\n",
      "        -1.7100e+03, -3.6916e+01, -2.1406e+01,  6.7534e+01, -3.2102e+01,\n",
      "         1.8543e+01, -2.4588e+01, -2.1553e+02, -8.2347e+01,  8.5291e+00,\n",
      "         5.6128e+03,  6.6119e+01, -1.2985e+02, -5.4840e+00, -7.8750e+01,\n",
      "        -1.0382e+02, -3.6412e+02,  1.2081e+01,  6.0805e+01, -1.5303e+01,\n",
      "        -1.7037e+01, -1.8870e+01, -7.4822e+00, -1.5673e+02, -7.7890e+00,\n",
      "        -2.4561e+02, -1.4055e+01, -3.7445e+01, -9.2373e+02,  1.3314e+01,\n",
      "         1.0603e+01, -4.6279e+03, -1.2950e+01, -2.9567e+01, -1.0721e+02,\n",
      "        -3.8543e+01, -8.7464e+02,  1.5851e+01,  1.9844e+00,  6.0823e+00,\n",
      "         2.6131e+00, -5.4185e+01, -8.6282e+00, -3.0853e+01, -4.4559e+02,\n",
      "         2.0224e+01, -1.0988e+01,  2.3546e+01,  8.2449e+00, -8.0009e+02,\n",
      "        -1.7698e+01, -1.3015e+02, -4.2868e+00, -5.9119e+02,  2.3340e+00,\n",
      "        -8.3721e+01,  1.2620e+02,  5.4990e+01, -2.6180e+01, -2.6079e+03,\n",
      "         3.3725e+02,  1.4478e+01,  3.8772e+01, -1.3445e+01, -1.3233e+02,\n",
      "        -3.1325e+01, -3.4874e+01, -1.1565e+02,  4.5524e+01,  1.3796e+01,\n",
      "        -1.4436e+03,  1.6769e+02,  4.2003e+01,  8.3205e+00, -4.8706e+00,\n",
      "        -4.4521e+02,  3.7838e+00,  2.7306e+02,  6.8242e+00, -1.5718e+02,\n",
      "        -4.7642e+02, -6.4646e+02,  8.9105e-01, -7.1144e+01, -3.3366e+01,\n",
      "        -1.1862e+02, -5.9846e+01,  3.3055e+01, -3.8473e+01, -2.4615e+02,\n",
      "        -7.5595e-01,  9.6811e+01, -4.4617e+01, -2.2424e+01,  1.5896e-01,\n",
      "         1.3722e+02,  1.9869e+02,  8.6848e+00, -1.0588e+00, -4.8640e+01,\n",
      "         1.2939e+00,  4.0192e+02, -1.9658e+01, -3.3173e+01,  2.6092e+01,\n",
      "        -1.2945e+01, -3.5425e+00,  4.2895e+01,  4.7716e+01, -2.5197e+01,\n",
      "        -3.9199e+01, -6.2074e+01, -3.0977e+00, -1.3184e+01,  1.0237e+01,\n",
      "         2.3261e-01,  8.1368e+01, -4.2631e+01, -2.1509e+02, -4.0742e+02,\n",
      "         1.4953e+01, -7.1768e+00,  2.2284e+02, -7.8563e+01,  6.0636e+00,\n",
      "        -1.3761e+02,  8.9791e+00,  2.1993e+01, -3.3719e+01, -8.3883e+01,\n",
      "         4.5647e+01, -5.1498e+00,  1.3243e+01,  2.9552e+02,  6.3637e+02,\n",
      "        -4.5343e+01, -2.1882e+01, -1.2330e+02, -1.9454e+02, -6.8805e+00,\n",
      "        -5.0333e+02,  2.2908e+01, -4.8039e+01, -7.5102e+01, -9.9883e+00,\n",
      "         4.3634e+00,  3.5698e+01, -3.8946e+01,  5.5807e+02, -1.0369e+00,\n",
      "        -1.2644e+01, -1.4936e+02, -1.7564e+02, -1.8440e+00, -2.0944e+01,\n",
      "        -9.4982e+01,  6.0608e+02, -9.5491e+00, -1.8587e+01,  1.9066e+01,\n",
      "        -8.4233e+02, -7.3587e+00, -3.4853e+00, -2.2691e+01, -8.0900e+01,\n",
      "        -4.3870e+00, -9.2931e+01, -4.3100e+01, -1.0278e+03, -9.7582e+01,\n",
      "        -4.3792e+02, -3.2947e+01,  1.9663e+01, -1.4837e+02, -1.4104e+01,\n",
      "         7.6139e+01,  1.1143e+01,  5.3058e+00, -6.2284e+02, -5.4560e+00,\n",
      "        -6.2505e+00, -8.9693e-01,  2.8384e+01,  1.6288e+02,  8.2414e+00,\n",
      "         5.5296e+00, -3.0273e+00,  2.1162e+01, -7.7577e+01,  2.1742e+02,\n",
      "        -1.4055e+03,  1.6030e+00,  1.9820e+01, -3.9699e+00,  1.4456e+02,\n",
      "        -5.8604e+01,  4.5563e+00, -3.3781e+00,  7.4614e+01,  1.3434e+02,\n",
      "        -1.0646e+02, -3.9632e+02,  2.8740e+01, -8.8479e+01,  7.0407e+01,\n",
      "         9.7403e+00,  2.0308e+02,  3.1536e+01,  5.5159e+01,  1.0761e+01,\n",
      "         2.4767e+01, -4.8021e+00,  5.2452e+00, -2.0145e+02,  2.2701e+02,\n",
      "        -7.9089e+01, -3.0640e+02,  2.7261e+00, -1.8907e+02, -1.1547e+01,\n",
      "        -1.3114e+01, -6.1103e+00,  5.4858e+00,  1.4021e+01, -1.1156e+02,\n",
      "        -9.9186e-01, -1.3054e-01, -1.5815e+03, -4.1656e+02, -3.5199e+01,\n",
      "        -4.5914e+00,  2.7055e+02, -2.4243e+02, -1.8729e+02, -8.7044e+02,\n",
      "        -2.0297e+02, -5.7551e+02, -2.5610e+01, -1.0638e+02,  5.3135e+01,\n",
      "         6.1028e+01,  8.9007e+00,  1.4014e+01, -1.6589e+01,  3.2396e+01,\n",
      "        -9.5754e+01, -1.7747e+02,  3.1767e+01,  9.6412e+00, -1.5665e+02,\n",
      "        -1.1804e+01, -3.4449e+00, -3.3337e+02,  1.6378e+01,  1.6229e+02,\n",
      "         6.2500e+00, -5.9783e+02, -4.7743e+01, -5.7824e+00,  4.5136e+01,\n",
      "         1.3537e+01, -3.6621e+01, -6.9137e+01, -5.3626e+02, -1.9144e+02,\n",
      "         5.4394e+00, -2.2698e+01, -4.9756e+01,  7.2559e+00, -5.8051e+01,\n",
      "        -8.5026e+01, -1.7867e+01, -5.4020e+01, -2.1914e+00, -1.7245e+01,\n",
      "        -1.3550e+02, -1.2792e+01,  6.2258e+01, -2.4393e+01,  6.2842e+01],\n",
      "       grad_fn=<SumBackward1>)\n",
      "neg_scores :  tensor([ 2.1660e+01,  4.3315e+00, -1.0297e+01,  1.1139e+00,  4.1559e+01,\n",
      "        -2.8793e+01, -1.1344e+01,  8.9002e+00,  1.1418e+00,  1.7391e+02,\n",
      "         4.6904e+00,  3.3803e-01, -6.5022e+01,  2.8687e+00,  6.1544e+01,\n",
      "         0.0000e+00,  0.0000e+00, -1.5668e+02,  4.4123e+01, -1.0022e+01,\n",
      "         1.8662e+00, -8.7497e+02,  0.0000e+00, -3.1226e+01, -1.2256e+01,\n",
      "        -2.1279e+02, -1.5831e+01,  2.3102e+01, -1.5007e+01,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -7.0392e+01,  5.4492e+01,  2.2140e+01,\n",
      "        -7.5000e+02, -4.8882e-01,  0.0000e+00,  7.5360e+00, -3.4034e+01,\n",
      "        -7.9171e-01, -9.2184e+01,  7.5270e-01,  7.5993e+01,  5.4536e+00,\n",
      "         1.9807e+01, -4.8576e+00,  6.4375e+02,  0.0000e+00, -4.8172e+00,\n",
      "         1.4187e+01, -2.8730e+01, -6.0409e+02, -3.6976e+00,  2.7108e-01,\n",
      "        -3.3195e+00,  8.7190e+00,  2.8901e+02, -3.1159e+01,  5.1812e+01,\n",
      "        -9.5279e+00,  2.5204e+02, -7.9300e+01, -9.1428e+00,  0.0000e+00,\n",
      "        -7.1970e-01,  3.5085e+01, -8.5883e+00,  8.9648e-01,  1.4261e+01,\n",
      "         0.0000e+00,  2.6578e+00, -2.0220e+01,  0.0000e+00,  5.9824e+00,\n",
      "         1.0801e+01, -5.2946e+00,  1.0486e+02,  1.9172e+00,  4.9028e+00,\n",
      "        -8.0495e+00, -1.7612e+01,  0.0000e+00, -6.6514e+00, -4.1795e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.7855e+01, -1.1801e+00, -2.8263e+02,\n",
      "         3.2474e+01, -1.1194e+03, -5.7201e+00,  6.1736e-01, -4.5382e-01,\n",
      "         1.7183e+01,  0.0000e+00, -1.3427e+01,  0.0000e+00,  2.9489e+00,\n",
      "        -6.0090e+01, -4.3645e+01,  1.0159e+01,  2.6180e+00, -8.4878e+01,\n",
      "         2.5243e+00, -1.1207e+02,  0.0000e+00, -1.9399e+01,  0.0000e+00,\n",
      "         1.8608e+01, -5.9096e+00, -1.7991e+01, -6.7524e+01,  3.0534e+00,\n",
      "         9.5686e+00, -3.3813e+02,  3.5473e+01, -4.1191e+01, -1.7121e+01,\n",
      "         2.2493e+01,  5.0771e+00, -5.9808e+02,  7.7253e-01, -1.9975e+00,\n",
      "         1.5637e+00,  0.0000e+00,  0.0000e+00, -4.8963e+00,  0.0000e+00,\n",
      "         3.7540e+00,  7.4954e+00,  1.0961e+02, -1.0457e+02,  1.8187e+01,\n",
      "        -7.4526e+02, -1.0351e+02, -9.6656e+00, -6.4641e+01,  5.2724e+00,\n",
      "         5.2007e-01,  2.9771e+01,  1.0481e+01,  0.0000e+00, -9.2927e+00,\n",
      "         0.0000e+00, -1.6324e+02,  1.8508e+03, -7.8124e+02,  4.7669e+01,\n",
      "         1.5865e+02,  0.0000e+00,  5.8764e+00,  6.6769e+00,  2.6424e+01,\n",
      "        -4.2004e+01, -3.9669e+02, -3.0011e+00,  3.3453e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0601e+00,  0.0000e+00, -3.1840e+00,  0.0000e+00,\n",
      "        -2.5474e+02, -6.3750e+00, -1.7122e+00,  4.6795e+01, -1.2482e+01,\n",
      "         2.3545e+00,  7.0071e-01, -3.7193e+02,  0.0000e+00, -9.5772e+00,\n",
      "         2.8736e+01, -3.9442e+01, -1.4082e+01,  7.9479e+00, -3.6988e+00,\n",
      "         3.9761e+01,  0.0000e+00, -4.5093e+01, -5.3184e+00,  9.6493e+00,\n",
      "         0.0000e+00,  4.5283e+02,  0.0000e+00,  2.1432e+00, -2.9939e+01,\n",
      "         0.0000e+00, -2.7218e+02,  2.5506e+01, -3.6962e+01,  1.1707e+03,\n",
      "         0.0000e+00, -9.7160e+00, -7.7089e+01, -8.7480e+02, -2.0273e+00,\n",
      "         8.2204e+01, -4.3234e+02,  1.1450e-01, -1.3368e+02, -3.6367e+01,\n",
      "        -7.8275e-01, -1.8572e+00,  1.4077e+01,  9.7503e-01,  2.6075e+00,\n",
      "        -8.1207e+00,  1.5771e+01,  7.3331e+00,  1.8654e+02, -1.7718e-01,\n",
      "        -1.4989e+03,  1.2211e+00,  3.4389e+01,  4.1935e+00,  1.6964e+00,\n",
      "         0.0000e+00, -1.4062e+01,  2.4386e+01,  0.0000e+00, -1.9984e+02,\n",
      "        -4.2901e+00, -1.1073e+01, -2.6820e+00,  0.0000e+00,  8.0430e+01,\n",
      "         1.5675e+01, -1.4074e+01,  5.4249e+00,  0.0000e+00,  1.4610e+03,\n",
      "         0.0000e+00,  1.8055e+01,  0.0000e+00,  0.0000e+00, -1.5681e+02,\n",
      "         1.4361e+01,  1.3625e+02, -2.2310e+00,  0.0000e+00, -5.5517e+02,\n",
      "        -2.4977e+00,  0.0000e+00, -1.1111e+00, -1.9059e+01, -2.7723e+00,\n",
      "         3.4930e+00,  2.2954e+00, -3.2527e+00, -4.3319e+00, -5.6584e+01,\n",
      "         2.0748e+00, -6.7616e+00,  0.0000e+00, -7.3304e+01,  0.0000e+00,\n",
      "         0.0000e+00, -2.1996e+01, -2.0110e+01,  6.1233e+02,  4.0410e+01,\n",
      "        -9.9895e-01,  4.1469e+01,  0.0000e+00, -1.1535e+01,  0.0000e+00,\n",
      "         0.0000e+00,  1.0426e+03, -5.2749e+01,  2.9899e+01, -1.7241e+01,\n",
      "         1.5482e+00,  8.6974e+00,  7.2504e+01, -5.3152e+01,  0.0000e+00,\n",
      "        -3.8765e+01, -3.2513e+00, -1.1804e+01,  3.1506e+01, -2.3992e+01,\n",
      "         1.1026e+01,  4.6293e+00,  6.6966e+00,  8.1980e+00, -3.1175e+00,\n",
      "         5.0586e+00, -3.4501e-01,  3.9250e+02,  0.0000e+00, -5.5508e+01,\n",
      "         8.6203e+01, -5.5013e+00, -2.4161e+00, -4.8334e+01, -1.1890e+02,\n",
      "         4.2291e+01,  3.1254e+02, -4.1186e+01,  8.3297e-01, -9.3030e+01,\n",
      "         0.0000e+00,  2.0617e+01, -8.0731e+02, -1.1000e+02,  3.5248e+01,\n",
      "         0.0000e+00, -3.0842e-01,  1.2982e+00,  0.0000e+00,  6.2402e-01,\n",
      "        -2.8586e+01, -2.6146e+00,  0.0000e+00,  1.2774e+03, -5.3313e+00,\n",
      "         0.0000e+00,  5.3060e+00,  2.0702e+01,  0.0000e+00, -8.3195e-01,\n",
      "         0.0000e+00,  2.3590e+01, -2.9364e+01, -2.6502e+03, -6.5759e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  6.6317e+00, -1.0790e+02,\n",
      "        -5.1858e+01,  6.5515e+01, -1.4141e+01,  3.4284e+01, -3.7353e+00,\n",
      "         0.0000e+00, -4.5574e+00,  2.7870e+02, -8.0809e+00, -7.7654e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  5.8757e+02, -1.5657e+00,\n",
      "         0.0000e+00, -1.7653e+03,  0.0000e+00, -1.7388e+01, -1.9405e+01,\n",
      "         0.0000e+00, -1.5984e+01,  1.7299e+01,  0.0000e+00, -1.9376e+01,\n",
      "         3.2509e+00,  5.4289e-01, -2.9651e+00,  1.5153e+01,  0.0000e+00,\n",
      "         4.3466e+01,  0.0000e+00, -5.1973e+01, -8.0469e+01, -5.1648e+00,\n",
      "        -3.6327e+01,  0.0000e+00, -1.4476e+00,  7.8648e+00,  1.7672e+00,\n",
      "         4.3416e+01, -9.2966e+01, -1.8758e-01,  1.0612e+01,  1.5353e+01,\n",
      "         1.2007e+01,  1.8699e+01,  8.3724e-01, -8.6280e-02,  0.0000e+00,\n",
      "         1.9120e+02,  1.9727e+00, -5.2592e-01,  8.8259e+00, -2.8635e+02,\n",
      "         1.4841e+01,  2.5894e+02,  1.1814e+01, -3.3245e+01, -1.1730e+00,\n",
      "        -1.1546e+02, -7.4458e-01,  0.0000e+00,  0.0000e+00, -4.8343e+00,\n",
      "        -2.7128e+01,  2.9387e+02,  0.0000e+00, -8.8133e+00,  0.0000e+00,\n",
      "        -1.8788e+02, -9.0179e-01, -1.8132e+01,  5.8993e+01,  1.4311e+01,\n",
      "        -1.0781e+03,  3.7714e+02, -1.2257e+02, -7.1605e+00,  9.1114e+01,\n",
      "         1.6798e+01,  4.1820e+00, -1.4876e+00,  5.7088e+00, -1.8424e+01,\n",
      "         2.9267e+00, -1.0715e+01,  5.1674e+00, -2.6709e+01,  1.1744e+01,\n",
      "         2.5783e+01, -1.8707e+00,  6.5988e+00,  5.0139e+01,  7.5120e+00,\n",
      "        -1.1614e+01, -5.5566e+00,  0.0000e+00,  1.1760e+01, -2.9610e+01,\n",
      "         0.0000e+00,  1.4367e+01, -1.5877e+03,  7.4528e+00,  8.1734e-01,\n",
      "         0.0000e+00, -6.3037e+01,  1.1210e+00,  1.4181e+00,  0.0000e+00,\n",
      "        -2.0903e+01, -2.1107e+00, -1.4703e+01, -2.3238e+00,  1.2432e+02,\n",
      "         0.0000e+00,  4.0648e+00,  0.0000e+00,  1.1961e+02,  1.7740e-02,\n",
      "         0.0000e+00, -2.4530e+01,  2.3278e+00,  2.7489e-01,  2.1172e+01,\n",
      "         0.0000e+00,  1.6757e+00,  7.3230e+01, -1.5613e+00, -5.1798e+00,\n",
      "        -1.1537e+02,  9.4506e+01,  2.3977e-01, -3.1318e+01, -5.3935e+01,\n",
      "        -6.0566e+01,  1.6384e+01,  1.2876e+01, -1.2232e+02, -3.2638e+02,\n",
      "        -1.7724e+03,  1.1248e+01, -1.7051e+00,  3.8225e+00, -2.2190e+01,\n",
      "         1.4036e+02,  0.0000e+00, -9.1005e+00,  3.4518e+00,  1.1930e+01,\n",
      "         2.0555e+01, -9.3442e+00,  1.0054e+00,  9.8019e-01, -4.6885e+01,\n",
      "        -3.2397e-01, -1.6099e+00,  6.8435e+00,  1.1345e+02,  0.0000e+00,\n",
      "         9.2261e+01, -4.8019e-01,  2.9417e+00,  0.0000e+00,  0.0000e+00,\n",
      "         3.4747e+01, -1.2075e-01,  0.0000e+00,  0.0000e+00, -2.1545e+01,\n",
      "        -2.3972e+01, -1.0418e+01, -2.6113e+02, -6.9073e+02, -3.0782e+01,\n",
      "        -3.4560e+00,  1.0284e+00,  6.5239e+01, -1.2585e+00,  0.0000e+00,\n",
      "         4.0746e+01,  0.0000e+00,  0.0000e+00,  4.0107e+01, -5.4277e+01,\n",
      "         2.2958e+01, -5.3471e+01,  2.4300e+01, -1.0034e+03, -1.6991e+00,\n",
      "         4.7826e+00, -5.8634e+00,  8.0632e+01,  0.0000e+00,  0.0000e+00,\n",
      "        -1.4077e+02,  1.2785e+00, -2.2818e+02,  0.0000e+00,  1.5463e+00,\n",
      "        -2.7855e+01,  2.6739e+01, -1.9642e+01,  0.0000e+00, -2.2323e+00,\n",
      "         0.0000e+00,  0.0000e+00, -1.3730e+03,  1.3410e+01,  1.1838e+01,\n",
      "         5.9075e+01, -5.6851e+01, -8.7540e+01,  3.6420e+02,  0.0000e+00,\n",
      "        -8.7867e+00,  0.0000e+00, -2.1592e+01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -2.8455e+01, -1.9238e+02,  0.0000e+00,  3.2342e+00,\n",
      "        -1.9553e+01,  9.1631e-01,  0.0000e+00,  1.7359e+01,  0.0000e+00,\n",
      "         2.4213e+02,  1.6647e+01, -3.2781e+00,  7.3276e+00,  1.6135e+01,\n",
      "        -1.2970e+02, -7.5470e-01,  1.3409e+01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  7.1746e-01,  2.2106e+00,  1.2511e+00,\n",
      "        -4.6582e+00,  0.0000e+00,  0.0000e+00, -3.2953e+00,  9.0835e+01,\n",
      "        -1.0403e+01,  0.0000e+00,  0.0000e+00,  2.3695e+00,  0.0000e+00,\n",
      "         1.6901e+02, -6.6755e+00,  1.7256e+01,  0.0000e+00, -5.6481e+00,\n",
      "        -1.2619e+01,  1.7620e+01,  0.0000e+00, -2.7559e-01, -7.4846e+00,\n",
      "        -4.8673e+02, -6.0790e+01,  2.0310e+01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  3.7075e+00,  3.3571e+01, -2.1710e+01,  3.6765e+01,\n",
      "         0.0000e+00,  1.8209e+01,  0.0000e+00,  7.4503e+00,  3.3777e+00,\n",
      "        -4.0582e+01,  1.0008e+01,  0.0000e+00,  5.6946e-01, -5.4589e+00,\n",
      "        -2.0768e+02, -7.7499e+00, -6.2115e+01,  0.0000e+00, -1.3227e+00,\n",
      "        -2.4991e+01,  1.1159e+00,  0.0000e+00, -2.3392e+01, -1.7915e+01,\n",
      "         2.1403e+02, -5.4845e+00, -1.0810e+01, -1.3944e+02,  6.8301e+01,\n",
      "         3.2097e+00,  1.8763e+02, -2.5128e+01, -1.1007e+02,  2.5753e+00,\n",
      "        -2.4320e+02, -1.6994e+00, -4.7702e+01,  2.8604e+01, -2.6964e+01,\n",
      "         0.0000e+00, -1.1591e+02,  6.2919e+01, -6.5872e+00, -1.1488e+03,\n",
      "         0.0000e+00, -5.7874e-01,  1.4960e+00, -6.9850e+01,  4.7157e+01,\n",
      "         5.7130e+01,  1.0643e+01, -1.6589e+01, -4.5064e+00,  3.6426e+02,\n",
      "        -8.7761e+00,  1.0939e+02,  0.0000e+00, -2.5371e+01, -4.3093e+00,\n",
      "         1.4537e+01, -5.3780e+01,  3.5746e+00, -5.2803e+01,  9.9773e+00,\n",
      "         0.0000e+00, -1.3946e+01,  1.2199e+01,  1.3071e+01, -1.9267e+01,\n",
      "        -4.1537e+01,  7.5988e+01,  3.5955e+02, -7.0378e+01,  5.0072e+00,\n",
      "         6.6391e+00,  0.0000e+00,  1.7649e+00,  0.0000e+00, -1.7521e+00,\n",
      "         0.0000e+00, -2.9569e+02, -2.8876e+00, -5.3546e+01, -1.4818e+02,\n",
      "         6.0055e+01,  0.0000e+00,  2.7985e+01, -1.1693e+01,  0.0000e+00,\n",
      "         7.7674e+01, -3.0939e+00, -1.1911e+02,  1.5726e+02,  7.5328e-01,\n",
      "         2.3495e+00,  4.6005e+02,  0.0000e+00, -2.5885e+01,  1.2656e+01,\n",
      "         1.3917e+01,  1.8689e+00, -1.4444e+00,  1.3627e+01, -2.6008e+01,\n",
      "         0.0000e+00, -5.8068e+02, -2.6673e+02, -7.0410e+02,  0.0000e+00,\n",
      "        -4.0121e+01,  7.5195e+00, -1.1710e+02,  3.0179e-01,  2.3964e+01,\n",
      "        -2.1663e+02,  6.1927e+00,  9.1609e-01, -2.5013e+01, -3.2495e+01,\n",
      "         1.5152e+01,  0.0000e+00,  1.9731e+02,  4.9359e+01,  9.2805e+00,\n",
      "         0.0000e+00,  9.8294e+01, -3.5240e+01,  0.0000e+00, -2.0827e+02,\n",
      "         4.3999e+01,  0.0000e+00,  4.8478e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -2.7471e+00,  5.7514e+00,  4.0715e+01,  1.9985e+02, -3.6617e-01,\n",
      "         0.0000e+00,  9.6989e+00,  0.0000e+00, -1.1795e+01,  2.5120e+01,\n",
      "        -3.2553e+00,  1.4143e+03,  1.9661e+01, -8.0610e+01,  0.0000e+00,\n",
      "         5.2814e+00, -5.4343e+01,  5.6163e+00, -8.4631e+00, -5.1088e+00,\n",
      "         0.0000e+00,  3.0140e+00,  3.3655e+00,  0.0000e+00, -8.2825e+02,\n",
      "         0.0000e+00,  1.0526e+02, -1.3699e+01, -5.8243e-02,  0.0000e+00,\n",
      "        -3.1748e+00, -1.8484e+01,  1.6515e+01,  2.3635e+02, -1.0304e+01,\n",
      "         1.9810e+02,  3.5132e+02, -5.7681e+01,  2.2409e+01, -1.5797e+02,\n",
      "        -2.9973e+02, -1.6446e+00,  0.0000e+00,  0.0000e+00, -4.1973e+01,\n",
      "        -2.1714e+01,  0.0000e+00, -5.2775e+02, -2.8379e+00,  6.1660e-01,\n",
      "        -3.2848e+01, -3.1009e+01, -1.3037e+02,  7.8040e+00,  1.1236e+02,\n",
      "        -2.0553e+01, -3.9357e+00,  0.0000e+00,  1.9346e+00, -1.4102e+02,\n",
      "         1.0847e+02, -7.9103e+02,  0.0000e+00,  1.4980e+00, -2.8923e+01,\n",
      "         8.1763e+00, -7.0067e+01,  6.6149e+01,  1.0636e+00, -1.9862e+02,\n",
      "         0.0000e+00, -1.8478e+01,  2.3188e+01,  0.0000e+00, -4.4273e-01,\n",
      "        -1.2882e+02,  5.0236e+01,  3.4941e+01, -9.4622e+00,  6.6667e+01,\n",
      "         2.4499e+00, -5.5415e+02, -1.6974e+02, -1.5219e+01,  4.3314e+00,\n",
      "         0.0000e+00, -1.2390e+01,  8.9858e+00, -3.9629e+01,  0.0000e+00,\n",
      "         0.0000e+00,  1.7013e-01, -2.4193e+00,  3.6665e+00,  6.1124e+00,\n",
      "        -7.3089e+01,  2.8147e+00,  0.0000e+00,  2.6720e+02,  1.4298e+01,\n",
      "         5.3055e+00,  0.0000e+00, -5.5623e+01, -4.5775e+01, -1.0586e+01,\n",
      "        -2.7766e+01, -5.4163e+00, -2.3721e+00,  5.2261e+01,  5.7711e+02,\n",
      "         0.0000e+00, -1.0233e+00, -3.1647e+01,  0.0000e+00, -1.9300e+02,\n",
      "        -2.7606e+00, -1.4307e+01, -5.5256e+01, -8.6092e+01,  2.7460e+00,\n",
      "        -7.5544e+00,  1.0773e+01,  1.3492e+01, -9.4363e+01, -8.5686e+00,\n",
      "        -8.5504e+00, -8.8223e+00, -4.4849e+00, -1.1393e+03, -1.0315e+00,\n",
      "         5.8189e+00,  4.8467e+01, -9.8733e+01,  1.1303e+00, -3.7197e+01,\n",
      "        -1.0212e+02,  1.2082e+02,  0.0000e+00, -2.0054e+01, -1.6412e+01,\n",
      "        -5.8431e+02, -5.4381e+01,  1.3294e+01, -1.1632e+01, -1.9810e+01,\n",
      "        -2.5088e+00,  4.3848e+00, -2.8234e+01, -3.2036e+02,  3.5431e+02,\n",
      "        -4.6657e+02,  0.0000e+00,  3.3864e+01,  0.0000e+00,  1.6494e+00,\n",
      "        -1.2594e+01, -1.9763e+02,  0.0000e+00, -1.4448e+01,  7.7490e+00,\n",
      "        -3.5089e+00,  0.0000e+00, -2.9065e+01, -5.8362e+01,  2.2856e+01,\n",
      "         0.0000e+00, -5.8257e+00, -1.2367e+00,  0.0000e+00, -2.5194e+01,\n",
      "        -1.5413e+02, -1.9810e+01, -4.7735e+00, -7.1167e+00,  3.7797e+01,\n",
      "         7.6337e-01,  7.9305e+00,  3.7684e+01, -5.9523e+01, -3.1473e-01,\n",
      "        -7.0883e+01, -6.7526e+01,  3.2098e+00, -8.4585e+01,  1.2689e+01,\n",
      "        -3.4759e+01,  1.1174e+00, -1.4214e+01,  1.0748e+02, -4.8573e+01,\n",
      "         3.0154e+01, -1.8778e+00,  1.1146e+00,  4.9547e+01,  6.0262e+01,\n",
      "        -2.4034e+00,  7.3025e+01,  9.9912e-01, -2.7037e+02,  1.4856e+01,\n",
      "        -1.2250e+00,  1.0806e+00, -2.2986e+01, -4.0891e+00, -6.4346e+00,\n",
      "         0.0000e+00, -3.1179e-01,  0.0000e+00, -7.8776e+01,  0.0000e+00,\n",
      "        -4.7428e+00, -9.8855e+00, -1.2739e+02, -1.0282e+02, -6.0285e+02,\n",
      "         2.2548e+01, -3.2957e+01,  0.0000e+00, -7.7873e+01,  4.5139e+00,\n",
      "        -2.3039e+00,  2.7996e+01, -1.0018e+01, -3.6979e+00,  2.9968e+00,\n",
      "         2.4516e+01, -2.4323e+01, -1.3842e+01, -5.1572e-01,  5.4237e+02,\n",
      "        -2.3925e+02,  1.9030e+00, -1.0061e+02,  0.0000e+00,  3.4868e+02,\n",
      "        -1.3807e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.2588e+00,\n",
      "        -1.2897e+02, -1.8919e+00, -3.4192e+01,  9.0691e+01, -5.3669e+01,\n",
      "         0.0000e+00, -1.0106e+00,  0.0000e+00,  3.9023e+00, -3.2117e+01,\n",
      "         1.7381e+01, -1.4954e+00,  6.3542e+00,  1.9927e+00,  2.1125e-01,\n",
      "         0.0000e+00, -3.7588e+00,  0.0000e+00,  0.0000e+00,  4.5470e+00],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(267767.9375, grad_fn=<AddBackward0>) tensor(126.3225, grad_fn=<AddBackward0>) tensor(267641.6250, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pos_items, neg_items = sampled_graph_to_matrix(path='ml-1m', iteration = iteration).divide_pos_neg()\n",
    "# 완성형은 GATLayer(6040, 3953, 64,64, pos_items, neg_items)\n",
    "model = GATLayer(1000, 3953, 64, 64)\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "path = 'ml-1m'\n",
    "for epoch in range(1):\n",
    "    loss, mf_loss, emb_loss = 0., 0., 0.\n",
    "    \n",
    "    sampled_graph = sampled_graph_to_matrix(path = path, iteration = epoch, batch_size=1000)\n",
    "    adj_matrix = sampled_graph.get_adj_mat().toarray()\n",
    "    adj_matrix = torch.Tensor(adj_matrix)\n",
    "    users, pos_items, neg_items = sampled_graph.sample()\n",
    "    \n",
    "    for iteration in range(1):\n",
    "        t1 = time()\n",
    "        \n",
    "        # sampled_graph.get_adj_mat 함수로 바꿔야할듯\n",
    "#         adj_matrix = sp.load_npz(path + '/s_adj_mat_' + str(iteration+1) +'.npz').toarray()\n",
    "#         adj_matrix = torch.Tensor(adj_matrix)\n",
    "\n",
    "        '''불러오는 sampled graph matrix마다 pos, neg item set을 만들고 (함수 사용) \n",
    "        bpr loss 함수에다가 각 item들에 해당하는 embedding을 입력으로 넣어줌'''\n",
    "\n",
    "        # sampled_graph = sampled_graph_to_matrix(path = path, iteration = iteration, batch_size=1000)\n",
    "        # users, pos_items, neg_items = sampled_graph.sample()\n",
    "        u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings = model(users,\n",
    "                                                                       pos_items,\n",
    "                                                                       neg_items,\n",
    "                                                                       adj_matrix,\n",
    "                                                                       iteration)\n",
    "        \n",
    "        batch_loss, batch_mf_loss, batch_emb_loss = model.create_bpr_loss(u_g_embeddings,\n",
    "                                                                          pos_i_g_embeddings,\n",
    "                                                                          neg_i_g_embeddings)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print(batch_loss, batch_mf_loss, batch_emb_loss)\n",
    "        \n",
    "        loss += batch_loss\n",
    "        mf_loss += batch_mf_loss\n",
    "        emb_loss += batch_emb_loss\n",
    "        \n",
    "        print(loss, mf_loss, emb_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "superior-simple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f8825c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3953)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'initializer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2c47a566dc4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0madj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcoef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3953\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0muser_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mitem_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3953\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mw_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initializer' is not defined"
     ]
    }
   ],
   "source": [
    "adj = sp.load_npz('ml-1m/s_adj_mat_1.npz')\n",
    "adj = adj.toarray()\n",
    "print(adj.shape)\n",
    "adj = torch.Tensor(adj)\n",
    "coef = torch.zeros(3,3953)\n",
    "user_emb = nn.Parameter(initializer(torch.empty(1000,32)))\n",
    "item_emb = nn.Parameter(initializer(torch.empty(3953,32)))\n",
    "w_1 = nn.Parameter(initializer(torch.empty(32,32)))\n",
    "w_2 = nn.Parameter(initializer(torch.empty(32,32)))\n",
    "\n",
    "x = torch.exp(torch.inner(user_emb, item_emb))\n",
    "print(x)\n",
    "x = torch.multiply(x, adj)\n",
    "print(x)\n",
    "for i in range(3):\n",
    "    tot = sum(x[i])\n",
    "    score = x[i] / tot\n",
    "    coef[i] = score\n",
    "    \n",
    "w1 = torch.matmul(coef, item_emb)\n",
    "print(w1.shape)\n",
    "w1 = torch.matmul(w1, w_1)\n",
    "print(w1.shape)\n",
    "w2 = torch.matmul(coef.T, user_emb[:3])\n",
    "print(w2.shape)\n",
    "w2 = torch.matmul(w2, w_2)\n",
    "print(w2.shape)\n",
    "w3 = torch.cat((w1, w2), dim=0)\n",
    "w3.shape\n",
    "\n",
    "user_emb = nn.Parameter(initializer(torch.empty(1000,32)))\n",
    "item_emb = nn.Parameter(initializer(torch.empty(3953,32)))\n",
    "avg_emb = torch.matmul(adj.T, user_emb)\n",
    "avg_emb.shape\n",
    "# x = torch.multiply(x, adj)\n",
    "# print(x.shape)\n",
    "# x = F.softmax(x, dim=1)\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3840a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_num(row):\n",
    "    return sum(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumemb = torch.zeros((5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    sumemb[i] = sum(adj[i])\n",
    "    \n",
    "torch.matmul(sumemb, item_emb[:5]).shape\n",
    "# sumemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead80d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = nn.init.xavier_uniform_\n",
    "a = nn.Parameter(initializer(torch.empty(5,5)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646153ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = nn.init.xavier_uniform_\n",
    "user_emb = nn.Parameter(initializer(torch.empty(5,10)))\n",
    "user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.matrix([[0,1,2],[1,1,1],[0,0,1]])\n",
    "adj = np.matrix([[0,0,1],[1,0,0],[1,1,0]])\n",
    "e = np.matrix([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "print(a,'\\n')\n",
    "print(adj,'\\n')\n",
    "print(e,'\\n')\n",
    "\n",
    "a_jk = np.multiply(a,adj)\n",
    "print(a_jk, '\\n')\n",
    "\n",
    "np.matmul(a_jk, e)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80349c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
