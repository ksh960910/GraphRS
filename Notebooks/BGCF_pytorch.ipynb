{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277c185d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "\n",
    "# directory = '../MF/ml-1m'\n",
    "# ratings = []\n",
    "# with open(os.path.join(directory, 'ratings.dat'), encoding='latin1') as f:\n",
    "#     for l in f:\n",
    "#         user_id, movie_id, rating, timestamp = [int(_) for _ in l.split('::')]\n",
    "#         ratings.append({\n",
    "#             'user_id': user_id,\n",
    "#             'movie_id': movie_id,\n",
    "#             'rating': rating,\n",
    "#             'timestamp': timestamp,\n",
    "#             })\n",
    "# ratings = pd.DataFrame(ratings)\n",
    "\n",
    "# ratings = ratings.drop(['timestamp'],axis=1)\n",
    "# print(ratings)\n",
    "# # 어떤 비율로 표본 추출을 하고싶은지 df.sample에서 frac을 0~1 사이로 설정\n",
    "# df = ratings.sample(frac=1).reset_index(drop=True)\n",
    "# filter_user = deepcopy(df)\n",
    "# counts = filter_user['user_id'].value_counts()\n",
    "# filter_user = filter_user[filter_user['user_id'].isin(counts[counts >= 10].index)]\n",
    "\n",
    "# filtered_df = deepcopy(filter_user)\n",
    "# counts = filtered_df['movie_id'].value_counts()\n",
    "# filtered_df = filtered_df[filtered_df['movie_id'].isin(counts[counts >=10].index)]\n",
    "# filtered_df = filtered_df.reset_index(drop=True)\n",
    "# print(filtered_df)\n",
    "\n",
    "\n",
    "class generate_graph(object):\n",
    "    def __init__(self, path):\n",
    "        self.path = path  # '../NGCF-PyTorch/Data/ml-1m'\n",
    "        train_file = path + '/train.txt'\n",
    "        \n",
    "        self.neighbor_dict = {}\n",
    "        self.user, self.item = [], []\n",
    "        \n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    self.neighbor_dict[int(l[0])] = [int(i) for i in l[1:]]\n",
    "                    self.user.append(int(l[0]))\n",
    "        self.user = self.user[:1000]\n",
    "        \n",
    "    def jaccard_index(self, u_i, u_j, neighbor_dict):\n",
    "        u_i_neighbor = self.neighbor_dict[u_i]\n",
    "        u_j_neighbor = self.neighbor_dict[u_j]\n",
    "        return len(list(set(u_i_neighbor) & set(u_j_neighbor))) / len(list(set(u_i_neighbor) | set(u_j_neighbor)))\n",
    "\n",
    "    # 새로운 graph의 node j 에다가 기존 graph의 어떤 node의 neighborhood를 복사할지 zeta에 담기\n",
    "    def node_copying(self):\n",
    "        t1 = time()\n",
    "        zeta = []\n",
    "        \n",
    "        for u_j in self.user:\n",
    "            nor = 0\n",
    "            zeta_distribution = []\n",
    "            for u_i in self.user:\n",
    "                nor+=self.jaccard_index(u_j, u_i, self.neighbor_dict)\n",
    "            for u_m in self.user:\n",
    "                zeta_distribution.append(self.jaccard_index(u_j, u_m, self.neighbor_dict) / nor)\n",
    "            zeta.append(random.choices(self.user, weights=zeta_distribution)[0])\n",
    "        print('total node copying time cost : ', time() - t1)\n",
    "        np.save(self.path + '/zeta.npy', zeta)\n",
    "        \n",
    "        return zeta\n",
    "    \n",
    "    def generate_graph(self, epsilon, iteration):\n",
    "        t2 = time()\n",
    "        self.epsilon = epsilon\n",
    "        self.iteration = iteration\n",
    "        \n",
    "        generated_node = []\n",
    "        \n",
    "        with open(self.path+'/sampled_graph/sampled_graph_'+str(iteration+1), 'w') as f:\n",
    "            for i in self.user:\n",
    "                if random.uniform(0,1) < 1-self.epsilon:  # 1-epsilon의 확률로 원래 neighbor 넣기\n",
    "                    generated_node.append(i)\n",
    "                else:                                # epsilon의 확률로 zeta에 있는 node의 neighbor로 copy해서 넣기\n",
    "                    generated_node.append(zeta[i])\n",
    "                    \n",
    "                # 만들어지는 새로운 graph를 txt 파일로 저장\n",
    "                f.write(str(i))\n",
    "                f.write(' ')\n",
    "                for j in self.neighbor_dict[generated_node[i]][:-1]:\n",
    "                    f.write(str(j))\n",
    "                    f.write(' ')\n",
    "                f.write(str(self.neighbor_dict[generated_node[i]][-1]))\n",
    "                f.write('\\n')\n",
    "        print('#',iteration+1,' Graph sampled time cost : ', time() - t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a07a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = generate_graph('ml-1m')\n",
    "# zeta = node_copying(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta = t.node_copying()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enabling-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta = np.load('ml-1m/zeta.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0790388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1  Graph sampled time cost :  0.19529509544372559\n",
      "# 2  Graph sampled time cost :  0.19852304458618164\n",
      "# 3  Graph sampled time cost :  0.1815776824951172\n"
     ]
    }
   ],
   "source": [
    "# zeta를 한번 만든 후 iteration마다 generate graph하는 방식\n",
    "\n",
    "for epoch in range(3):\n",
    "    t.generate_graph(epsilon = 0.01, iteration = epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c05d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT\n",
    "# 만들어지는 graph마다 embedding 초기화 \n",
    "# embedding 학습하는 GNN 구조 짜기\n",
    "# 학습된 embedding 가지고 x_hat (eq.11) 구하고 BPR-OPT를 maximization 시키는 방향으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9df93286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data로 sparse matrix 및 adjacency matrix 만들기\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        train_file = path + '/train.txt'\n",
    "        # path : ml-1m\n",
    "        \n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.exist_users = []\n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "\n",
    "        self.n_users+=1\n",
    "        self.n_items+=1\n",
    "        print(self.n_users, self.n_items)\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.train_items = {}\n",
    "\n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l)==0:\n",
    "                    break\n",
    "                l = l.strip('\\n').split(' ')\n",
    "                uid, items = int(l[0]), [int(i) for i in l[1:]]\n",
    "\n",
    "                for i in items:\n",
    "                    self.R[uid, i] = 1\n",
    "\n",
    "                self.train_items[uid] = items\n",
    "                \n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            obs_adj_mat = sp.load_npz(self.path + '/obs_adj_mat.npz')\n",
    "            \n",
    "        except Exception:\n",
    "            obs_adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/obs_adj_mat.npz', obs_adj_mat)\n",
    "            \n",
    "        return obs_adj_mat\n",
    "    \n",
    "    \n",
    "    def create_adj_mat(self):\n",
    "        obs_adj_mat = self.R.todok()[:1000,:]\n",
    "        print('already create observed graph adjacency matrix', obs_adj_mat.shape)\n",
    "        return obs_adj_mat.tocsr()\n",
    "    \n",
    "    # bgcf는 G_obs로부터 만들어진 sampled graphs에 대해 x hat들의 integral을 구함\n",
    "    def sample(self):\n",
    "        # positive / negative items 나누기\n",
    "        if self.batch_size <= self.n_users:\n",
    "            obs_users = random.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            obs_users = [random.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "            \n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            # u유저의 neighbor중 num개 만큼 positive item sampling\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num:\n",
    "                    break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            # u유저의 neighbor가 아닌 item 중 num개 만큼 sampling\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num:\n",
    "                    break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "\n",
    "    #         def sample_neg_items_for_u_from_pools(u, num):\n",
    "    #             neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "    #             return random.sample(neg_items, num)     \n",
    "\n",
    "        obs_pos_items, obs_neg_items = [], []\n",
    "        for u in obs_users:\n",
    "            obs_pos_items += sample_pos_items_for_u(u,1)\n",
    "            obs_neg_items += sample_neg_items_for_u(u,1)\n",
    "\n",
    "        return obs_users, obs_pos_items, obs_neg_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b5b0a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# node copying으로 만들어진 graph들에 대해서 adj matrix 만들고 npz 저장하는 과정 \n",
    "\n",
    "class sampled_graph_to_matrix(object):\n",
    "    def __init__(self, path, iteration, batch_size):\n",
    "        self.path =path\n",
    "        self.iteration = iteration\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        sampled_graph = path + '/sampled_graph/sampled_graph_' + str(iteration+1)\n",
    "        # path : 'ml-1m'\n",
    "        \n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.exist_users = []\n",
    "        self.neg_pools = {}\n",
    "        \n",
    "        with open(sampled_graph, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "\n",
    "        self.n_users+=1\n",
    "        self.n_items+=1\n",
    "        print(self.n_users, self.n_items)\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.train_items = {}\n",
    "\n",
    "        with open(sampled_graph, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l)==0:\n",
    "                    break\n",
    "                l = l.strip('\\n').split(' ') \n",
    "                uid, items = int(l[0]), [int(i) for i in l[1:]]\n",
    "\n",
    "                for i in items:\n",
    "                    self.R[uid, i] = 1\n",
    "\n",
    "                self.train_items[uid] = items\n",
    "                \n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat_' + str(self.iteration+1) + '.npz')\n",
    "            \n",
    "        except Exception:\n",
    "            adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat_' + str(self.iteration+1) + '.npz', adj_mat)\n",
    "            \n",
    "        return adj_mat\n",
    "    \n",
    "     \n",
    "    def create_adj_mat(self):\n",
    "        adj_mat = self.R.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape)\n",
    "        return adj_mat.tocsr()\n",
    "    \n",
    "    def negative_pool(self):\n",
    "        t1 = time()\n",
    "        for u in self.train_items.keys():\n",
    "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\n",
    "            pools = [random.choice(neg_items) for _ in range(100)]\n",
    "            self.neg_pools[u] = pools\n",
    "        print('refresh negative pools', time() - t1)\n",
    "    \n",
    "    # bgcf는 G_obs로부터 만들어진 sampled graphs에 대해 x hat들의 integral을 구함\n",
    "    def sample(self):\n",
    "        # positive / negative items 나누기\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = random.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [random.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "            \n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            # u유저의 neighbor중 num개 만큼 positive item sampling\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num:\n",
    "                    break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "                \n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "        \n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            # u유저의 neighbor가 아닌 item 중 num개 만큼 sampling\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num:\n",
    "                    break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "        \n",
    "#         def sample_neg_items_for_u_from_pools(u, num):\n",
    "#             neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "#             return random.sample(neg_items, num)     \n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u,1)\n",
    "            neg_items += sample_neg_items_for_u(u,1)\n",
    "            \n",
    "        return users, pos_items, neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "hired-surgery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 3953\n"
     ]
    }
   ],
   "source": [
    "obs_graph = Data(path='ml-1m', batch_size = 1000)\n",
    "obs_users, obs_pos_items, obs_neg_items = obs_graph.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044aa381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 3953\n",
      "1000 3953\n",
      "1000 3953\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(3):\n",
    "    sampled_graph = sampled_graph_to_matrix(path='ml-1m', iteration = iteration, batch_size=1000)\n",
    "    sampled_graph.get_adj_mat()\n",
    "    users, pos_items, neg_items = sampled_graph.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c07d7",
   "metadata": {},
   "source": [
    "#### ngcf에서의 w_gc와 w_bi는 gc는 neighbor aggregate하는 부분의 weight, bi는 element-wise product하는 부분의 weight\n",
    "#### bgcf에서는 single feed forward layer로 구성하였고 총 3개의 weight가 학습됨 (GAT에서 2개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f0be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous version of GAT layer\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    # TODO : 초기함수에 positive/negative item 리스트를 받아서 각 positive/negative item들의 embedding이 output으로 나오게끔\n",
    "    def __init__(self, n_users, n_items, in_features, out_features, dropout = 0.2):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features      # 기존 노드의 embedding 차원수 \n",
    "        self.out_features = out_features    # 결과 weight의 embedding 차원수\n",
    "        \n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        \n",
    "        # self.W = nn.Parameter(initializer(torch.empty(in_features, out_features)))\n",
    "        # W_1은 a*e aggregate 부분, W_2는 e aggregate 부분\n",
    "        self.weight_dict = nn.ParameterDict({\n",
    "            'W_1' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))),\n",
    "            'W_2' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))),\n",
    "            'W_observed' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))) \n",
    "        })\n",
    "        \n",
    "        self.embedding_dict = nn.ParameterDict({\n",
    "            'user_emb' : nn.Parameter(initializer(torch.empty(self.n_users, self.out_features))),\n",
    "            'item_emb' : nn.Parameter(initializer(torch.empty(self.n_items, self.out_features)))\n",
    "        })\n",
    "        \n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "        \n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "        \n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "        \n",
    "        regularizer = (torch.norm(users) ** 2\n",
    "                       + torch.norm(pos_items) ** 2\n",
    "                       + torch.norm(neg_items) ** 2) / 2\n",
    "        emb_loss = regularizer / self.batch_size\n",
    "        #emb_loss = self.decay * regularizer / self.batch_size  (최종)\n",
    "            \n",
    "    \n",
    "    def forward(self, users, pos_items, neg_items, adj_matrix, iteration):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.iteration = iteration\n",
    "        coef = torch.zeros(self.n_users, self.n_items)\n",
    "        \n",
    "#         adj_matrix = sp.load_npz(path + '/s_adj_mat_' + str(iteration+1) +'.npz').toarray()\n",
    "#         adj_matrix = torch.Tensor(adj)\n",
    "        \n",
    "        user_emb = self.embedding_dict['user_emb']\n",
    "        item_emb = self.embedding_dict['item_emb']\n",
    "        \n",
    "        # self.W의 크기를 정확하게 확인해야함, 최종적인 각 node의 embedding size 는 1 x emb_size 로 나오게끔\n",
    "        # score는 각각 user-item 간의 attention score\n",
    "        score = torch.exp(torch.inner(user_emb, item_emb))\n",
    "        score = torch.multiply(score, adj_matrix)\n",
    "        \n",
    "        for i in range(score.shape[0]):\n",
    "            norm = sum(score[i])\n",
    "            normalized_score = score[i] / norm\n",
    "            coef[i] = normalized_score\n",
    "        # coef에다가 normalized된 최종적인 attention score 저장\n",
    "        w_1 = self.weight_dict['W_1']\n",
    "        w_2 = self.weight_dict['W_2']\n",
    "        # h_tilde_1은 w_1과 곱해지는 부분의 embedding (e_k)를 user와 item으로 쪼개서 계산후 concat\n",
    "        h_tilde_1_user = torch.matmul(coef, item_emb)\n",
    "        h_tilde_1_user = torch.matmul(h_tilde_1_user, w_1)\n",
    "        h_tilde_1_item = torch.matmul(coef.T, user_emb)\n",
    "        h_tilde_1_item = torch.matmul(h_tilde_1_item, w_1)\n",
    "        h_tilde_1 = torch.cat((h_tilde_1_user, h_tilde_1_item), dim=0)\n",
    "        \n",
    "        neighbor_num_user, neighbor_num_item = torch.zeros(self.n_users), torch.zeros(self.n_items)\n",
    "        for i in range(self.n_users):\n",
    "            neighbor_num_user[i] = sum(adj_matrix[i])\n",
    "        for j in range(self.n_items):\n",
    "            neighbor_num_item[i] = sum(adj_matrix.T[i])\n",
    "        # neighbor_num은 n_j에 해당하는 부분\n",
    "        \n",
    "        # h_tilde_2는 w_2와 곱해지는 부분\n",
    "        h_tilde_2_user = torch.matmul(adj_matrix, item_emb)\n",
    "        # h_tilde_2_user = torch.matmul(neighbor_num_user, h_tilde_2_user)\n",
    "        # 각 user의 neighbor의 수 만큼 item embedding에 곱해주는 과정\n",
    "        for i in range(neighbor_num_user.shape[0]):\n",
    "            h_tilde_2_user[i] = neighbor_num_user[i] * h_tilde_2_user[i]\n",
    "        h_tilde_2_user = torch.matmul(h_tilde_2_user, w_2)\n",
    "        h_tilde_2_item = torch.matmul(adj_matrix.T, user_emb)\n",
    "        # h_tilde_2_item = torch.matmul(neighbor_num_item, h_tilde_2_item)\n",
    "        # 각 item의 neighbor의 수 만큼 user embedding에 곱해주는 과정\n",
    "        for j in range(neighbor_num_item.shape[0]):\n",
    "            h_tilde_2_item[i] = neighbor_num_item[i] * h_tilde_2_item[i]\n",
    "        h_tilde_2_item = torch.matmul(h_tilde_2_item, w_2)\n",
    "        h_tilde_2 = torch.cat((h_tilde_2_user, h_tilde_2_item), dim=0)\n",
    "        \n",
    "        print(h_tilde_1.shape, h_tilde_2.shape)\n",
    "        \n",
    "        h_tilde_sampled = torch.cat((h_tilde_1, h_tilde_2), dim=0)\n",
    "        \n",
    "        # h_tilde_observed \n",
    "        \n",
    "        # h_tilde_sampled를 user embedidng / pos, neg item embedding들로 쪼개야함\n",
    "        return h_tilde_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "strategic-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGCFLayer(nn.Module):\n",
    "    # TODO : 초기함수에 positive/negative item 리스트를 받아서 각 positive/negative item들의 embedding이 output으로 나오게끔\n",
    "    def __init__(self, n_users, n_items, in_features, out_features, dropout = 0.2):\n",
    "        super(BGCFLayer, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features      # 기존 노드의 embedding 차원수 \n",
    "        self.out_features = out_features    # 결과 weight의 embedding 차원수\n",
    "        \n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        \n",
    "        # self.W = nn.Parameter(initializer(torch.empty(in_features, out_features)))\n",
    "        # W_1은 a*e aggregate 부분, W_2는 e aggregate 부분\n",
    "        self.weight_dict = nn.ParameterDict({\n",
    "            'W_1' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))),\n",
    "            'W_2' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))),\n",
    "            'W_obs' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features)))\n",
    "        })\n",
    "        \n",
    "        self.embedding_dict = nn.ParameterDict({\n",
    "            'user_emb' : nn.Parameter(initializer(torch.empty(self.n_users, self.out_features))),\n",
    "            'item_emb' : nn.Parameter(initializer(torch.empty(self.n_items, self.out_features)))\n",
    "        })\n",
    "        \n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "        \n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "        \n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "        \n",
    "        regularizer = (torch.norm(users) ** 2\n",
    "                       + torch.norm(pos_items) ** 2\n",
    "                       + torch.norm(neg_items) ** 2) / 2\n",
    "        \n",
    "        emb_loss = regularizer / 1000\n",
    "        # emb_loss = self.decay * regularizer / self.batch_size  (최종)\n",
    "        \n",
    "        mf_loss = torch.nan_to_num(mf_loss)\n",
    "        emb_loss = torch.nan_to_num(emb_loss)\n",
    "        \n",
    "        return mf_loss+emb_loss, mf_loss, emb_loss\n",
    "            \n",
    "    \n",
    "    def forward(self, \n",
    "                users, \n",
    "                pos_items, \n",
    "                neg_items, \n",
    "                adj_matrix, \n",
    "                obs_users, \n",
    "                obs_pos_items, \n",
    "                obs_neg_items, \n",
    "                obs_adj_matrix, \n",
    "                iteration):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.iteration = iteration\n",
    "        coef = torch.zeros(self.n_users, self.n_items)\n",
    "        \n",
    "#         adj_matrix = sp.load_npz(path + '/s_adj_mat_' + str(iteration+1) +'.npz').toarray()\n",
    "#         adj_matrix = torch.Tensor(adj)\n",
    "        \n",
    "        user_emb = self.embedding_dict['user_emb']\n",
    "        item_emb = self.embedding_dict['item_emb']\n",
    "        \n",
    "        # score는 각각 user-item 간의 attention score\n",
    "        score = torch.exp(torch.inner(user_emb, item_emb))\n",
    "        score = torch.multiply(score, adj_matrix)\n",
    "        \n",
    "        for i in range(score.shape[0]):\n",
    "            norm = sum(score[i])\n",
    "            normalized_score = score[i] / norm\n",
    "            coef[i] = normalized_score\n",
    "        # coef에다가 normalized된 최종적인 attention score 저장\n",
    "        w_1 = self.weight_dict['W_1']\n",
    "        w_2 = self.weight_dict['W_2']\n",
    "        # h_tilde_1은 w_1과 곱해지는 부분의 embedding (e_k)를 user와 item으로 쪼개서 계산후 concat\n",
    "        h_tilde_1_user = torch.matmul(coef, item_emb)\n",
    "        h_tilde_1_user = torch.matmul(h_tilde_1_user, w_1)\n",
    "        h_tilde_1_item = torch.matmul(coef.T, user_emb)\n",
    "        h_tilde_1_item = torch.matmul(h_tilde_1_item, w_1)\n",
    "        # h_tilde_1 = torch.cat((h_tilde_1_user, h_tilde_1_item), dim=0)\n",
    "        \n",
    "        neighbor_num_user, neighbor_num_item = torch.zeros(self.n_users), torch.zeros(self.n_items)\n",
    "        for i in range(self.n_users):\n",
    "            neighbor_num_user[i] = sum(adj_matrix[i])\n",
    "        for j in range(self.n_items):\n",
    "            neighbor_num_item[i] = sum(adj_matrix.T[i])\n",
    "        # neighbor_num은 n_j에 해당하는 부분\n",
    "        \n",
    "        # h_tilde_2는 w_2와 곱해지는 부분\n",
    "        h_tilde_2_user = torch.matmul(adj_matrix, item_emb)\n",
    "        # h_tilde_2_user = torch.matmul(neighbor_num_user, h_tilde_2_user)\n",
    "        # 각 user의 neighbor의 수 만큼 item embedding에 곱해주는 과정\n",
    "        for i in range(neighbor_num_user.shape[0]):\n",
    "            h_tilde_2_user[i] = (1 / neighbor_num_user[i]) * h_tilde_2_user[i]\n",
    "        h_tilde_2_user = torch.matmul(h_tilde_2_user, w_2)\n",
    "        h_tilde_2_item = torch.matmul(adj_matrix.T, user_emb)\n",
    "        # h_tilde_2_item = torch.matmul(neighbor_num_item, h_tilde_2_item)\n",
    "        # 각 item의 neighbor의 수 만큼 user embedding에 곱해주는 과정\n",
    "        for j in range(neighbor_num_item.shape[0]):\n",
    "            h_tilde_2_item[i] = (1 / neighbor_num_item[i]) * h_tilde_2_item[i]\n",
    "        h_tilde_2_item = torch.matmul(h_tilde_2_item, w_2)\n",
    "        # h_tilde_2 = torch.cat((h_tilde_2_user, h_tilde_2_item), dim=0)\n",
    "        \n",
    "        h_tilde_sampled_user = torch.cat((h_tilde_1_user, h_tilde_2_user), dim=1)\n",
    "        h_tilde_sampled_item = torch.cat((h_tilde_2_item, h_tilde_2_item), dim=1)\n",
    "        \n",
    "        h_tilde_sampled_pos_item = h_tilde_sampled_item[pos_items,:]\n",
    "        h_tilde_sampled_neg_item = h_tilde_sampled_item[neg_items,:]\n",
    "        \n",
    "        print('user embedding shape : ',h_tilde_sampled_user.shape)\n",
    "        print('positive item embedding shape : ',h_tilde_sampled_pos_item.shape)\n",
    "        print('negative item embedding shape : ',h_tilde_sampled_neg_item.shape)\n",
    "        \n",
    "        \n",
    "        ### h_tilde_observed \n",
    "        w_obs = self.weight_dict['W_obs']\n",
    "        \n",
    "        # observed graph의 neighbor 정보 \n",
    "        obs_neighbor_num_user, obs_neighbor_num_item = torch.zeros(self.n_users), torch.zeros(self.n_items)\n",
    "        # 각 원소 = 각 node의 neighbor수\n",
    "        for i in range(self.n_users):\n",
    "            obs_neighbor_num_user[i] = sum(obs_adj_matrix[i])\n",
    "        for j in range(self.n_items):\n",
    "            obs_neighbor_num_item[i] = sum(obs_adj_matrix.T[i])\n",
    "        # neighbor_num은 n_j에 해당하는 부분\n",
    "        \n",
    "        # h_tilde_obs_user\n",
    "        h_tilde_obs_user = torch.matmul(obs_adj_matrix, item_emb)\n",
    "        for i in range(obs_neighbor_num_user.shape[0]):\n",
    "            h_tilde_obs_user[i] = (1 / obs_neighbor_num_user[i]) * h_tilde_obs_user[i]\n",
    "        h_tilde_obs_user = torch.sigmoid(torch.matmul(h_tilde_obs_user, w_obs))\n",
    "        \n",
    "        # h_tilde_obs_item\n",
    "        h_tilde_obs_item = torch.matmul(obs_adj_matrix.T, user_emb)\n",
    "        for j in range(obs_neighbor_num_item.shape[0]):\n",
    "            h_tilde_obs_item[i] = (1 / obs_neighbor_num_item[i]) * h_tilde_obs_item[i]\n",
    "        h_tilde_obs_item = torch.sigmoid(torch.matmul(h_tilde_obs_item, w_obs))\n",
    "        \n",
    "        h_tilde_obs_pos_item = h_tilde_obs_item[obs_pos_items,:]\n",
    "        h_tilde_obs_neg_item = h_tilde_obs_item[obs_neg_items,:]\n",
    "        \n",
    "        # Final embedding\n",
    "        h_tilde_user = torch.sigmoid(torch.cat((h_tilde_sampled_user, h_tilde_obs_user), dim=1))\n",
    "        h_tilde_pos_item = torch.sigmoid(torch.cat((h_tilde_sampled_pos_item, h_tilde_obs_pos_item), dim=1))\n",
    "        h_tilde_neg_item = torch.sigmoid(torch.cat((h_tilde_sampled_neg_item, h_tilde_obs_neg_item), dim=1))\n",
    "        \n",
    "        # h_tilde_sampled를 user embedidng / pos, neg item embedding들로 쪼개야함\n",
    "        return h_tilde_user, h_tilde_pos_item, h_tilde_neg_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c44826b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 3953\n",
      "1000 3953\n",
      "user embedding shape :  torch.Size([1000, 128])\n",
      "positive item embedding shape :  torch.Size([1000, 128])\n",
      "negative item embedding shape :  torch.Size([1000, 128])\n",
      "pos_scores :  tensor([59.1553, 56.8840, 56.5704, 57.6291, 57.1323, 56.7671, 57.8176, 57.8524,\n",
      "        57.8459, 56.1807, 56.8830, 56.8802, 57.5251, 56.0263, 56.9701, 58.0207,\n",
      "        56.8089, 56.4857, 56.3988, 57.9065, 56.7772, 56.8028, 56.1511, 56.5376,\n",
      "        56.6839, 58.8255, 56.7236, 58.8100, 57.3102, 59.1374, 57.6681, 56.6598,\n",
      "        56.8446, 56.9003, 56.8299, 57.2104, 57.6041, 56.6081, 57.5836, 56.8928,\n",
      "        57.2445, 57.8059, 57.5111, 57.7835, 57.1465, 56.6595, 56.6108, 57.5349,\n",
      "        56.1517, 57.2596, 55.9557, 57.5799, 57.0889, 56.6063, 59.2190, 56.8124,\n",
      "        58.6131, 57.2274, 58.1403, 56.9822, 56.0698, 57.2879, 57.2703, 56.9146,\n",
      "        56.5990, 58.4139, 56.7911, 56.6289, 57.1644, 57.2906, 57.1941, 58.3887,\n",
      "        56.7106, 57.3783, 56.9993, 56.7839, 55.8838, 56.5094, 56.7225, 56.4861,\n",
      "        57.5710, 57.1189, 57.6062, 56.9987, 57.6856, 56.1743, 56.4030, 56.4439,\n",
      "        57.1285, 59.1290, 59.1340, 56.8036, 56.7877, 57.5743, 57.1619, 60.4323,\n",
      "        56.6857, 58.4226, 58.2339, 57.8714, 58.3977, 59.1384, 57.8963, 56.9997,\n",
      "        55.9927, 56.9252, 56.4466, 56.3003, 58.5941, 57.1056, 58.2648, 58.7182,\n",
      "        57.4030, 56.5712, 57.4386, 57.4678, 57.6520, 56.8597, 56.7276, 57.0971,\n",
      "        58.7624, 57.3613, 57.5088, 57.3476, 56.8281, 57.7116, 57.0961, 56.4412,\n",
      "        58.3524, 56.7889, 57.4038, 56.4933, 56.2355, 58.7263, 57.3576, 56.9423,\n",
      "        55.8975, 56.1441, 57.5304, 58.6334, 58.8793, 56.2166, 57.7280, 57.2903,\n",
      "        57.1895, 57.4260, 56.1175, 57.6179, 57.0725, 57.1108, 56.6037, 56.5925,\n",
      "        57.0745, 57.1702, 57.0815, 57.2804, 57.5865, 57.3333, 57.0432, 58.5971,\n",
      "        58.0496, 57.3032, 57.3546, 55.1956, 56.8013, 56.1522, 56.0072, 57.3009,\n",
      "        56.5056, 56.0675, 56.5695, 56.3227, 57.5127, 56.2957, 58.1070, 57.4232,\n",
      "        57.7698, 57.1780, 56.1772, 57.0616, 57.5039, 57.1039, 56.6627, 57.5666,\n",
      "        56.8991, 55.7394, 57.0106, 58.1852, 56.7876, 56.1866, 55.5186, 56.8534,\n",
      "        57.4382, 56.8942, 57.5913, 58.2499, 56.7371, 55.9617, 59.1999, 58.4261,\n",
      "        56.9481, 58.1124, 57.4816, 56.7541, 55.1285, 56.4517, 56.8630, 58.5401,\n",
      "        57.2876, 55.1790, 57.6796, 57.2149, 56.7530, 56.9330, 56.8050, 57.5073,\n",
      "        57.1055, 57.7517, 56.6428, 56.1350, 57.4390, 56.1689, 57.8101, 57.6042,\n",
      "        56.6280, 56.8423, 56.8185, 57.7290, 55.1678, 57.4362, 56.5265, 59.4541,\n",
      "        56.9257, 57.4894, 57.0889, 57.1673, 56.7883, 56.6961, 57.4079, 57.4289,\n",
      "        56.3801, 57.8415, 57.3212, 55.7800, 57.3804, 57.8725, 57.2851, 57.2096,\n",
      "        56.1802, 57.4075, 58.0958, 56.9490, 56.7154, 58.7596, 56.3485, 57.7420,\n",
      "        57.5749, 56.5915, 58.0621, 57.8246, 59.1928, 57.3491, 59.2809, 56.5219,\n",
      "        57.6224, 56.8716, 57.2839, 58.7435, 57.0761, 56.2152, 57.3535, 56.5834,\n",
      "        56.9304, 56.9648, 57.5960, 57.3675, 56.6785, 57.2239, 56.4750, 56.9801,\n",
      "        56.2362, 56.9276, 56.0780, 58.4618, 56.8410, 59.2877, 56.5011, 56.7402,\n",
      "        56.6390, 57.0930, 57.0061, 58.1942, 57.0007, 58.0820, 57.2111, 57.1632,\n",
      "        56.8175, 58.4735, 56.4836, 57.1963, 57.2208, 56.4509, 55.7638, 55.2525,\n",
      "        58.7365, 57.7615, 57.2965, 56.8148, 56.9627, 56.1894, 56.6086, 57.9562,\n",
      "        55.3966, 57.6334, 57.0176, 56.1412, 58.9863, 55.7488, 57.4926, 59.2783,\n",
      "        57.4986, 57.8641, 57.1324, 57.2455, 56.6642, 56.7730, 57.5005, 56.4550,\n",
      "        56.4565, 57.3922, 57.1024, 57.1235, 56.7921, 57.4431, 58.1844, 56.6239,\n",
      "        56.4798, 57.4935, 56.9350, 56.6974, 58.1012, 57.3725, 56.7138, 57.0669,\n",
      "        56.6917, 56.8410, 56.4082, 56.8064, 58.3463, 57.2820, 56.6078, 56.9320,\n",
      "        57.5178, 56.2409, 57.8257, 57.1137, 57.6501, 58.2956, 57.7259, 57.4910,\n",
      "        57.6629, 57.5067, 60.4185, 55.9786, 56.0946, 56.8555, 56.7533, 55.8361,\n",
      "        56.9779, 59.1040, 56.3775, 56.5946, 58.1514, 57.2724, 56.7877, 57.0024,\n",
      "        57.0994, 56.6089, 59.0505, 56.6660, 57.9256, 56.9182, 56.3482, 56.3862,\n",
      "        57.8845, 56.3573, 56.9477, 57.0265, 56.8081, 58.1506, 56.2695, 57.4472,\n",
      "        57.0640, 57.3321, 58.5846, 56.7030, 57.9253, 58.1543, 57.1442, 58.2842,\n",
      "        57.0777, 56.6026, 57.1176, 57.0117, 56.5218, 57.2172, 57.2590, 55.9500,\n",
      "        57.5879, 56.7606, 57.5268, 57.2060, 58.3562, 56.0466, 57.0182, 55.4951,\n",
      "        57.5037, 57.6102, 55.4092, 58.0257, 57.3447, 56.7536, 57.0458, 56.5044,\n",
      "        57.8745, 57.5491, 57.5361, 57.7279, 56.2108, 56.2221, 57.8377, 57.4529,\n",
      "        57.1626, 57.3550, 57.7190, 57.5708, 56.6088, 57.1042, 56.7038, 56.7822,\n",
      "        56.7115, 59.8266, 57.5508, 57.1348, 56.3179, 57.3477, 58.4895, 57.4592,\n",
      "        57.5250, 56.5200, 57.1620, 57.0164, 56.8649, 57.2414, 57.3043, 56.5125,\n",
      "        57.0974, 57.2267, 56.8313, 56.1604, 56.4050, 58.1828, 57.1363, 57.3136,\n",
      "        57.3368, 58.2779, 56.3373, 56.5444, 56.4364, 57.3153, 57.3336, 56.2303,\n",
      "        56.0482, 56.5648, 56.2535, 57.3526, 56.8274, 56.8310, 57.0737, 56.6480,\n",
      "        57.0225, 56.4931, 58.2508, 56.5904, 56.2843, 57.4489, 55.8318, 57.5665,\n",
      "        55.4299, 55.3888, 57.2506, 56.0661, 56.7291, 59.2398, 55.7518, 57.6729,\n",
      "        59.6475, 56.8309, 56.2443, 57.2369, 57.2271, 57.6195, 58.3275, 56.4544,\n",
      "        55.7376, 57.2790, 57.4246, 57.1653, 56.3546, 56.5778, 58.0948, 56.3860,\n",
      "        56.3724, 56.9112, 56.0643, 56.7070, 57.1882, 57.5750, 57.0423, 57.2440,\n",
      "        57.5449, 57.4129, 56.7876, 56.5865, 55.5312, 57.9179, 58.2742, 56.7568,\n",
      "        57.0342, 56.8212, 57.0330, 57.1835, 56.3616, 56.6593, 56.6897, 57.7380,\n",
      "        56.6810, 56.1014, 58.4208, 58.9337, 57.3424, 57.3959, 58.0405, 56.8548,\n",
      "        55.7783, 56.4840, 56.5292, 57.2521, 56.6301, 56.7384, 59.2615, 56.8673,\n",
      "        57.5734, 56.6496, 57.9590, 57.4461, 56.4968, 56.8695, 57.5365, 56.5270,\n",
      "        56.5455, 56.5250, 58.0340, 57.8686, 56.4886, 56.3618, 56.7870, 57.5997,\n",
      "        59.8978, 56.9071, 58.8643, 56.5115, 57.1437, 56.6266, 56.3587, 56.8545,\n",
      "        56.2139, 57.4242, 58.1309, 57.0364, 57.4891, 57.1903, 56.4395, 58.1258,\n",
      "        56.8428, 57.0963, 57.0612, 58.8696, 56.2550, 55.3180, 57.9110, 56.4748,\n",
      "        57.2752, 58.8882, 59.9214, 57.0696, 55.4962, 56.8412, 56.1663, 57.9776,\n",
      "        58.9201, 58.3553, 57.4867, 56.4637, 57.4964, 56.0325, 57.4296, 56.4092,\n",
      "        58.3932, 56.6391, 57.0566, 56.1090, 57.8267, 56.7425, 56.2775, 58.3402,\n",
      "        56.6877, 57.6428, 57.2680, 57.1438, 57.6710, 57.5752, 56.6934, 56.3140,\n",
      "        58.9880, 57.6259, 56.3041, 56.1177, 56.9856, 55.8424, 56.5160, 57.5049,\n",
      "        56.4818, 56.8185, 56.8765, 57.3246, 56.6418, 57.5814, 56.1248, 56.9972,\n",
      "        57.2375, 56.3805, 56.9858, 58.2525, 58.8104, 56.5537, 56.6805, 56.8481,\n",
      "        58.7942, 57.5605, 57.0291, 55.7242, 56.4816, 57.1260, 57.2840, 56.2389,\n",
      "        56.5458, 56.0743, 57.0877, 57.0895, 58.0741, 58.3748, 56.9403, 57.4717,\n",
      "        57.0819, 56.5664, 58.1488, 56.6161, 57.3848, 56.8761, 59.0845, 57.6963,\n",
      "        56.1787, 57.3002, 56.2845, 57.5878, 58.4671, 56.7259, 58.0372, 57.5928,\n",
      "        58.3301, 56.3904, 58.8224, 56.1944, 57.2186, 57.1459, 58.5318, 56.5155,\n",
      "        57.0453, 58.8882, 55.6185, 56.3925, 57.0882, 57.4479, 56.5315, 56.6071,\n",
      "        57.8555, 56.5461, 57.2775, 56.3370, 55.9676, 56.7252, 56.5032, 56.9539,\n",
      "        57.4748, 58.1952, 57.1994, 56.8868, 56.7439, 56.5623, 56.7031, 58.1322,\n",
      "        56.4982, 56.9056, 56.7173, 58.0180, 56.3078, 58.7794, 55.2664, 57.0611,\n",
      "        58.0391, 57.1719, 55.8282, 57.6121, 57.2973, 56.8474, 56.9985, 56.6455,\n",
      "        57.7495, 56.4959, 55.4900, 57.9288, 56.9448, 57.0144, 56.3129, 56.2769,\n",
      "        56.7124, 56.8723, 57.5641, 55.9877, 57.2983, 56.6455, 56.2195, 58.4036,\n",
      "        57.0862, 56.3116, 57.0448, 58.7280, 56.2136, 56.6453, 56.9720, 58.2042,\n",
      "        56.4734, 57.2351, 57.0084, 57.5351, 56.3823, 57.0218, 57.0521, 57.0101,\n",
      "        55.1767, 59.3671, 56.7042, 57.8414, 57.2174, 58.0588, 56.6590, 57.1722,\n",
      "        56.9423, 57.0062, 57.0914, 57.4680, 57.2951, 57.6858, 57.6015, 56.2304,\n",
      "        56.6174, 58.2388, 56.2404, 58.9456, 58.1817, 56.7754, 56.9885, 56.8469,\n",
      "        56.8913, 56.5871, 57.3263, 57.0189, 57.3098, 58.5485, 56.6773, 58.2202,\n",
      "        58.3854, 56.7485, 57.8204, 58.0369, 57.6524, 57.2240, 57.7794, 56.4149,\n",
      "        59.2372, 57.5587, 56.6802, 56.3471, 56.0541, 56.8575, 59.4057, 56.9355,\n",
      "        56.6842, 56.5719, 58.0040, 56.4412, 58.1888, 58.1376, 57.6355, 56.6959,\n",
      "        55.9709, 57.9011, 56.9746, 56.8291, 56.6438, 57.3958, 58.1620, 57.4675,\n",
      "        57.1066, 55.9141, 57.1241, 56.5947, 57.0649, 57.9836, 56.7593, 57.9089,\n",
      "        58.1404, 56.3252, 57.2407, 58.2895, 57.0720, 57.5401, 56.3414, 57.5107,\n",
      "        57.8285, 57.7721, 56.5866, 57.5642, 57.2399, 58.2274, 59.1826, 55.3709,\n",
      "        56.4166, 56.3506, 57.5396, 59.8014, 58.3435, 56.6078, 57.0290, 57.2883,\n",
      "        56.5319, 57.2414, 58.2994, 57.7235, 57.3775, 56.8956, 56.3107, 57.1507,\n",
      "        57.8295, 57.0081, 56.8353, 56.9098, 57.6223, 58.3632, 56.0219, 56.7332,\n",
      "        56.7420, 55.8721, 57.1673, 57.2233, 57.5742, 57.4618, 56.5032, 57.0880,\n",
      "        57.7014, 58.3615, 57.2018, 57.2828, 56.3383, 56.7465, 56.3535, 58.3812,\n",
      "        56.7160, 58.7051, 57.2855, 58.0812, 56.8705, 57.5466, 57.5499, 57.2765,\n",
      "        57.0236, 55.8129, 56.8653, 56.0256, 57.4155, 56.2905, 58.2346, 56.3592,\n",
      "        57.1174, 56.4816, 57.8557, 55.8750, 56.9070, 57.8522, 58.5860, 57.0084,\n",
      "        56.7943, 57.0640, 56.1864, 56.5546, 56.8401, 56.7840, 57.9429, 56.5480,\n",
      "        57.8885, 56.5555, 57.3974, 57.1505, 57.6228, 57.1910, 57.1174, 56.8531,\n",
      "        58.4219, 55.6396, 57.8411, 56.1801, 56.8957, 56.2520, 57.4201, 57.2411,\n",
      "        57.2536, 56.9124, 58.3171, 57.8835, 55.6465, 56.1835, 56.7926, 57.6861,\n",
      "        56.1376, 55.9489, 55.4518, 56.5630, 57.3352, 56.1259, 57.0226, 57.2111,\n",
      "        57.3687, 56.6574, 56.8020, 56.9927, 56.6640, 56.2093, 59.0860, 56.2823,\n",
      "        55.7692, 57.3032, 56.2298, 57.4727, 59.2829, 57.3156, 56.6065, 56.6517,\n",
      "        56.5080, 57.1445, 57.1393, 56.3740, 56.3111, 59.0583, 57.2861, 59.0083,\n",
      "        57.6130, 57.5979, 57.2996, 57.5405, 56.9054, 57.4972, 56.6310, 56.6980,\n",
      "        56.9406, 56.4436, 56.8378, 56.8168, 57.3534, 57.0873, 58.4125, 56.9807,\n",
      "        57.0564, 55.9257, 57.5008, 58.2104, 57.9001, 56.2455, 56.3941, 57.4482],\n",
      "       grad_fn=<SumBackward1>)\n",
      "neg_scores :  tensor([56.9886, 57.0155, 56.7797, 56.8407, 57.0446, 56.9503, 57.4417, 57.2077,\n",
      "        56.6929, 56.6715, 56.6839, 56.7859, 56.8683, 56.9534, 56.7665, 57.0331,\n",
      "        57.4973, 57.1464, 57.1736, 57.0922, 56.7894, 57.1240, 56.7681, 56.7609,\n",
      "        56.7984, 57.5832, 57.5805, 56.3820, 57.1086, 56.6251, 57.0825, 57.8847,\n",
      "        57.2324, 56.7947, 56.6762, 56.8936, 57.0616, 56.6972, 56.8227, 57.2008,\n",
      "        56.7942, 56.8568, 57.6563, 57.8553, 56.3987, 56.7605, 57.0171, 56.9183,\n",
      "        56.6971, 56.8084, 57.0214, 58.3474, 56.7947, 56.7047, 56.7823, 57.0732,\n",
      "        56.7882, 56.6058, 57.0543, 56.8612, 56.9254, 56.6621, 57.0951, 57.6915,\n",
      "        56.7202, 57.1885, 57.5154, 56.7939, 56.8432, 56.5939, 56.9930, 56.8766,\n",
      "        56.5867, 56.6349, 56.8363, 56.7965, 57.3326, 56.7772, 56.7917, 56.8223,\n",
      "        56.6566, 56.1789, 56.7600, 56.8000, 57.0788, 56.7991, 57.4075, 56.3629,\n",
      "        56.5640, 56.7552, 56.2267, 56.3213, 56.6999, 56.7923, 56.6696, 56.9621,\n",
      "        56.6273, 56.4865, 57.6115, 56.8076, 56.5030, 57.7622, 57.6400, 57.4041,\n",
      "        57.1690, 57.2671, 56.8687, 56.7724, 56.5343, 56.4105, 57.0550, 57.4885,\n",
      "        57.2239, 56.6395, 56.7574, 56.8214, 57.4000, 56.8279, 56.7835, 56.8409,\n",
      "        56.9321, 56.7893, 57.2003, 56.9317, 56.5517, 56.6376, 57.2625, 56.8179,\n",
      "        56.7808, 56.8524, 56.8276, 56.5402, 56.2531, 56.9612, 57.2997, 56.9354,\n",
      "        56.6814, 57.2191, 57.0532, 57.0764, 56.5722, 57.1925, 58.4189, 56.6799,\n",
      "        56.7198, 56.6548, 55.9852, 57.0138, 57.0339, 57.3755, 56.1971, 56.1076,\n",
      "        56.5951, 56.7056, 56.6901, 56.7956, 56.8659, 56.4333, 57.2651, 57.0559,\n",
      "        56.8377, 57.0344, 56.7239, 56.7567, 57.1705, 56.3887, 56.3486, 57.0210,\n",
      "        56.9164, 56.8131, 56.7742, 57.8623, 57.0377, 56.7942, 56.8435, 58.3846,\n",
      "        56.9427, 56.6866, 58.4521, 57.4853, 56.4425, 56.9241, 56.6651, 56.7856,\n",
      "        57.2421, 57.1870, 56.3223, 57.5798, 56.8198, 57.6174, 56.7052, 56.7574,\n",
      "        56.7925, 56.9183, 58.3871, 56.4962, 57.5158, 56.7719, 56.6343, 56.8035,\n",
      "        56.9077, 56.7968, 57.1329, 56.5492, 56.1815, 57.9175, 56.7264, 57.1612,\n",
      "        56.6087, 56.8164, 56.8311, 57.2023, 56.4054, 56.9439, 56.7958, 57.4357,\n",
      "        56.7249, 57.1420, 56.6163, 56.1089, 56.8900, 56.9147, 57.7323, 56.5413,\n",
      "        56.8361, 57.2735, 56.7322, 56.1717, 57.1329, 57.1023, 56.7856, 56.6340,\n",
      "        56.7927, 57.1132, 56.7980, 56.8861, 57.7506, 56.9583, 56.7560, 56.5715,\n",
      "        56.2967, 57.2645, 57.0252, 56.7907, 57.0057, 56.2689, 56.8378, 56.9166,\n",
      "        56.7313, 56.8231, 57.0050, 56.9938, 56.7881, 57.4946, 56.7913, 56.8447,\n",
      "        57.3204, 56.7928, 56.6871, 56.8159, 56.9165, 56.9397, 56.8843, 56.8980,\n",
      "        56.7447, 56.7719, 56.2303, 56.5332, 56.8044, 56.9695, 57.0291, 57.0343,\n",
      "        56.8241, 56.7716, 56.8357, 56.9855, 56.6797, 57.0352, 57.3097, 56.7971,\n",
      "        57.2562, 56.8210, 56.4643, 57.2356, 57.2271, 56.7737, 56.7985, 56.4720,\n",
      "        57.3033, 56.8346, 57.1015, 56.6955, 56.6981, 57.2480, 56.6453, 56.9750,\n",
      "        57.1086, 56.8282, 56.4234, 57.1135, 56.8059, 56.5128, 57.2462, 56.7887,\n",
      "        57.2892, 56.7037, 56.8911, 56.5668, 57.5779, 57.1523, 56.9877, 56.9500,\n",
      "        57.1708, 56.8429, 57.3432, 56.7545, 57.0674, 56.7017, 56.5988, 57.0775,\n",
      "        56.5568, 56.9407, 56.7981, 56.1107, 57.6756, 56.9453, 57.1620, 55.9337,\n",
      "        56.2998, 57.2310, 56.8496, 57.0287, 56.4177, 57.0368, 56.8813, 56.9659,\n",
      "        56.7763, 58.2093, 56.7217, 56.7915, 57.5626, 56.7641, 58.2962, 56.7596,\n",
      "        56.7835, 56.3942, 56.5351, 56.6941, 56.7782, 56.8202, 57.2023, 56.7773,\n",
      "        56.5577, 56.8983, 57.1480, 55.7755, 56.2953, 57.1513, 56.7798, 56.8064,\n",
      "        56.6438, 57.2129, 56.5852, 56.7782, 56.7960, 57.8366, 57.7853, 56.6917,\n",
      "        57.0856, 56.7842, 56.2437, 57.3741, 57.0588, 57.1422, 57.1188, 56.8680,\n",
      "        57.3458, 56.8573, 56.7246, 56.1413, 56.0678, 56.7369, 57.2181, 56.9298,\n",
      "        56.2082, 55.7477, 56.9829, 56.7858, 57.0994, 57.1575, 57.4404, 56.7995,\n",
      "        56.6765, 57.8133, 56.7710, 57.0764, 56.8091, 56.1949, 56.7808, 56.8070,\n",
      "        56.8395, 56.6073, 56.9869, 56.9158, 56.4816, 56.7149, 56.7819, 56.6839,\n",
      "        56.8029, 56.8517, 57.4650, 56.4346, 57.1979, 56.7525, 56.7784, 56.8234,\n",
      "        56.8791, 58.2966, 56.8029, 56.8154, 56.6024, 56.5982, 58.1092, 56.6595,\n",
      "        56.7353, 56.7905, 56.8050, 56.8489, 56.8292, 55.9508, 57.2181, 56.6860,\n",
      "        56.7638, 56.7473, 56.7941, 57.1657, 56.9823, 57.2195, 56.5628, 56.7341,\n",
      "        56.1259, 56.4289, 57.2973, 56.8541, 57.1554, 56.7956, 56.7285, 56.9082,\n",
      "        56.9916, 57.7299, 56.9632, 57.4058, 56.8003, 56.6575, 56.4653, 57.3274,\n",
      "        57.0043, 56.9197, 57.2490, 56.8000, 56.7620, 56.9075, 57.2574, 57.1017,\n",
      "        56.9024, 56.7683, 57.2605, 57.8083, 57.4464, 57.7822, 57.5701, 56.9207,\n",
      "        57.5013, 58.4655, 56.6063, 56.6421, 56.8516, 56.6279, 56.6625, 56.7989,\n",
      "        56.8210, 56.8022, 56.6007, 57.2181, 56.7506, 56.8390, 57.5971, 57.1308,\n",
      "        56.4164, 56.9853, 56.7815, 56.4854, 56.3589, 56.6399, 56.5236, 56.7824,\n",
      "        56.7243, 56.7675, 56.6559, 57.3821, 57.1056, 56.7884, 56.8540, 56.8359,\n",
      "        57.2155, 56.6415, 56.6621, 57.0369, 56.7612, 56.8020, 56.7033, 56.5733,\n",
      "        56.4611, 57.1317, 57.0683, 58.1537, 56.7901, 56.7292, 58.2362, 57.0622,\n",
      "        56.9236, 56.8945, 57.0130, 56.0466, 57.0412, 56.9289, 56.9949, 56.5298,\n",
      "        56.7331, 56.9566, 55.9940, 57.5435, 56.7025, 56.6248, 57.0714, 56.9318,\n",
      "        56.8279, 56.5730, 56.7048, 56.7400, 56.8098, 56.7715, 56.9130, 56.5807,\n",
      "        57.0470, 56.5429, 56.7426, 56.7905, 57.3097, 56.7504, 57.2565, 57.2306,\n",
      "        57.0269, 57.7257, 55.8810, 56.8001, 57.4056, 56.8201, 56.6918, 56.7017,\n",
      "        56.3202, 56.9245, 57.4000, 56.7850, 57.8047, 56.7166, 56.9449, 56.8807,\n",
      "        56.7107, 56.8140, 56.8440, 56.7643, 56.8467, 56.9971, 56.9665, 56.7852,\n",
      "        57.7042, 56.9589, 56.8284, 56.5758, 56.8146, 57.1972, 57.4393, 56.9238,\n",
      "        56.7549, 56.6217, 56.7069, 58.3316, 58.6957, 57.4893, 57.7006, 56.1655,\n",
      "        57.0106, 57.0712, 56.7959, 56.9558, 56.8008, 56.4444, 57.0121, 56.4648,\n",
      "        57.8263, 57.0553, 57.0804, 57.0276, 57.6032, 56.7217, 57.4272, 56.3918,\n",
      "        56.7789, 56.2454, 57.0816, 56.9963, 56.7621, 57.2139, 57.7613, 57.2567,\n",
      "        55.8943, 56.8753, 56.9502, 59.0872, 56.7727, 57.4868, 57.0891, 58.0095,\n",
      "        56.6312, 57.4065, 58.2205, 56.6450, 56.7749, 56.7282, 57.2010, 56.5508,\n",
      "        56.9953, 56.6716, 56.1125, 57.0743, 56.7499, 56.8908, 57.0260, 57.0337,\n",
      "        56.9190, 56.0383, 56.9149, 56.7962, 56.8054, 57.1620, 56.7571, 56.6479,\n",
      "        56.8535, 56.6346, 57.5643, 56.0730, 56.0898, 56.7977, 57.0557, 56.7897,\n",
      "        57.3308, 56.4878, 57.0922, 57.2436, 56.8783, 56.7974, 56.9147, 56.5943,\n",
      "        56.9777, 57.4741, 56.9133, 56.7999, 56.7263, 56.9910, 56.8217, 56.7398,\n",
      "        56.4670, 57.4366, 56.4058, 56.2260, 57.2470, 56.7155, 57.0661, 56.6242,\n",
      "        56.7983, 56.7999, 56.4629, 56.8069, 56.2946, 56.8036, 58.0741, 57.1725,\n",
      "        56.7826, 56.9529, 56.4826, 57.0323, 56.7927, 57.2099, 56.7863, 56.6982,\n",
      "        56.7782, 57.0239, 56.7441, 56.3643, 57.1180, 57.4368, 57.4246, 56.7745,\n",
      "        55.7793, 57.1045, 56.8378, 57.1721, 57.0722, 56.4638, 57.0359, 56.3600,\n",
      "        56.8635, 56.8319, 58.2789, 56.7174, 55.9244, 56.6808, 56.6990, 56.7997,\n",
      "        56.9898, 56.8004, 56.7562, 56.7623, 57.2593, 56.8007, 56.9860, 56.3277,\n",
      "        57.4180, 56.9233, 56.9102, 58.1834, 57.5614, 56.6780, 57.3037, 56.8449,\n",
      "        56.6795, 56.7565, 57.1173, 56.8936, 57.4639, 57.0557, 56.8700, 56.8854,\n",
      "        56.0747, 56.9093, 56.7367, 56.0451, 56.7663, 57.2887, 56.7807, 56.7291,\n",
      "        56.7574, 57.2633, 56.7789, 56.8488, 56.4447, 56.7248, 56.7028, 55.8734,\n",
      "        56.7962, 57.1694, 56.7443, 56.8074, 57.5994, 56.7954, 56.9441, 56.6024,\n",
      "        56.7168, 56.9513, 56.8085, 56.7985, 57.2873, 56.7393, 56.7912, 56.8164,\n",
      "        56.8219, 56.9035, 56.0920, 56.8260, 56.9640, 56.4078, 56.7948, 56.9518,\n",
      "        56.9515, 56.4804, 56.5108, 57.6759, 56.3833, 56.8788, 56.7349, 56.8013,\n",
      "        57.1809, 56.7980, 57.4513, 56.6822, 56.8242, 57.2821, 55.9168, 57.0684,\n",
      "        56.8426, 56.7801, 56.8056, 56.4788, 56.6041, 55.6367, 57.0799, 56.8888,\n",
      "        55.9676, 56.6416, 57.1125, 57.2481, 55.8776, 56.9049, 56.6810, 56.8171,\n",
      "        56.7057, 57.2889, 56.7256, 56.9040, 57.9000, 57.0477, 56.8272, 56.2615,\n",
      "        57.2874, 56.8266, 56.5877, 56.7888, 56.6329, 57.0709, 56.7087, 56.9318,\n",
      "        57.4635, 56.6347, 56.7992, 56.7436, 56.9655, 57.0794, 56.7931, 57.0207,\n",
      "        57.0731, 56.5756, 56.7428, 57.0086, 56.6789, 56.7956, 56.8072, 56.1876,\n",
      "        56.4884, 56.8844, 56.6891, 56.6579, 57.4647, 56.8804, 58.1665, 56.3859,\n",
      "        56.7941, 55.9514, 56.1108, 56.6133, 57.7866, 57.3267, 58.6547, 56.6647,\n",
      "        56.8309, 57.3383, 56.6246, 56.9518, 56.7781, 56.7044, 56.8040, 56.6203,\n",
      "        56.9439, 56.8042, 56.9383, 56.7029, 58.0843, 57.1701, 56.7915, 57.1844,\n",
      "        56.6816, 57.5685, 56.8594, 57.0076, 56.6724, 57.3436, 57.1973, 57.0416,\n",
      "        56.8568, 56.4127, 57.2998, 56.9263, 56.7927, 57.2554, 56.7963, 56.7198,\n",
      "        56.6377, 57.1468, 57.4888, 56.7498, 57.5826, 56.6763, 57.0924, 56.8031,\n",
      "        56.5213, 57.3208, 56.5273, 56.5937, 56.7399, 56.7210, 56.7896, 57.2647,\n",
      "        56.5444, 57.6293, 56.6824, 56.5106, 56.7889, 57.4710, 57.1831, 57.0374,\n",
      "        56.9142, 56.8287, 57.2510, 56.8009, 57.0654, 57.0055, 56.8534, 56.4577,\n",
      "        56.7909, 56.7045, 56.8782, 56.9442, 56.8018, 57.0493, 56.6811, 56.8369,\n",
      "        56.2785, 56.8040, 56.4921, 56.7131, 57.2474, 56.6995, 56.9904, 56.5617,\n",
      "        56.9048, 57.0732, 56.3116, 56.6482, 56.9189, 57.7879, 56.6771, 56.8955,\n",
      "        57.3204, 56.6305, 57.9240, 58.2307, 56.9763, 57.6067, 56.4703, 56.7974,\n",
      "        57.0102, 56.5887, 56.5302, 57.0144, 56.9583, 56.3001, 57.1893, 56.7202,\n",
      "        57.3324, 56.7971, 56.7983, 56.9156, 56.7083, 56.7425, 56.9409, 55.9226,\n",
      "        56.8299, 57.0217, 56.1980, 56.6264, 57.0289, 56.9039, 56.7917, 57.1280,\n",
      "        56.9229, 57.9732, 56.6775, 56.2580, 56.8066, 57.3046, 57.4198, 57.1506,\n",
      "        56.9287, 57.4773, 56.5499, 56.6400, 56.8391, 56.7969, 56.4094, 56.8434],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(87.3232, grad_fn=<AddBackward0>) tensor(0.6681, grad_fn=<AddBackward0>) tensor(86.6551, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pos_items, neg_items = sampled_graph_to_matrix(path='ml-1m', iteration = iteration).divide_pos_neg()\n",
    "# 완성형은 GATLayer(6040, 3953, 64,64, pos_items, neg_items)\n",
    "model = BGCFLayer(1000, 3953, 64, 64)\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "path = 'ml-1m'\n",
    "for epoch in range(1):\n",
    "    loss, mf_loss, emb_loss = 0., 0., 0.\n",
    "    \n",
    "    obs_graph = Data(path = path, batch_size = 1000)\n",
    "    obs_adj_matrix = obs_graph.get_adj_mat().toarray()\n",
    "    obs_adj_matrix = torch.Tensor(obs_adj_matrix)\n",
    "    obs_users, obs_pos_items, obs_neg_items = obs_graph.sample()\n",
    "    \n",
    "    sampled_graph = sampled_graph_to_matrix(path = path, iteration = epoch, batch_size=1000)\n",
    "    adj_matrix = sampled_graph.get_adj_mat().toarray()\n",
    "    adj_matrix = torch.Tensor(adj_matrix)\n",
    "    users, pos_items, neg_items = sampled_graph.sample()\n",
    "    \n",
    "    for iteration in range(1):\n",
    "        t1 = time()\n",
    "        \n",
    "        # sampled_graph.get_adj_mat 함수로 바꿔야할듯\n",
    "#         adj_matrix = sp.load_npz(path + '/s_adj_mat_' + str(iteration+1) +'.npz').toarray()\n",
    "#         adj_matrix = torch.Tensor(adj_matrix)\n",
    "\n",
    "        '''불러오는 sampled graph matrix마다 pos, neg item set을 만들고 (함수 사용) \n",
    "        bpr loss 함수에다가 각 item들에 해당하는 embedding을 입력으로 넣어줌'''\n",
    "\n",
    "        # sampled_graph = sampled_graph_to_matrix(path = path, iteration = iteration, batch_size=1000)\n",
    "        # users, pos_items, neg_items = sampled_graph.sample()\n",
    "        u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings = model(users,\n",
    "                                                                       pos_items,\n",
    "                                                                       neg_items,\n",
    "                                                                       adj_matrix,\n",
    "                                                                       obs_users,\n",
    "                                                                       obs_pos_items,\n",
    "                                                                       obs_neg_items,\n",
    "                                                                       obs_adj_matrix,\n",
    "                                                                       iteration)\n",
    "        \n",
    "        batch_loss, batch_mf_loss, batch_emb_loss = model.create_bpr_loss(u_g_embeddings,\n",
    "                                                                          pos_i_g_embeddings,\n",
    "                                                                          neg_i_g_embeddings)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print(batch_loss, batch_mf_loss, batch_emb_loss)\n",
    "        \n",
    "        loss += batch_loss\n",
    "        mf_loss += batch_mf_loss\n",
    "        emb_loss += batch_emb_loss\n",
    "        \n",
    "        print(loss, mf_loss, emb_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f8825c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3953)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'initializer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2c47a566dc4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0madj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcoef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3953\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0muser_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mitem_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3953\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mw_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initializer' is not defined"
     ]
    }
   ],
   "source": [
    "adj = sp.load_npz('ml-1m/s_adj_mat_1.npz')\n",
    "adj = adj.toarray()\n",
    "print(adj.shape)\n",
    "adj = torch.Tensor(adj)\n",
    "coef = torch.zeros(3,3953)\n",
    "user_emb = nn.Parameter(initializer(torch.empty(1000,32)))\n",
    "item_emb = nn.Parameter(initializer(torch.empty(3953,32)))\n",
    "w_1 = nn.Parameter(initializer(torch.empty(32,32)))\n",
    "w_2 = nn.Parameter(initializer(torch.empty(32,32)))\n",
    "\n",
    "x = torch.exp(torch.inner(user_emb, item_emb))\n",
    "print(x)\n",
    "x = torch.multiply(x, adj)\n",
    "print(x)\n",
    "for i in range(3):\n",
    "    tot = sum(x[i])\n",
    "    score = x[i] / tot\n",
    "    coef[i] = score\n",
    "    \n",
    "w1 = torch.matmul(coef, item_emb)\n",
    "print(w1.shape)\n",
    "w1 = torch.matmul(w1, w_1)\n",
    "print(w1.shape)\n",
    "w2 = torch.matmul(coef.T, user_emb[:3])\n",
    "print(w2.shape)\n",
    "w2 = torch.matmul(w2, w_2)\n",
    "print(w2.shape)\n",
    "w3 = torch.cat((w1, w2), dim=0)\n",
    "w3.shape\n",
    "\n",
    "user_emb = nn.Parameter(initializer(torch.empty(1000,32)))\n",
    "item_emb = nn.Parameter(initializer(torch.empty(3953,32)))\n",
    "avg_emb = torch.matmul(adj.T, user_emb)\n",
    "avg_emb.shape\n",
    "# x = torch.multiply(x, adj)\n",
    "# print(x.shape)\n",
    "# x = F.softmax(x, dim=1)\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3840a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_num(row):\n",
    "    return sum(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumemb = torch.zeros((5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    sumemb[i] = sum(adj[i])\n",
    "    \n",
    "torch.matmul(sumemb, item_emb[:5]).shape\n",
    "# sumemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead80d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = nn.init.xavier_uniform_\n",
    "a = nn.Parameter(initializer(torch.empty(5,5)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646153ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = nn.init.xavier_uniform_\n",
    "user_emb = nn.Parameter(initializer(torch.empty(5,10)))\n",
    "user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.matrix([[0,1,2],[1,1,1],[0,0,1]])\n",
    "adj = np.matrix([[0,0,1],[1,0,0],[1,1,0]])\n",
    "e = np.matrix([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "print(a,'\\n')\n",
    "print(adj,'\\n')\n",
    "print(e,'\\n')\n",
    "\n",
    "a_jk = np.multiply(a,adj)\n",
    "print(a_jk, '\\n')\n",
    "\n",
    "np.matmul(a_jk, e)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80349c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
