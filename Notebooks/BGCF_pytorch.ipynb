{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277c185d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "\n",
    "# directory = '../MF/ml-1m'\n",
    "# ratings = []\n",
    "# with open(os.path.join(directory, 'ratings.dat'), encoding='latin1') as f:\n",
    "#     for l in f:\n",
    "#         user_id, movie_id, rating, timestamp = [int(_) for _ in l.split('::')]\n",
    "#         ratings.append({\n",
    "#             'user_id': user_id,\n",
    "#             'movie_id': movie_id,\n",
    "#             'rating': rating,\n",
    "#             'timestamp': timestamp,\n",
    "#             })\n",
    "# ratings = pd.DataFrame(ratings)\n",
    "\n",
    "# ratings = ratings.drop(['timestamp'],axis=1)\n",
    "# print(ratings)\n",
    "# # 어떤 비율로 표본 추출을 하고싶은지 df.sample에서 frac을 0~1 사이로 설정\n",
    "# df = ratings.sample(frac=1).reset_index(drop=True)\n",
    "# filter_user = deepcopy(df)\n",
    "# counts = filter_user['user_id'].value_counts()\n",
    "# filter_user = filter_user[filter_user['user_id'].isin(counts[counts >= 10].index)]\n",
    "\n",
    "# filtered_df = deepcopy(filter_user)\n",
    "# counts = filtered_df['movie_id'].value_counts()\n",
    "# filtered_df = filtered_df[filtered_df['movie_id'].isin(counts[counts >=10].index)]\n",
    "# filtered_df = filtered_df.reset_index(drop=True)\n",
    "# print(filtered_df)\n",
    "\n",
    "\n",
    "class generate_graph(object):\n",
    "    def __init__(self, path):\n",
    "        self.path = path  # '../NGCF-PyTorch/Data/ml-1m'\n",
    "        train_file = path + '/train.txt'\n",
    "        \n",
    "        self.neighbor_dict = {}\n",
    "        self.user, self.item = [], []\n",
    "        \n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    self.neighbor_dict[int(l[0])] = [int(i) for i in l[1:]]\n",
    "                    self.user.append(int(l[0]))\n",
    "        self.user = self.user[:1000]\n",
    "        \n",
    "    def jaccard_index(self, u_i, u_j, neighbor_dict):\n",
    "        u_i_neighbor = self.neighbor_dict[u_i]\n",
    "        u_j_neighbor = self.neighbor_dict[u_j]\n",
    "        return len(list(set(u_i_neighbor) & set(u_j_neighbor))) / len(list(set(u_i_neighbor) | set(u_j_neighbor)))\n",
    "\n",
    "    # 새로운 graph의 node j 에다가 기존 graph의 어떤 node의 neighborhood를 복사할지 zeta에 담기\n",
    "    def node_copying(self):\n",
    "        t1 = time()\n",
    "        zeta = []\n",
    "        \n",
    "        for u_j in self.user:\n",
    "            nor = 0\n",
    "            zeta_distribution = []\n",
    "            for u_i in self.user:\n",
    "                nor+=self.jaccard_index(u_j, u_i, self.neighbor_dict)\n",
    "            for u_m in self.user:\n",
    "                zeta_distribution.append(self.jaccard_index(u_j, u_m, self.neighbor_dict) / nor)\n",
    "            zeta.append(random.choices(self.user, weights=zeta_distribution)[0])\n",
    "        print('total node copying time cost : ', time() - t1)\n",
    "        return zeta\n",
    "    \n",
    "    def generate_graph(self, epsilon, iteration):\n",
    "        t2 = time()\n",
    "        self.epsilon = epsilon\n",
    "        self.iteration = iteration\n",
    "        \n",
    "        generated_node = []\n",
    "        \n",
    "        with open(self.path+'/sampled_graph/sampled_graph_'+str(iteration+1), 'w') as f:\n",
    "            for i in self.user:\n",
    "                if random.uniform(0,1) < 1-self.epsilon:  # 1-epsilon의 확률로 원래 neighbor 넣기\n",
    "                    generated_node.append(i)\n",
    "                else:                                # epsilon의 확률로 zeta에 있는 node의 neighbor로 copy해서 넣기\n",
    "                    generated_node.append(zeta[i])\n",
    "                    \n",
    "                # 만들어지는 새로운 graph를 txt 파일로 저장\n",
    "                f.write(str(i))\n",
    "                f.write(' ')\n",
    "                for j in self.neighbor_dict[generated_node[i]][:-1]:\n",
    "                    f.write(str(j))\n",
    "                    f.write(' ')\n",
    "                f.write(str(self.neighbor_dict[generated_node[i]][-1]))\n",
    "                f.write('\\n')\n",
    "        print(iteration+1,'-th Graph sampled time cost : ', time() - t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a07a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total node copying time cost :  68.20209217071533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[230,\n",
       " 616,\n",
       " 842,\n",
       " 310,\n",
       " 251,\n",
       " 169,\n",
       " 221,\n",
       " 479,\n",
       " 183,\n",
       " 236,\n",
       " 37,\n",
       " 507,\n",
       " 451,\n",
       " 13,\n",
       " 540,\n",
       " 375,\n",
       " 668,\n",
       " 715,\n",
       " 160,\n",
       " 415,\n",
       " 20,\n",
       " 675,\n",
       " 529,\n",
       " 959,\n",
       " 343,\n",
       " 756,\n",
       " 226,\n",
       " 248,\n",
       " 315,\n",
       " 211,\n",
       " 892,\n",
       " 546,\n",
       " 223,\n",
       " 721,\n",
       " 17,\n",
       " 344,\n",
       " 480,\n",
       " 248,\n",
       " 345,\n",
       " 83,\n",
       " 489,\n",
       " 128,\n",
       " 629,\n",
       " 48,\n",
       " 122,\n",
       " 615,\n",
       " 640,\n",
       " 9,\n",
       " 989,\n",
       " 541,\n",
       " 965,\n",
       " 482,\n",
       " 799,\n",
       " 768,\n",
       " 268,\n",
       " 794,\n",
       " 367,\n",
       " 402,\n",
       " 956,\n",
       " 726,\n",
       " 412,\n",
       " 857,\n",
       " 962,\n",
       " 18,\n",
       " 514,\n",
       " 90,\n",
       " 819,\n",
       " 270,\n",
       " 350,\n",
       " 53,\n",
       " 788,\n",
       " 8,\n",
       " 734,\n",
       " 519,\n",
       " 318,\n",
       " 499,\n",
       " 466,\n",
       " 848,\n",
       " 670,\n",
       " 148,\n",
       " 891,\n",
       " 904,\n",
       " 647,\n",
       " 731,\n",
       " 706,\n",
       " 346,\n",
       " 350,\n",
       " 247,\n",
       " 160,\n",
       " 20,\n",
       " 555,\n",
       " 420,\n",
       " 81,\n",
       " 496,\n",
       " 627,\n",
       " 584,\n",
       " 609,\n",
       " 662,\n",
       " 983,\n",
       " 16,\n",
       " 731,\n",
       " 772,\n",
       " 328,\n",
       " 115,\n",
       " 325,\n",
       " 576,\n",
       " 73,\n",
       " 541,\n",
       " 585,\n",
       " 285,\n",
       " 807,\n",
       " 399,\n",
       " 112,\n",
       " 878,\n",
       " 643,\n",
       " 960,\n",
       " 234,\n",
       " 378,\n",
       " 4,\n",
       " 927,\n",
       " 894,\n",
       " 94,\n",
       " 490,\n",
       " 832,\n",
       " 493,\n",
       " 294,\n",
       " 517,\n",
       " 456,\n",
       " 473,\n",
       " 120,\n",
       " 135,\n",
       " 863,\n",
       " 679,\n",
       " 168,\n",
       " 677,\n",
       " 980,\n",
       " 68,\n",
       " 874,\n",
       " 806,\n",
       " 139,\n",
       " 218,\n",
       " 112,\n",
       " 995,\n",
       " 143,\n",
       " 84,\n",
       " 799,\n",
       " 813,\n",
       " 129,\n",
       " 910,\n",
       " 753,\n",
       " 520,\n",
       " 911,\n",
       " 970,\n",
       " 103,\n",
       " 154,\n",
       " 981,\n",
       " 284,\n",
       " 62,\n",
       " 811,\n",
       " 571,\n",
       " 32,\n",
       " 495,\n",
       " 927,\n",
       " 73,\n",
       " 196,\n",
       " 315,\n",
       " 360,\n",
       " 27,\n",
       " 970,\n",
       " 395,\n",
       " 227,\n",
       " 913,\n",
       " 710,\n",
       " 802,\n",
       " 519,\n",
       " 657,\n",
       " 546,\n",
       " 713,\n",
       " 575,\n",
       " 587,\n",
       " 614,\n",
       " 528,\n",
       " 146,\n",
       " 761,\n",
       " 779,\n",
       " 118,\n",
       " 672,\n",
       " 271,\n",
       " 880,\n",
       " 809,\n",
       " 967,\n",
       " 215,\n",
       " 209,\n",
       " 937,\n",
       " 603,\n",
       " 770,\n",
       " 629,\n",
       " 51,\n",
       " 247,\n",
       " 441,\n",
       " 621,\n",
       " 305,\n",
       " 473,\n",
       " 29,\n",
       " 763,\n",
       " 908,\n",
       " 254,\n",
       " 14,\n",
       " 2,\n",
       " 41,\n",
       " 602,\n",
       " 292,\n",
       " 574,\n",
       " 424,\n",
       " 666,\n",
       " 730,\n",
       " 679,\n",
       " 885,\n",
       " 340,\n",
       " 908,\n",
       " 33,\n",
       " 572,\n",
       " 41,\n",
       " 425,\n",
       " 456,\n",
       " 694,\n",
       " 226,\n",
       " 215,\n",
       " 732,\n",
       " 504,\n",
       " 508,\n",
       " 451,\n",
       " 592,\n",
       " 250,\n",
       " 219,\n",
       " 228,\n",
       " 743,\n",
       " 727,\n",
       " 369,\n",
       " 262,\n",
       " 728,\n",
       " 328,\n",
       " 854,\n",
       " 604,\n",
       " 499,\n",
       " 955,\n",
       " 863,\n",
       " 884,\n",
       " 612,\n",
       " 188,\n",
       " 609,\n",
       " 251,\n",
       " 375,\n",
       " 126,\n",
       " 308,\n",
       " 226,\n",
       " 495,\n",
       " 89,\n",
       " 501,\n",
       " 3,\n",
       " 296,\n",
       " 955,\n",
       " 212,\n",
       " 676,\n",
       " 680,\n",
       " 172,\n",
       " 842,\n",
       " 532,\n",
       " 195,\n",
       " 38,\n",
       " 684,\n",
       " 928,\n",
       " 126,\n",
       " 810,\n",
       " 784,\n",
       " 729,\n",
       " 503,\n",
       " 512,\n",
       " 48,\n",
       " 5,\n",
       " 55,\n",
       " 224,\n",
       " 749,\n",
       " 474,\n",
       " 32,\n",
       " 1,\n",
       " 453,\n",
       " 999,\n",
       " 810,\n",
       " 5,\n",
       " 11,\n",
       " 358,\n",
       " 823,\n",
       " 226,\n",
       " 474,\n",
       " 45,\n",
       " 195,\n",
       " 137,\n",
       " 535,\n",
       " 32,\n",
       " 712,\n",
       " 256,\n",
       " 747,\n",
       " 506,\n",
       " 788,\n",
       " 989,\n",
       " 643,\n",
       " 555,\n",
       " 165,\n",
       " 868,\n",
       " 883,\n",
       " 964,\n",
       " 719,\n",
       " 948,\n",
       " 24,\n",
       " 176,\n",
       " 612,\n",
       " 991,\n",
       " 547,\n",
       " 275,\n",
       " 630,\n",
       " 238,\n",
       " 752,\n",
       " 712,\n",
       " 675,\n",
       " 224,\n",
       " 422,\n",
       " 449,\n",
       " 424,\n",
       " 813,\n",
       " 1,\n",
       " 86,\n",
       " 918,\n",
       " 641,\n",
       " 126,\n",
       " 532,\n",
       " 481,\n",
       " 653,\n",
       " 745,\n",
       " 287,\n",
       " 885,\n",
       " 929,\n",
       " 518,\n",
       " 518,\n",
       " 344,\n",
       " 465,\n",
       " 796,\n",
       " 89,\n",
       " 779,\n",
       " 897,\n",
       " 384,\n",
       " 508,\n",
       " 709,\n",
       " 832,\n",
       " 750,\n",
       " 663,\n",
       " 270,\n",
       " 372,\n",
       " 680,\n",
       " 516,\n",
       " 974,\n",
       " 117,\n",
       " 239,\n",
       " 946,\n",
       " 201,\n",
       " 212,\n",
       " 497,\n",
       " 958,\n",
       " 272,\n",
       " 991,\n",
       " 818,\n",
       " 828,\n",
       " 622,\n",
       " 923,\n",
       " 572,\n",
       " 353,\n",
       " 432,\n",
       " 936,\n",
       " 342,\n",
       " 673,\n",
       " 290,\n",
       " 320,\n",
       " 683,\n",
       " 276,\n",
       " 195,\n",
       " 47,\n",
       " 773,\n",
       " 698,\n",
       " 704,\n",
       " 784,\n",
       " 290,\n",
       " 880,\n",
       " 227,\n",
       " 76,\n",
       " 144,\n",
       " 147,\n",
       " 247,\n",
       " 376,\n",
       " 901,\n",
       " 494,\n",
       " 883,\n",
       " 461,\n",
       " 25,\n",
       " 61,\n",
       " 379,\n",
       " 769,\n",
       " 850,\n",
       " 407,\n",
       " 330,\n",
       " 995,\n",
       " 629,\n",
       " 181,\n",
       " 799,\n",
       " 260,\n",
       " 226,\n",
       " 674,\n",
       " 466,\n",
       " 184,\n",
       " 819,\n",
       " 284,\n",
       " 9,\n",
       " 164,\n",
       " 149,\n",
       " 612,\n",
       " 555,\n",
       " 761,\n",
       " 745,\n",
       " 533,\n",
       " 76,\n",
       " 907,\n",
       " 593,\n",
       " 181,\n",
       " 439,\n",
       " 738,\n",
       " 973,\n",
       " 935,\n",
       " 367,\n",
       " 437,\n",
       " 249,\n",
       " 177,\n",
       " 795,\n",
       " 582,\n",
       " 537,\n",
       " 595,\n",
       " 797,\n",
       " 847,\n",
       " 399,\n",
       " 327,\n",
       " 866,\n",
       " 392,\n",
       " 266,\n",
       " 776,\n",
       " 426,\n",
       " 953,\n",
       " 249,\n",
       " 354,\n",
       " 748,\n",
       " 468,\n",
       " 36,\n",
       " 973,\n",
       " 223,\n",
       " 169,\n",
       " 533,\n",
       " 90,\n",
       " 697,\n",
       " 938,\n",
       " 757,\n",
       " 489,\n",
       " 777,\n",
       " 408,\n",
       " 936,\n",
       " 92,\n",
       " 471,\n",
       " 301,\n",
       " 921,\n",
       " 418,\n",
       " 425,\n",
       " 584,\n",
       " 557,\n",
       " 841,\n",
       " 130,\n",
       " 910,\n",
       " 221,\n",
       " 927,\n",
       " 946,\n",
       " 380,\n",
       " 860,\n",
       " 840,\n",
       " 586,\n",
       " 247,\n",
       " 382,\n",
       " 298,\n",
       " 822,\n",
       " 536,\n",
       " 180,\n",
       " 547,\n",
       " 83,\n",
       " 535,\n",
       " 154,\n",
       " 499,\n",
       " 102,\n",
       " 9,\n",
       " 462,\n",
       " 513,\n",
       " 775,\n",
       " 187,\n",
       " 549,\n",
       " 849,\n",
       " 935,\n",
       " 425,\n",
       " 180,\n",
       " 511,\n",
       " 799,\n",
       " 63,\n",
       " 390,\n",
       " 507,\n",
       " 942,\n",
       " 190,\n",
       " 820,\n",
       " 660,\n",
       " 721,\n",
       " 655,\n",
       " 11,\n",
       " 848,\n",
       " 170,\n",
       " 305,\n",
       " 373,\n",
       " 515,\n",
       " 598,\n",
       " 582,\n",
       " 656,\n",
       " 769,\n",
       " 561,\n",
       " 65,\n",
       " 94,\n",
       " 515,\n",
       " 886,\n",
       " 729,\n",
       " 538,\n",
       " 539,\n",
       " 935,\n",
       " 840,\n",
       " 430,\n",
       " 117,\n",
       " 126,\n",
       " 267,\n",
       " 755,\n",
       " 590,\n",
       " 874,\n",
       " 508,\n",
       " 557,\n",
       " 631,\n",
       " 619,\n",
       " 144,\n",
       " 920,\n",
       " 695,\n",
       " 80,\n",
       " 749,\n",
       " 705,\n",
       " 623,\n",
       " 683,\n",
       " 625,\n",
       " 970,\n",
       " 838,\n",
       " 695,\n",
       " 662,\n",
       " 542,\n",
       " 750,\n",
       " 493,\n",
       " 989,\n",
       " 89,\n",
       " 953,\n",
       " 640,\n",
       " 757,\n",
       " 257,\n",
       " 244,\n",
       " 849,\n",
       " 363,\n",
       " 999,\n",
       " 790,\n",
       " 479,\n",
       " 645,\n",
       " 122,\n",
       " 929,\n",
       " 74,\n",
       " 500,\n",
       " 96,\n",
       " 917,\n",
       " 523,\n",
       " 38,\n",
       " 204,\n",
       " 735,\n",
       " 855,\n",
       " 734,\n",
       " 557,\n",
       " 172,\n",
       " 236,\n",
       " 923,\n",
       " 920,\n",
       " 155,\n",
       " 359,\n",
       " 601,\n",
       " 575,\n",
       " 838,\n",
       " 375,\n",
       " 659,\n",
       " 460,\n",
       " 271,\n",
       " 594,\n",
       " 665,\n",
       " 131,\n",
       " 649,\n",
       " 888,\n",
       " 452,\n",
       " 907,\n",
       " 275,\n",
       " 546,\n",
       " 468,\n",
       " 506,\n",
       " 171,\n",
       " 842,\n",
       " 862,\n",
       " 194,\n",
       " 451,\n",
       " 930,\n",
       " 657,\n",
       " 541,\n",
       " 536,\n",
       " 108,\n",
       " 309,\n",
       " 592,\n",
       " 491,\n",
       " 400,\n",
       " 47,\n",
       " 964,\n",
       " 262,\n",
       " 87,\n",
       " 972,\n",
       " 904,\n",
       " 154,\n",
       " 828,\n",
       " 677,\n",
       " 675,\n",
       " 301,\n",
       " 537,\n",
       " 354,\n",
       " 38,\n",
       " 866,\n",
       " 960,\n",
       " 921,\n",
       " 659,\n",
       " 970,\n",
       " 239,\n",
       " 189,\n",
       " 21,\n",
       " 538,\n",
       " 618,\n",
       " 857,\n",
       " 948,\n",
       " 72,\n",
       " 576,\n",
       " 427,\n",
       " 934,\n",
       " 676,\n",
       " 547,\n",
       " 522,\n",
       " 457,\n",
       " 294,\n",
       " 619,\n",
       " 169,\n",
       " 203,\n",
       " 920,\n",
       " 7,\n",
       " 537,\n",
       " 861,\n",
       " 601,\n",
       " 169,\n",
       " 714,\n",
       " 197,\n",
       " 540,\n",
       " 560,\n",
       " 653,\n",
       " 174,\n",
       " 58,\n",
       " 663,\n",
       " 477,\n",
       " 838,\n",
       " 265,\n",
       " 521,\n",
       " 713,\n",
       " 149,\n",
       " 200,\n",
       " 547,\n",
       " 762,\n",
       " 173,\n",
       " 391,\n",
       " 273,\n",
       " 300,\n",
       " 871,\n",
       " 234,\n",
       " 370,\n",
       " 701,\n",
       " 340,\n",
       " 469,\n",
       " 79,\n",
       " 419,\n",
       " 706,\n",
       " 728,\n",
       " 14,\n",
       " 530,\n",
       " 651,\n",
       " 900,\n",
       " 255,\n",
       " 323,\n",
       " 714,\n",
       " 508,\n",
       " 361,\n",
       " 704,\n",
       " 413,\n",
       " 156,\n",
       " 983,\n",
       " 673,\n",
       " 965,\n",
       " 267,\n",
       " 754,\n",
       " 706,\n",
       " 587,\n",
       " 312,\n",
       " 962,\n",
       " 594,\n",
       " 637,\n",
       " 586,\n",
       " 507,\n",
       " 148,\n",
       " 250,\n",
       " 305,\n",
       " 309,\n",
       " 935,\n",
       " 146,\n",
       " 726,\n",
       " 205,\n",
       " 273,\n",
       " 880,\n",
       " 678,\n",
       " 744,\n",
       " 728,\n",
       " 729,\n",
       " 953,\n",
       " 498,\n",
       " 719,\n",
       " 19,\n",
       " 126,\n",
       " 191,\n",
       " 326,\n",
       " 486,\n",
       " 22,\n",
       " 481,\n",
       " 920,\n",
       " 88,\n",
       " 963,\n",
       " 86,\n",
       " 790,\n",
       " 973,\n",
       " 763,\n",
       " 26,\n",
       " 332,\n",
       " 574,\n",
       " 217,\n",
       " 168,\n",
       " 37,\n",
       " 770,\n",
       " 753,\n",
       " 317,\n",
       " 830,\n",
       " 770,\n",
       " 680,\n",
       " 121,\n",
       " 519,\n",
       " 236,\n",
       " 622,\n",
       " 602,\n",
       " 472,\n",
       " 360,\n",
       " 800,\n",
       " 361,\n",
       " 608,\n",
       " 244,\n",
       " 410,\n",
       " 358,\n",
       " 494,\n",
       " 638,\n",
       " 643,\n",
       " 205,\n",
       " 492,\n",
       " 411,\n",
       " 263,\n",
       " 974,\n",
       " 259,\n",
       " 161,\n",
       " 290,\n",
       " 998,\n",
       " 245,\n",
       " 739,\n",
       " 32,\n",
       " 13,\n",
       " 452,\n",
       " 457,\n",
       " 994,\n",
       " 545,\n",
       " 690,\n",
       " 644,\n",
       " 458,\n",
       " 917,\n",
       " 497,\n",
       " 978,\n",
       " 859,\n",
       " 799,\n",
       " 192,\n",
       " 951,\n",
       " 47,\n",
       " 155,\n",
       " 695,\n",
       " 264,\n",
       " 594,\n",
       " 402,\n",
       " 744,\n",
       " 792,\n",
       " 791,\n",
       " 405,\n",
       " 354,\n",
       " 940,\n",
       " 522,\n",
       " 291,\n",
       " 973,\n",
       " 817,\n",
       " 167,\n",
       " 736,\n",
       " 682,\n",
       " 751,\n",
       " 724,\n",
       " 690,\n",
       " 13,\n",
       " 66,\n",
       " 181,\n",
       " 635,\n",
       " 886,\n",
       " 897,\n",
       " 779,\n",
       " 204,\n",
       " 309,\n",
       " 166,\n",
       " 28,\n",
       " 645,\n",
       " 742,\n",
       " 967,\n",
       " 713,\n",
       " 497,\n",
       " 857,\n",
       " 860,\n",
       " 689,\n",
       " 603,\n",
       " 558,\n",
       " 763,\n",
       " 655,\n",
       " 533,\n",
       " 794,\n",
       " 562,\n",
       " 181,\n",
       " 135,\n",
       " 249,\n",
       " 42,\n",
       " 903,\n",
       " 403,\n",
       " 934,\n",
       " 952,\n",
       " 858,\n",
       " 614,\n",
       " 662,\n",
       " 860,\n",
       " 213,\n",
       " 172,\n",
       " 918,\n",
       " 221,\n",
       " 166,\n",
       " 583,\n",
       " 925,\n",
       " 184,\n",
       " 516,\n",
       " 408,\n",
       " 589,\n",
       " 164,\n",
       " 288,\n",
       " 65,\n",
       " 257,\n",
       " 932,\n",
       " 218,\n",
       " 675,\n",
       " 791,\n",
       " 761,\n",
       " 871,\n",
       " 725,\n",
       " 599,\n",
       " 375,\n",
       " 653,\n",
       " 116,\n",
       " 746,\n",
       " 319,\n",
       " 580,\n",
       " 914,\n",
       " 450,\n",
       " 958,\n",
       " 387,\n",
       " 912,\n",
       " 325,\n",
       " 523,\n",
       " 275,\n",
       " 65,\n",
       " 903,\n",
       " 797,\n",
       " 865,\n",
       " 20,\n",
       " 94,\n",
       " 944,\n",
       " 883,\n",
       " 742,\n",
       " 925,\n",
       " 677,\n",
       " 384,\n",
       " 176,\n",
       " 729,\n",
       " 148,\n",
       " 464,\n",
       " 223,\n",
       " 249,\n",
       " 158,\n",
       " 283,\n",
       " 928,\n",
       " 380,\n",
       " 607,\n",
       " 331,\n",
       " 523,\n",
       " 162,\n",
       " 353,\n",
       " 898,\n",
       " 660,\n",
       " 434,\n",
       " 573,\n",
       " 567,\n",
       " 637,\n",
       " 21,\n",
       " 896,\n",
       " 198,\n",
       " 793,\n",
       " 839,\n",
       " 269,\n",
       " 172,\n",
       " 663,\n",
       " 221,\n",
       " 681,\n",
       " 636,\n",
       " 513,\n",
       " 844,\n",
       " 91,\n",
       " 992,\n",
       " 3,\n",
       " 887,\n",
       " 58,\n",
       " 967,\n",
       " 764,\n",
       " 726,\n",
       " 277,\n",
       " 67,\n",
       " 921,\n",
       " 699,\n",
       " 944,\n",
       " 82,\n",
       " 829,\n",
       " 331,\n",
       " 959,\n",
       " 757,\n",
       " 665,\n",
       " 245,\n",
       " 475,\n",
       " 475,\n",
       " 173,\n",
       " 820,\n",
       " 854,\n",
       " 519,\n",
       " 680,\n",
       " 419,\n",
       " 116,\n",
       " 900,\n",
       " 206,\n",
       " 410,\n",
       " 930,\n",
       " 401,\n",
       " 84,\n",
       " 997,\n",
       " 838,\n",
       " 660]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = generate_graph('ml-1m')\n",
    "zeta = t.node_copying()\n",
    "\n",
    "zeta\n",
    "# zeta = node_copying(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0790388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -th Graph sampled time cost :  0.12219595909118652\n",
      "2 -th Graph sampled time cost :  0.10221552848815918\n",
      "3 -th Graph sampled time cost :  0.1017293930053711\n"
     ]
    }
   ],
   "source": [
    "# zeta를 한번 만든 후 iteration마다 generate graph하는 방식\n",
    "\n",
    "for epoch in range(3):\n",
    "    t.generate_graph(epsilon = 0.01, iteration = epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c05d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT\n",
    "# 만들어지는 graph마다 embedding 초기화 \n",
    "# embedding 학습하는 GNN 구조 짜기\n",
    "# 학습된 embedding 가지고 x_hat (eq.11) 구하고 BPR-OPT를 maximization 시키는 방향으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df93286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data로 sparse matrix 및 adjacency matrix 만들기\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "        train_file = path + '/train.txt'\n",
    "        # path : ml-1m\n",
    "        \n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.exist_users = []\n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "\n",
    "        self.n_users+=1\n",
    "        self.n_items+=1\n",
    "        print(self.n_users, self.n_items)\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.train_items = {}\n",
    "\n",
    "        with open(train_file, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l)==0:\n",
    "                    break\n",
    "                l = l.strip('\\n').split(' ')\n",
    "                uid, items = int(l[0]), [int(i) for i in l[1:]]\n",
    "\n",
    "                for i in items:\n",
    "                    self.R[uid, i] = 1\n",
    "\n",
    "                self.train_items[uid] = items\n",
    "                \n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            adj_mat = sp.load_npz(self.path + '/train_adj_mat.npz')\n",
    "            \n",
    "        except Exception:\n",
    "            adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/train_adj_mat.npz', adj_mat)\n",
    "            \n",
    "        return adj_mat\n",
    "    \n",
    "    \n",
    "    def create_adj_mat(self):\n",
    "        adj_mat = self.R.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape)\n",
    "        return adj_mat.tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83b5b0a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# node copying으로 만들어진 graph들에 대해서 adj matrix 만들고 npz 저장하는 과정 \n",
    "\n",
    "class sampled_graph_to_matrix(object):\n",
    "    def __init__(self, path, iteration, batch_size):\n",
    "        self.path =path\n",
    "        self.iteration = iteration\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        sampled_graph = path + '/sampled_graph/sampled_graph_' + str(iteration+1)\n",
    "        # path : 'ml-1m'\n",
    "        \n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.exist_users = []\n",
    "        self.neg_pools = {}\n",
    "        \n",
    "        with open(sampled_graph, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "\n",
    "        self.n_users+=1\n",
    "        self.n_items+=1\n",
    "        print(self.n_users, self.n_items)\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.train_items = {}\n",
    "\n",
    "        with open(sampled_graph, 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l)==0:\n",
    "                    break\n",
    "                l = l.strip('\\n').split(' ') \n",
    "                uid, items = int(l[0]), [int(i) for i in l[1:]]\n",
    "\n",
    "                for i in items:\n",
    "                    self.R[uid, i] = 1\n",
    "\n",
    "                self.train_items[uid] = items\n",
    "                \n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat_' + str(self.iteration+1) + '.npz')\n",
    "            \n",
    "        except Exception:\n",
    "            adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat_' + str(self.iteration+1) + '.npz', adj_mat)\n",
    "            \n",
    "        return adj_mat\n",
    "    \n",
    "     \n",
    "    def create_adj_mat(self):\n",
    "        adj_mat = self.R.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape)\n",
    "        return adj_mat.tocsr()\n",
    "    \n",
    "    def negative_pool(self):\n",
    "        t1 = time()\n",
    "        for u in self.train_items.keys():\n",
    "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\n",
    "            pools = [random.choice(neg_items) for _ in range(100)]\n",
    "            self.neg_pools[u] = pools\n",
    "        print('refresh negative pools', time() - t1)\n",
    "    \n",
    "    def sample(self):\n",
    "        # positive / negative items 나누기\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = random.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [random.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "            \n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            # u유저의 neighbor중 num개 만큼 positive item sampling\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num:\n",
    "                    break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "                \n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "        \n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            # u유저의 neighbor가 아닌 item 중 num개 만큼 sampling\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num:\n",
    "                    break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "        \n",
    "#         def sample_neg_items_for_u_from_pools(u, num):\n",
    "#             neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "#             return random.sample(neg_items, num)     \n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u,1)\n",
    "            neg_items += sample_neg_items_for_u(u,1)\n",
    "            \n",
    "        return users, pos_items, neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044aa381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 3953\n",
      "1000 3953\n",
      "1000 3953\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(3):\n",
    "    sampled_graph = sampled_graph_to_matrix(path='ml-1m', iteration = iteration, batch_size=1024)\n",
    "    sampled_graph.get_adj_mat()\n",
    "    # users, pos_items, neg_items = sampled_graph.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c07d7",
   "metadata": {},
   "source": [
    "#### ngcf에서의 w_gc와 w_bi는 gc는 neighbor aggregate하는 부분의 weight, bi는 element-wise product하는 부분의 weight\n",
    "#### bgcf에서는 single feed forward layer로 구성하였고 총 3개의 weight가 학습됨 (GAT에서 2개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aggregate-editor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 2., 3., 4., 5., 3., 3., 2., 1., 2.],\n",
      "        [2., 2., 1., 2., 3., 3., 1., 2., 2., 4.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(5,10)\n",
    "b = torch.tensor([1,2,3,4,5,3,3,2,1,2])\n",
    "c = torch.tensor([2,2,1,2,3,3,1,2,2,4])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c9f0be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    # TODO : 초기함수에 positive/negative item 리스트를 받아서 각 positive/negative item들의 embedding이 output으로 나오게끔\n",
    "    def __init__(self, n_users, n_items, in_features, out_features, dropout = 0.2):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features      # 기존 노드의 embedding 차원수 \n",
    "        self.out_features = out_features    # 결과 weight의 embedding 차원수\n",
    "        \n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        \n",
    "        # self.W = nn.Parameter(initializer(torch.empty(in_features, out_features)))\n",
    "        # W_1은 a*e aggregate 부분, W_2는 e aggregate 부분\n",
    "        self.weight_dict = nn.ParameterDict({\n",
    "            'W_1' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features))),\n",
    "            'W_2' : nn.Parameter(initializer(torch.empty(self.in_features, self.out_features)))\n",
    "        })\n",
    "        \n",
    "        self.embedding_dict = nn.ParameterDict({\n",
    "            'user_emb' : nn.Parameter(initializer(torch.empty(self.n_users, self.out_features))),\n",
    "            'item_emb' : nn.Parameter(initializer(torch.empty(self.n_items, self.out_features)))\n",
    "        })\n",
    "        \n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pass\n",
    "            \n",
    "    \n",
    "    def forward(self, adj_matrix, iteration):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.iteration = iteration\n",
    "        coef = torch.zeros(self.n_users, self.n_items)\n",
    "        \n",
    "#         adj_matrix = sp.load_npz(path + '/s_adj_mat_' + str(iteration+1) +'.npz').toarray()\n",
    "#         adj_matrix = torch.Tensor(adj)\n",
    "        \n",
    "        user_emb = self.embedding_dict['user_emb']\n",
    "        item_emb = self.embedding_dict['item_emb']\n",
    "        \n",
    "        # self.W의 크기를 정확하게 확인해야함, 최종적인 각 node의 embedding size 는 1 x emb_size 로 나오게끔\n",
    "        # score는 각각 user-item 간의 attention score\n",
    "        score = torch.exp(torch.inner(user_emb, item_emb))\n",
    "        score = torch.multiply(score, adj_matrix)\n",
    "        \n",
    "        for i in range(score.shape[0]):\n",
    "            norm = sum(score[i])\n",
    "            normalized_score = score[i] / norm\n",
    "            coef[i] = normalized_score\n",
    "        # coef에다가 normalized된 최종적인 attention score 저장\n",
    "        w_1 = self.weight_dict['W_1']\n",
    "        w_2 = self.weight_dict['W_2']\n",
    "        # h_tilde_1은 w_1과 곱해지는 부분의 embedding (e_k)를 user와 item으로 쪼개서 계산후 concat\n",
    "        h_tilde_1_user = torch.matmul(coef, item_emb)\n",
    "        h_tilde_1_user = torch.matmul(h_tilde_1_user, w_1)\n",
    "        h_tilde_1_item = torch.matmul(coef.T, user_emb)\n",
    "        h_tilde_1_item = torch.matmul(h_tilde_1_item, w_1)\n",
    "        h_tilde_1 = torch.cat((h_tilde_1_user, h_tilde_1_item), dim=0)\n",
    "        \n",
    "        neighbor_num_user, neighbor_num_item = torch.zeros(self.n_users), torch.zeros(self.n_items)\n",
    "        for i in range(self.n_users):\n",
    "            neighbor_num_user[i] = sum(adj_matrix[i])\n",
    "        for j in range(self.n_items):\n",
    "            neighbor_num_item[i] = sum(adj_matrix.T[i])\n",
    "        # neighbor_num은 n_j에 해당하는 부분\n",
    "        \n",
    "        # h_tilde_2는 w_2와 곱해지는 부분\n",
    "        h_tilde_2_user = torch.matmul(adj_matrix, item_emb)\n",
    "        # h_tilde_2_user = torch.matmul(neighbor_num_user, h_tilde_2_user)\n",
    "        # 각 user의 neighbor의 수 만큼 item embedding에 곱해주는 과정\n",
    "        for i in range(neighbor_num_user.shape[0]):\n",
    "            h_tilde_2_user[i] = neighbor_num_user[i] * h_tilde_2_user[i]\n",
    "        h_tilde_2_user = torch.matmul(h_tilde_2_user, w_2)\n",
    "        h_tilde_2_item = torch.matmul(adj_matrix.T, user_emb)\n",
    "        # h_tilde_2_item = torch.matmul(neighbor_num_item, h_tilde_2_item)\n",
    "        # 각 item의 neighbor의 수 만큼 user embedding에 곱해주는 과정\n",
    "        for j in range(neighbor_num_item.shape[0]):\n",
    "            h_tilde_2_item[i] = neighbor_num_item[i] * h_tilde_2_item[i]\n",
    "        h_tilde_2_item = torch.matmul(h_tilde_2_item, w_2)\n",
    "        h_tilde_2 = torch.cat((h_tilde_2_user, h_tilde_2_item), dim=0)\n",
    "        \n",
    "        print(h_tilde_1.shape, h_tilde_2.shape)\n",
    "        \n",
    "        h_tilde_sampled = torch.cat((h_tilde_1, h_tilde_2), dim=0)\n",
    "        \n",
    "        return h_tilde_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c44826b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4953, 64]) torch.Size([4953, 64])\n",
      "tensor([[-6.6439e-03, -1.3936e-03,  1.9929e-03,  ..., -5.2830e-03,\n",
      "          1.6780e-03,  1.6225e-03],\n",
      "        [ 2.6042e-03, -2.3948e-03, -1.4094e-03,  ...,  5.7171e-03,\n",
      "         -1.3433e-03, -4.9075e-04],\n",
      "        [-6.3803e-04, -1.3037e-03,  6.1813e-05,  ...,  8.9731e-03,\n",
      "          9.7857e-04,  2.2679e-03],\n",
      "        ...,\n",
      "        [ 1.3844e-01,  6.1511e-02,  9.9717e-02,  ...,  1.0996e-01,\n",
      "          1.8550e-01,  1.3911e-01],\n",
      "        [ 2.4384e-01,  1.8024e-01,  6.8912e-02,  ..., -6.2643e-02,\n",
      "          1.0902e-01,  2.1208e-01],\n",
      "        [-3.8323e-01,  1.3401e-01,  4.8670e-01,  ..., -1.3615e-01,\n",
      "          9.2921e-01,  6.4686e-01]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pos_items, neg_items = sampled_graph_to_matrix(path='ml-1m', iteration = iteration).divide_pos_neg()\n",
    "# 완성형은 GATLayer(6040, 3953, 64,64, pos_items, neg_items)\n",
    "model = GATLayer(1000, 3953, 64, 64)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "path = 'ml-1m'\n",
    "for epoch in range(1):\n",
    "    for iteration in range(1):\n",
    "        t1 = time()\n",
    "        adj_matrix = sp.load_npz(path + '/s_adj_mat_' + str(iteration+1) +'.npz').toarray()\n",
    "        adj_matrix = torch.Tensor(adj_matrix)\n",
    "\n",
    "        '''불러오는 sampled graph matrix마다 pos, neg item set을 만들고 (함수 사용) \n",
    "        bpr loss 함수에다가 각 item들에 해당하는 embedding을 입력으로 넣어줌'''\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(adj_matrix, iteration)\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f8825c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adj = sp.load_npz('ml-1m/s_adj_mat_1.npz')\n",
    "adj = adj.toarray()\n",
    "print(adj.shape)\n",
    "adj = torch.Tensor(adj)\n",
    "coef = torch.zeros(3,3953)\n",
    "user_emb = nn.Parameter(initializer(torch.empty(1000,32)))\n",
    "item_emb = nn.Parameter(initializer(torch.empty(3953,32)))\n",
    "w_1 = nn.Parameter(initializer(torch.empty(32,32)))\n",
    "w_2 = nn.Parameter(initializer(torch.empty(32,32)))\n",
    "\n",
    "x = torch.exp(torch.inner(user_emb, item_emb))\n",
    "print(x)\n",
    "x = torch.multiply(x, adj)\n",
    "print(x)\n",
    "for i in range(3):\n",
    "    tot = sum(x[i])\n",
    "    score = x[i] / tot\n",
    "    coef[i] = score\n",
    "    \n",
    "w1 = torch.matmul(coef, item_emb)\n",
    "print(w1.shape)\n",
    "w1 = torch.matmul(w1, w_1)\n",
    "print(w1.shape)\n",
    "w2 = torch.matmul(coef.T, user_emb[:3])\n",
    "print(w2.shape)\n",
    "w2 = torch.matmul(w2, w_2)\n",
    "print(w2.shape)\n",
    "w3 = torch.cat((w1, w2), dim=0)\n",
    "w3.shape\n",
    "\n",
    "user_emb = nn.Parameter(initializer(torch.empty(1000,32)))\n",
    "item_emb = nn.Parameter(initializer(torch.empty(3953,32)))\n",
    "avg_emb = torch.matmul(adj.T, user_emb)\n",
    "avg_emb.shape\n",
    "# x = torch.multiply(x, adj)\n",
    "# print(x.shape)\n",
    "# x = F.softmax(x, dim=1)\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3840a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_num(row):\n",
    "    return sum(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumemb = torch.zeros((5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    sumemb[i] = sum(adj[i])\n",
    "    \n",
    "torch.matmul(sumemb, item_emb[:5]).shape\n",
    "# sumemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead80d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = nn.init.xavier_uniform_\n",
    "a = nn.Parameter(initializer(torch.empty(5,5)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646153ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = nn.init.xavier_uniform_\n",
    "user_emb = nn.Parameter(initializer(torch.empty(5,10)))\n",
    "user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.matrix([[0,1,2],[1,1,1],[0,0,1]])\n",
    "adj = np.matrix([[0,0,1],[1,0,0],[1,1,0]])\n",
    "e = np.matrix([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "print(a,'\\n')\n",
    "print(adj,'\\n')\n",
    "print(e,'\\n')\n",
    "\n",
    "a_jk = np.multiply(a,adj)\n",
    "print(a_jk, '\\n')\n",
    "\n",
    "np.matmul(a_jk, e)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80349c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
