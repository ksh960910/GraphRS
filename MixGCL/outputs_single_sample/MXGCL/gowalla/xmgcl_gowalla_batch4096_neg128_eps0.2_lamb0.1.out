nohup python main.py --dataset gowalla --gnn xmgcl --dim 64 --lr 0.001 --batch_size 4096 --gpu_id 2 --context_hops 3 --pool mean --ns mixgcf --n_negs 128 --K 1 --eps 0.2 --lamb 0.1 &
reading train and test user-item set ...
building the adj mat ...
loading over ...
X Mix Simple GCL model setup
start training ...
+-------+--------------------+-------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)  |        Loss       |               recall               |                ndcg                |              novelty               |             precision              |             hit_ratio              |
+-------+--------------------+-------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   0   | 104.51511406898499 | 47.70416522026062 | 270.7212829589844 | [0.13987266 0.19761226 0.23994198] | [0.11369518 0.13218703 0.1448456 ] | [0.62307032 0.64525218 0.65794919] | [0.04262174 0.03044829 0.02482305] | [0.46922098 0.56718467 0.62442227] |
+-------+--------------------+-------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
using time 103.6921s, training loss at epoch 1: 250.8394
using time 102.9491s, training loss at epoch 2: 240.0313
using time 103.5405s, training loss at epoch 3: 228.4610
using time 105.9318s, training loss at epoch 4: 215.3710
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |               recall               |                ndcg                |              novelty               |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   5   | 103.02883863449097 | 53.082112550735474 | 201.68312072753906 | [0.16265621 0.22912808 0.27713675] | [0.13729415 0.15820623 0.17256624] | [0.54716325 0.57494596 0.59166505] | [0.0509093  0.03620638 0.02942427] | [0.55077366 0.65489986 0.7127403 ] |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
using time 103.3312s, training loss at epoch 6: 189.2645
using time 103.4559s, training loss at epoch 7: 179.6293
using time 102.9368s, training loss at epoch 8: 172.1886
using time 107.3464s, training loss at epoch 9: 166.4118
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |               recall               |                ndcg                |              novelty               |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   10  | 103.41814637184143 | 53.620527267456055 | 161.78318786621094 | [0.17319149 0.24272633 0.29211853] | [0.1470791  0.16890759 0.18371055] | [0.55062186 0.57821647 0.59473338] | [0.05419988 0.03841182 0.03110668] | [0.5720075  0.67703798 0.73186416] |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
using time 104.6242s, training loss at epoch 11: 157.9959
using time 99.6460s, training loss at epoch 12: 154.6821
using time 103.4138s, training loss at epoch 13: 151.9548
using time 106.5962s, training loss at epoch 14: 149.5322
+-------+-------------------+------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |  tesing time(s)  |        Loss        |               recall               |                ndcg                |              novelty               |             precision              |             hit_ratio              |
+-------+-------------------+------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   15  | 103.5760669708252 | 53.3706419467926 | 147.45205688476562 | [0.1784582  0.2494277  0.30073483] | [0.15152529 0.17389075 0.18923971] | [0.56004594 0.58675417 0.60261927] | [0.05582256 0.03955389 0.03204501] | [0.5810838  0.6835354  0.74013665] |
+-------+-------------------+------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
using time 99.5427s, training loss at epoch 16: 145.5883
using time 98.9270s, training loss at epoch 17: 143.9913
using time 103.2869s, training loss at epoch 18: 142.4861
using time 105.1428s, training loss at epoch 19: 141.1747
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |               recall               |                ndcg                |              novelty               |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   20  | 102.75458693504333 | 52.175458908081055 | 140.05726623535156 | [0.18215315 0.25427182 0.30541185] | [0.15425364 0.17697096 0.192248  ] | [0.56661537 0.59244001 0.60777778] | [0.05689932 0.04027564 0.0325005 ] | [0.58617456 0.68912854 0.74623217] |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
using time 103.4155s, training loss at epoch 21: 138.9641
using time 103.7942s, training loss at epoch 22: 137.8962
using time 103.0169s, training loss at epoch 23: 137.0543
using time 110.6980s, training loss at epoch 24: 136.2330
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |               recall               |                ndcg                |              novelty               |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   25  | 103.60714173316956 | 51.807130575180054 | 135.55258178710938 | [0.18366917 0.25653464 0.30826424] | [0.15526546 0.17827155 0.19367997] | [0.57140638 0.59650603 0.61144934] | [0.0573096  0.04059967 0.03273829] | [0.58771519 0.69110456 0.74807422] |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
using time 104.5497s, training loss at epoch 26: 134.9090
using time 103.7239s, training loss at epoch 27: 134.2493
using time 99.5263s, training loss at epoch 28: 133.7515
using time 102.8776s, training loss at epoch 29: 133.1737
+-------+-------------------+------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |  tesing time(s)  |        Loss        |               recall               |                ndcg                |              novelty               |             precision              |             hit_ratio              |
+-------+-------------------+------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   30  | 99.67096519470215 | 52.3290958404541 | 132.71449279785156 | [0.184293   0.25818141 0.30898354] | [0.1555532  0.1788642  0.19406035] | [0.57484404 0.59932319 0.61380932] | [0.05749045 0.04076546 0.032817  ] | [0.58818407 0.69321455 0.74780628] |
+-------+-------------------+------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
using time 103.6877s, training loss at epoch 31: 132.2426
using time 103.1429s, training loss at epoch 32: 131.7611
using time 103.5210s, training loss at epoch 33: 131.3875
using time 109.6976s, training loss at epoch 34: 130.9347
+-------+--------------------+-------------------+---------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)  |      Loss     |               recall               |                ndcg                |              novelty               |             precision              |             hit_ratio              |
+-------+--------------------+-------------------+---------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   35  | 102.98601937294006 | 50.83914041519165 | 130.599609375 | [0.18478086 0.25890912 0.30964138] | [0.15562571 0.17900083 0.19416483] | [0.57777929 0.60174199 0.61577017] | [0.05764619 0.040845   0.03285161] | [0.58878692 0.69358296 0.7481412 ] |
+-------+--------------------+-------------------+---------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
using time 98.8272s, training loss at epoch 36: 130.3139
using time 102.5166s, training loss at epoch 37: 129.9972
using time 102.0016s, training loss at epoch 38: 129.7163
using time 107.0461s, training loss at epoch 39: 129.4053
+-------+--------------------+-------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |               recall               |                ndcg                |              novelty               |             precision              |             hit_ratio              |
+-------+--------------------+-------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   40  | 103.60551595687866 | 47.55589771270752 | 129.19781494140625 | [0.18455062 0.25907043 0.30963727] | [0.15515754 0.17866079 0.19378445] | [0.58012182 0.60358545 0.6172457 ] | [0.05757084 0.04082993 0.03282202] | [0.58898788 0.69318106 0.74844263] |
+-------+--------------------+-------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
using time 102.8036s, training loss at epoch 41: 128.8761
using time 99.5967s, training loss at epoch 42: 128.7048
using time 99.6388s, training loss at epoch 43: 128.4363
using time 106.8744s, training loss at epoch 44: 128.2461
+-------+--------------------+-------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)  |        Loss        |               recall               |                ndcg                |              novelty               |             precision              |             hit_ratio              |
+-------+--------------------+-------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   45  | 103.71036171913147 | 52.18689680099487 | 128.01625061035156 | [0.18435059 0.25852629 0.30971453] | [0.15473047 0.17812108 0.19341131] | [0.58235152 0.60523136 0.6185162 ] | [0.05744189 0.04069596 0.0327729 ] | [0.58845201 0.69170742 0.74797374] |
+-------+--------------------+-------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
using time 103.9475s, training loss at epoch 46: 127.8800
using time 103.0789s, training loss at epoch 47: 127.6438
using time 99.7962s, training loss at epoch 48: 127.5093
using time 108.4479s, training loss at epoch 49: 127.2980
Traceback (most recent call last):
  File "main.py", line 205, in <module>
    batch_loss = model(neg_candidate_users, neg_candidate_items, batch)
  File "/home/sanghun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sanghun/Desktop/GraphRS/MixGCL/modules/XmixSimGCL.py", line 156, in forward
    cl_loss = self.cl_rate * self.cal_cl_loss([user, pos_item], rec_user_emb,cl_user_emb,rec_item_emb,cl_item_emb, neg_candidate_users, neg_candidate_items)
  File "/home/sanghun/Desktop/GraphRS/MixGCL/modules/XmixSimGCL.py", line 283, in cal_cl_loss
    neg_item_view = self.items_mixup(cl_item_view0, item_view2, neg_candidate_items, i_idx)
  File "/home/sanghun/Desktop/GraphRS/MixGCL/modules/XmixSimGCL.py", line 262, in items_mixup
    neg_emb = neg_emb[[i for i in range(neg_emb.shape[0])],indices,:]
RuntimeError: CUDA error: the launch timed out and was terminated
